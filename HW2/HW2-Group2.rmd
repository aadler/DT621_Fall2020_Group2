---
title: "DATA 621 - Business Analytics and Data Mining"
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg, Bryan Persaud,
  Jeff Shamp
date: "10/11/2020"
output:
  html_document: default
  pdf_document: default
subtitle: 'Fall 2020 - Group 2 - Homework #2'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, error=FALSE, warning=FALSE, message=FALSE)
```

# 0. Preamble
***For all the following answers, it is expected that we are only dealing with***
***binary classification in which negative is coded as 0 and positive is coded***
***as 1. It is not difficult to expand the functions to respond to more general***
***cases, but it is an unnecessary complication for this homework assignment.***

# 1. The Data
```{r loadData, echo=FALSE}
library(tidyverse)
# urlRemote  = "https://raw.githubusercontent.com/"
# pathGithub = "aadler/DT621_Fall2020_Group2/master/HW1/data/"
# fileTrain   = "classification-output-data.csv"
# data = paste0(urlRemote, pathGithub, fileTrain) %>% read.csv()

data <- read.csv("./classification-output-data.csv")
```

The data set contains `r dim(data)[[1]]` data points, and has three key target
columns:  

  * **class**: the actual class for the observation  
  * **scored.class**: the predicted class for the observation (based on a
  threshold of 0.5)  
  * **scored.probability**: the predicted probability of success for the
  observation  

# 2. Confusion Matrix
A [confusion matrix](http://www.saedsayad.com/model_evaluation_c.htm) is a
format for summarizing the performance of a classification algorithm. It shows
the number of correct and incorrect predictions made by the classification model
and compares it to the actual outcomes in the data. The matrix is
\(N \times N\), where \(N\) is the number of target values. The following table
displays a \(2\times 2\) confusion matrix for two classes---positive and
negative. The actual confusion matrix are the 4 cells containing `a`, `b`, `c`,
and `d`. The enhancments around the matrix display the definitions for many
common classification metrics.

```{r confMatrixDesc, echo=FALSE}
cat("----------------------------------------------------------------------------
                 |           Target          |
Confusion Matrix |---------------------------|
                 |   Positive  |   Negative  |
-----------------|-------------|-------------|------------------------------
      | Positive |a (True Pos) |b (False Pos)| Positive Predictive | a/(a+b)
Model |----------|-------------|-------------|------------------------------
      | Negative |c (False Neg)|d (True Neg) | Negative Predictive | d/(c+d)
-----------------|-------------|-------------|------------------------------
                 | Sensitivity | Specificity |                    
                 |-------------|-------------|  Accuracy = (a+d)/(a+b+c+d)
                 |   a/(a+c)   |   d/(b+d)   | 
----------------------------------------------------------------------------")
```

  * **Accuracy**: the proportion of the total number of predictions that were
  correct.
  * **Positive Predictive Value** or **Precision**: the proportion of modeled
  positive cases that were correct.
  * **Negative Predictive Value**: the proportion of modeled negative cases that
  were correct.
  * **Sensitivity** or **Recall**: the proportion of actual positive cases that
  were properly detected by the model.
  * **Specificity**: the proportion of actual negative cases that were
  properly detected by the model.

```{r dataCM, echo=FALSE}
class_data = data %>% select(class, pred_class = scored.class,
                             pred_prob = scored.probability) 
table(class_data[,1:2]) %>% knitr::kable(caption = "Raw Confusion Matrix")
```

The raw confusion matrix is shown above. In this case, the rows represent the 
**true** classes and the columns represent the **predicted** classes. This is
not the same order as the hand-drawn confusion matrix above, but both are
internally consistent.

# 3. Accuracy
The function `ACC` below takes as input a data frame whose first two columns are
the true class and the modeled class respectively. It returns the accuracy of
the predictions using the following formula:

\[
\textrm{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN}
\]

```{r accuracy}
ACC <- function(df) {
  if (!is.data.frame(df)) {
    stop("Data set is not a dataframe. Try as.data.frame(dataset).")
  } else {
    dt <- table(df[, 1:2])
    sum(diag(dt)) / sum(dt)
  }
}
```

For this dataset the accuracy is 
`r prettyNum(ACC(class_data) * 100, digits = 5L)`%.

# 4. Classification Error Rate
The function `CER` below takes as input a data frame whose first two columns
are the true class and the modeled class respectively. It returns the
classification error rate of the predictions using the following formula:

\[
\textrm{Classification Error Rate} = \frac{FP + FN}{TP + FP + TN + FN}
\]

```{r error}
CER <- function(df) {
  if (!is.data.frame(df)) {
    stop("Data set is not a dataframe. Try as.data.frame(dataset).")
  } else {
    dt <- table(df[, 1:2])
    (sum(dt) - sum(diag(dt))) / sum(dt)
  }
}
```

For this dataset the classification error rate is
`r prettyNum(CER(class_data) * 100, digits = 5L)`%.

Mathematically, the classification error rate must be identical to
\(1 - \textrm{Accuracy}\). In this data set that is clear as 
`r prettyNum(ACC(class_data) * 100, digits = 5L)`% +
`r prettyNum(CER(class_data) * 100, digits = 5L)`% = 
`r prettyNum((CER(class_data) + ACC(class_data)) * 100, digits = 5L)`%.

# 5. Precision
The function `PRC` below takes as input a data frame whose first two columns
are the true class and the modeled class respectively. It returns the
classification error rate of the predictions using the following formula:

\[
\textrm{Precision} = \frac{TP}{TP + FP}
\]

```{r precision}
PRC <- function(df) {
  if (!is.data.frame(df)) {
    stop("Data set is not a dataframe. Try as.data.frame(dataset).")
  } else {
    dt <- table(df[, 1:2])
    dt[2, 2] / sum(dt[, 2])
  }
}
```

For this dataset the precision is
`r prettyNum(PRC(class_data) * 100, digits = 5L)`%.

# 6. Sensitivity or Recall
The function `SNS` below takes as input a data frame whose first two columns
are the true class and the modeled class respectively. It returns the
sensitivity of the predictions using the following formula:

\[
\textrm{Sensitivity} = \frac{TP}{TP + FN}
\]

```{r sensitivity}
SNS <- function(df) {
  if (!is.data.frame(df)) {
    stop("Data set is not a dataframe. Try as.data.frame(dataset).")
  } else {
    dt <- table(df[, 1:2])
    dt[2, 2] / sum(dt[2, ])
  }
}
```

For this dataset the sensitivity is
`r prettyNum(SNS(class_data) * 100, digits = 5L)`%.

# 7. Specificity
The function `SPC` below takes as input a data frame whose first two columns
are the true class and the modeled class respectively. It returns the
specificity of the predictions using the following formula:

\[
\textrm{Specificity} = \frac{TN}{TN + FP}
\]

```{r specificity}
SPC <- function(df) {
  if (!is.data.frame(df)) {
    stop("Data set is not a dataframe. Try as.data.frame(dataset).")
  } else {
    dt <- table(df[, 1:2])
    dt[1, 1] / sum(dt[1, ])
  }
}
```

For this dataset the specificity is
`r prettyNum(SPC(class_data) * 100, digits = 5L)`%.

# 8. \(F_1\) Score
The function `F1` below takes as input a data frame whose first two columns
are the true class and the modeled class respectively. It returns the
\(F_1\) score of the predictions using the following formula:

\[
\textrm{F}_1\;\textrm{Score} = \frac{2 \times \textrm{Precision} \times
\textrm{Sensitivity}}{\textrm{Precision} + \textrm{Sensitivity}}
\]

```{r f1}
F1 <- function(df) {
  if (!is.data.frame(df)) {
    stop("Data set is not a dataframe. Try as.data.frame(dataset).")
  } else {
    (2 * PRC(df) * SNS(df)) / (PRC(df) + SNS(df))
  }
}
```

For this dataset the \(F_1\) score is
`r prettyNum(F1(class_data) * 100, digits = 5L)`%.
