---
title: 'DATA 621 - Business Analytics and Data Mining'
subtitle: 'Fall 2020 - Group 2 - Homework #2'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "10/11/2020"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE)
```

```{r dataImport}
library(tidyverse)
library(pkr)
library(ggplot2)
library(caret)

# download data
urlRemote  = "https://raw.githubusercontent.com/"
pathGithub = "aadler/DT621_Fall2020_Group2/master/HW2/"
fileTrain   = "classification-output-data.csv"
data = paste0(urlRemote, pathGithub, fileTrain) %>% read.csv()
```

## The Dataset

The data set contains n = 181 data points, and has three key columns that will be used:  

* **class**: the actual class for the observation  
* **scored.class**: the predicted class for the observation (based on a threshold of 0.5)  
* **scored.probability**: the predicted probability of success for the observation  

Use the table() function to get the raw confusion matrix for this scored dataset. Make sure you understand the output. In particular, do the rows represent the actual or predicted class? The columns?

```{r data_table}

class_data = data %>% select(class, pred_class = scored.class, pred_prob = scored.probability)

# raw confusion matrix
table("Predicted"= data$scored.class, "Actual"= data$class)

#class_data %>% head() %>% knitr::kable(caption = "Preview of the Dataset")
#table(class_data[,1:2]) %>% knitr::kable(caption = "Counts of the Classification Data")
```

**Rows represent the predicted value and columns represent the actual value.**

* 119 records were predicted to be 0 that were actually 0. (True negative)
* 30 records were predicted to be 0 that were actually 1. (False negative)
* 5 records were predicted to be 1 that were actually 0. (False positive)
* 27 records were predicted to be 1 that were actually 1. (True positive)


## Confusion Matrix & Functions

A [confusion matrix](http://www.saedsayad.com/model_evaluation_c.htm) is a technique for summarizing the performance of a classification algorithm. It shows the number of correct and incorrect predictions made by the classification model compared to the actual outcomes (target value) in the data. The matrix is $N \times N$, where N is the number of target values (classes). The following table displays a 2x2 confusion matrix for two classes (Positive and Negative).

```{r}
cat("----------------------------------------------------------------------------
                 |           Target          |
Confusion Matrix |---------------------------|
                 |   Positive  |   Negative  |
-----------------|-------------|-------------|------------------------------
      | Positive |      a      |      b      | Positive Predictive | a/(a+b)
Model |----------|-------------|-------------|------------------------------
      | Negative |      c      |      d      | Negative Predictive | d/(c+d)
-----------------|-------------|-------------|------------------------------
                 | Sensitivity | Specificity |                    
                 |-------------|-------------|  Accuracy = (a+d)/(a+b+c+d)
                 |   a/(a+c)   |   d/(b+d)   | 
----------------------------------------------------------------------------")
```


* **Accuracy**: the proportion of the total number of predictions that were correct.  
* **Positive Predictive Value or Precision**: the proportion of positive cases that were correctly identified.  
* **Negative Predictive Value**: the proportion of negative cases that were correctly identified.  
* **Sensitivity or Recall**: the proportion of actual positive cases which are correctly identified.   
* Specificity: the proportion of actual negative cases which are correctly identified.  

### Accuracy of the Predictions

Function `accuracy` that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions. The formula is:

\[Accuracy = \frac{TP + TN}{TP + FP + TN + FN}\]


```{r accuracy, echo = TRUE}
accuracy = function(df) {
  if (is.data.frame(df) == FALSE) {
    return("Data set is not a dataframe. Try as.data.frame(dataset).")
  } else {
    dt = as.data.frame(table(df[, 1:2]))
    TN = dt %>% filter(class == 0 & pred_class == 0) %>% select(Freq)
    TP = dt %>% filter(class == 1 & pred_class == 1) %>% select(Freq)
    return((TP + TN) / nrow(df))
  }
}
```


### Error Rate of the Predictions

Function `error` that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the classification error rate of the predictions. The formula is:

\[\text{Classification Error Rate} = \frac{FP + FN}{TP + FP + TN + FN}\]

```{r error, echo = TRUE}
error = function(df) {
  if (is.data.frame(df) == FALSE) {
    return("Data set is not a dataframe. Try as.data.frame(dataset).")
  } else {
    dt = as.data.frame(table(df[, 1:2]))
    FP = dt %>% filter(class == 0 & pred_class == 1) %>% select(Freq)
    FN = dt %>% filter(class == 1 & pred_class == 0) %>% select(Freq)
    return((FP + FN) / nrow(df))
  }
}
```


### Precision of the Predictions

Function `precision` that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the precision of the predictions. The formula is:

\[precision = \frac{TP}{TP + FP}\]

```{r precision, echo = TRUE}
precision = function(df) {
  if (is.data.frame(df) == FALSE) {
    return("Data set is not a dataframe. Try as.data.frame(dataset).")
  } else {
    dt = as.data.frame(table(df[, 1:2]))
    FP = dt %>% filter(class == 0 & pred_class == 1) %>% select(Freq)
    TP = dt %>% filter(class == 1 & pred_class == 1) %>% select(Freq)
    return(TP / (TP + FP))
  }
}
```


### Sensitivity of the Predictions

Function `sensitivity` that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the sensitivity of the predictions. Sensitivity is also known as recall. The formula is:

\[sensitivity = \frac{TP}{TP + FN}\]

```{r sensitivity, echo = TRUE}
sensitivity = function(df) {
  if (is.data.frame(df) == FALSE) {
    return("Data set is not a dataframe. Try as.data.frame(dataset).")
  } else {
    dt = as.data.frame(table(df[, 1:2]))
    FN = dt %>% filter(class == 1 & pred_class == 0) %>% select(Freq)
    TP = dt %>% filter(class == 1 & pred_class == 1) %>% select(Freq)
    return(TP / (TP + FN))
  }
}
```



### Specificity of the Predictions

Function `specificity` that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the specificity of the predictions. The formula is:

\[specificity = \frac{TN}{TN + FP}\]

```{r specificity, echo = TRUE}
specificity = function(df) {
  if (is.data.frame(df) == FALSE) {
    return("Data set is not a dataframe. Try as.data.frame(dataset).")
  } else {
    dt = as.data.frame(table(df[, 1:2]))
    TN = dt %>% filter(class == 0 & pred_class == 0) %>% select(Freq)
    FP = dt %>% filter(class == 0 & pred_class == 1) %>% select(Freq)
    return(TN / (TN + FP))
  }
}
```


### F1 Score of the Predictions

Function `f1_score` that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the F1 score of the predictions. The formula is:

\[\text{F1 Score} = \frac{2 \times Precision \times Sensitivity}{Precision + Sensitivity}\]

```{r f1_score, echo = TRUE}
f1_score = function(df) {
  if (is.data.frame(df) == FALSE) {
    return("Data set is not a dataframe. Try as.data.frame(dataset).")
  } else {
    return((2*precision(df)*sensitivity(df)) / (precision(df) + sensitivity(df)))
  }
}
```


**What are the bounds on the F1 score? Show that the F1 score will always be between 0 and 1. (Hint: If 0 < 𝑎 < 1 and 0 < 𝑏 < 1 then 𝑎𝑏 < 𝑎.)**

We know that both precision and sensitivity must be between 0 and 1. 

* If these metrics are 0, it means that no true positives are identified. 
* If precision = 1, this means that there are no false positives. In other words, we didn't identify any records as being 1 that are actually 0. 
* If sensitivity = 1, this means that there are no false negatives. In other words, we didn't identify any records as 0 that are actually 1. 

We know that the maximum possible value of the F1 score occurs when both precision and sensitivity are 1. If we plug these values into our F1 score equation, we can see that the upper bound is 1:
$F1\ score = \frac{2*1*1}{1+1} = \frac{2}{2} = 1$

Similarly, we know that the minimum possible value of the F1 score occurs when either precision or sensitivity equals 0 because the numerator in the equation ends up being 0. We can see that our lower bound is therefore 0:
$F1\ score = \frac{2*0*1}{1+0} = \frac{0}{1} = 0$

Therefore, the bounds for an F1 score are [0,1].


### ROC Curve
Function `roc_score` that generates an ROC curve from a data set with a true classification column (class in our example) and a probability column (scored.probability in our example). Your function should return a list that includes the plot of the ROC curve and a vector that contains the calculated area under the curve (AUC). Note that I recommend using a sequence of thresholds ranging from 0 to 1 at 0.01 intervals.


```{r roc, echo=TRUE}
# function for roc score 
roc_score <- function(df){
  if (is.data.frame(df) == FALSE) {
    return("Data set is not a dataframe. Try as.data.frame(dataset).")
  } else {
    
    seqOver <- seq(0,1,0.01)
    finalFrame <- data.frame(threshold=double(), 
                           tpr=double(),
                           fpr=double(),
                           stringsAsFactors=FALSE) 
    
    for (i in seqOver){
      df2 <- df %>%
        mutate(tp = ifelse(class == 1 & pred_class == 1, 1, 0),
               tn = ifelse(class == 0 & pred_class == 0, 1, 0),
               fp = ifelse(class == 0 & pred_class == 1, 1, 0),
               fn = ifelse(class == 1 & pred_class == 0, 1, 0)) %>%
        filter(pred_prob <= i)
      
      if (nrow(df2) > 0){
        tpr <- sum(df2$tp)/ (sum(df2$tp)+sum(df2$fn))
        fpr <- sum(df2$fp)/ (sum(df2$fp)+sum(df2$tn))
        finalFrame <- rbind(finalFrame,data.frame(i, tpr, fpr))
      }
    }

    roc_plt <- ggplot(data=finalFrame, aes(x=fpr, y=tpr)) +
      geom_line() +
      ggtitle("ROC Curve") +
      xlab("False Positive Rate") +
      ylab("True Positive Rate") +
      geom_abline(intercept = 0, slope = 1)
    
    return(roc_plt)
  }
  
}

roc_final <- roc_score(class_data)
roc_final$plot_env$roc_plt

```


### Data Metrics

```{r accuracy_check}
sprintf("The accuracy of the predictions for the classification data set is %1.3f%%.", accuracy(class_data)*100)
```

```{r error_check}
sprintf("The error rate of the predictions for the classification data set is %0.3f.", error(class_data))

sprintf("The accuracy and error rate of the predictions must sum to 1. For the classification data set, there summation equals to %.3f.", accuracy(class_data) + error(class_data))
```

```{r precision_check}
sprintf("The precision of the predictions for the classification data set is %0.3f.", 
precision(class_data))
```

```{r sensitivity_check}
sprintf("The sensitivity of the predictions for the classification data set is %0.3f.", 
sensitivity(class_data))
```

```{r specificity_check}
sprintf("The specificity of the predictions for the classification data set is %0.3f.", 
specificity(class_data))
```

```{r f1_score_check}
sprintf("The F1 score of the predictions for the classification data set is %0.3f.", 
f1_score(class_data))
```

### Caret Package

```{r### Package
