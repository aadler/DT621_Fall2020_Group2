---
title: 'DATA 621---Business Analytics and Data Mining'
subtitle: 'Fall 2020---Group 2---Homework #1'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "9/27/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loadData, include=FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(ggplot2)
library(corrplot)
library(data.table)
library(tidymodels)
library(mice)
library(psych)
library(Hmisc)
train_set <- read.csv('https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/HW1/data/moneyball-training-data.csv')
eval_set <- read.csv('https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/HW1/data/moneyball-evaluation-data.csv')

set.seed(9450)
```


## DATA EXPLORATION

### How often does the team win?

We are given a data set of 2,276 records containing 15 seasonal statistics and the
total number of wins a team had in a given year. On average, about 50% of games played 
are won (81 games out of 162), with the best individual season having 146 wins and the worst 
season having 0 wins. The data is normally distributed and most years have between 49 
and 112 wins (blue lines below). The nature of the distribution means there aren't too 
many extreme seasons where wins are significantly higher or lower than usual. This serves 
as a good gut-check for our final predictions; if the predicted wins are too high or too 
low, we know something in our model is probably off. 

```{r wins, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 2.5, fig.width = 5, fig.align = 'center'}

avgWins <- mean(train_set$TARGET_WINS)
sdev <- sd(train_set$TARGET_WINS)
l1 <- mean(train_set$TARGET_WINS) - (2*sdev)
l2 <- mean(train_set$TARGET_WINS) + (2*sdev)

# histogram of wins
p <- ggplot(train_set, aes(x=TARGET_WINS)) + 
  geom_histogram() + 
  geom_vline(aes(xintercept= avgWins),
            color="red", linetype="dashed", size=1)+
  geom_vline(aes(xintercept=l1),
            color="blue", linetype="dashed", size=1) +
    geom_vline(aes(xintercept=l2),
            color="blue", linetype="dashed", size=1) +
  labs(title="Distribution of Wins",x="Number of wins", y = "Number of seasons")

p
```


### What's missing? 
A first look at the data shows that only about 8% of the records have a full set of 
information. The good news is that most of the missing values come from statistics 
that don't happen too often: hit-by-pitch (`TEAM_BATTING_HBP`, 92% missing!), caught 
stealing (`TEAM_BASERUN_CS`, 34% missing), and double plays (`TEAM_FIELDING_DP`, 13% 
missing). Since we have so little hit-by-pitch data, we expect that it doesn't contribute 
much to overall wins and will eliminate it from a few of the models we propose. The 
other two stats have less than half of the data missing, so we'll need to think of a 
clever way to fill in these values. The remaining missing information is from a 
combination of stolen bases and strikeouts by pitchers and batters (`TEAM_BASERUN_SB`,
`TEAM_PITCHING_SO`, `TEAM_BATTING_SO`). **It seems completely unreasonable** to have zero 
strike outs in a season, so this is something we'll most certainly have to impute. 

```{r pctNull, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 2, fig.width = 5, fig.align = 'center'}

pct_null <- data.frame(do.call("rbind", map(train_set %>% select(-INDEX), ~mean(is.na(.)))))
colnames(pct_null) <- c('PCT_NULL')

totalNulls <- pct_null %>%
  mutate(VARIABLE = rownames(.)) %>%
  arrange(desc(PCT_NULL)) %>%
  filter(PCT_NULL > 0) %>%
  select(VARIABLE, PCT_NULL)

ggplot(totalNulls, aes(x=reorder(VARIABLE, PCT_NULL), y=PCT_NULL, label = round(PCT_NULL, 2))) + 
  geom_text(vjust = 0.5, hjust = -0.05)+
  geom_bar(stat = "identity") +
  ggtitle("Variables with Missing Information") +
  xlab("Statistic") + ylab("Percent Missing") + 
  coord_flip() +
  expand_limits(y = 1)

```


### Do the individual stats affect winning?

**Stats with an expected negative impact: ** Intuitively, we expect that Caught stealing, 
Errors, Hits allowed, Home Runs allowed, Strikeouts by batters, and Walks allowed would 
all have a **negative** impact on the total wins. In other words, as these values increase, 
we expect that the team is less likely to win.  

  
```{r negFactors, echo = FALSE, message = FALSE, warning = FALSE}

negSet <- train_set %>%
  select(TARGET_WINS, 
         'Errors' = TEAM_FIELDING_E, 
         'Hits allowed' = TEAM_PITCHING_H, 
         'Strikeouts by batters' = TEAM_BATTING_SO, 
         'Caught stealing' = TEAM_BASERUN_CS, 
         'Walks allowed' = TEAM_PITCHING_BB, 
         'Home Runs allowed' = TEAM_PITCHING_HR)

ggplot(data = negSet %>%
  gather(-TARGET_WINS, key = "STAT", value = "VALUE"), aes(x = VALUE, y = TARGET_WINS)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title="Number of wins with respect to recorded stats",x="", y = "Number of Wins") +
  facet_wrap(~ STAT, scales = "free")
```
  

When we take a closer look at the data, these negative relationships aren't obvious. In 
fact, only **Errors** and **Hits allowed** seem to have a negative impact on wins. **Caught**
**stealing** and **Strikeouts by batters** appear to be random; this means that whether the 
stat for a particular season is high or low doesn't affect the overall number of wins. 
  
Even more interestingly, **Home Runs allowed** and **Walks allowed** have the *opposite* 
effect; as these stats increase, so do the number of wins! 
  
  
**Stats with an expected positive impact: ** We can look at the same information for the 
stats that we expect to have a **positive** effect on wins: Base hits, Doubles, Triples, Home 
Runs, Walks, Batters getting hit by pitches, Stolen bases, Double Plays, and Strikeouts by 
pitchers.  
  

```{r posFactors, echo = FALSE, message = FALSE, warning = FALSE}

posSet <- train_set %>%
  select(TARGET_WINS, 
         'Base hits' = TEAM_BATTING_H, 
         'Doubles' = TEAM_BATTING_2B, 
         'Triples' = TEAM_BATTING_3B, 
         'Home Runs' = TEAM_BATTING_HR, 
         'Walks' = TEAM_BATTING_BB, 
         'Batters hit' = TEAM_BATTING_HBP,
         'Stolen bases' = TEAM_BASERUN_SB,
         'Double plays' = TEAM_FIELDING_DP,
         'Strikeouts' = TEAM_PITCHING_SO)

ggplot(data = posSet %>%
  gather(-TARGET_WINS, key = "STAT", value = "VALUE"), aes(x = VALUE, y = TARGET_WINS)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title="Number of wins with respect to recorded stats",x="", y = "Number of Wins") +
  facet_wrap(~ STAT, scales = "free")

```

  
Many of these stats *do* seem to have an effect on the number of wins, most notably,
**Base hits** and **Walks**. We see weaker positive relationships for **Home runs**, 
**Doubles**, **Triples**, and **Stolen bases**. This makes sense when we think about it; 
these things tend to happen less often in games than pure base hits and walks, so they don't 
have as much of an effect on winning. Finally, **Double plays** and **Batters hit** don't 
appear to have any correlation with the number of wins. Once again, this intuitively makes 
sense because they are less likely to happen in a game. 
  
One thing to note is the number of strikeouts compared to the number of wins. We can see
that there are a few outliers (abnormally high numbers of strikeouts in a season). This
should be taken with caution, as they don't represent a typical season's stats. 


### Are some stats more skewed than others?

Before using any of the statistics in a model, we need to take a closer look at the variation 
in the data. We call out-of-the-ordinary values (exceptionally high or low values) **outliers**. 
We need to take these into account in our modeling because we want to make sure our predictions 
aren't skewed because of them.


```{r summaryInfo, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 3, fig.width = 5, fig.align = 'center'}

#summary(train_set)

totalOutliers <- data.frame(sapply(train_set %>% select(-INDEX, -TARGET_WINS, - TEAM_BATTING_HBP), 
       function(x){length(boxplot.stats(x)$out)/nrow(train_set)}))

totalOutliers$VARIABLE_NM <- rownames(totalOutliers)
colnames(totalOutliers) <- c('PCT_OUTLIERS', 'VARIABLE_NM')

ggplot(totalOutliers , aes(x=reorder(VARIABLE_NM, PCT_OUTLIERS), y=PCT_OUTLIERS, label = round(PCT_OUTLIERS, 3))) + 
  geom_text(vjust = 0.5, hjust = -0.05)+
  geom_bar(stat = "identity") +
  ggtitle("Percent Outliers") +
  xlab("Statistic") + ylab("Percent of Data that is an Outlier") + 
  coord_flip() +
  expand_limits(y = 0.15)


```

As we can see, some of the provided statistics are well-balanced in the sense that there are very 
*few* (or no) extreme values. **Home Runs allowed** (`TEAM_PITCHING_HR`), **Strikeouts by batters** 
(`TEAM_BATTING_SO`), and **Home Runs by batters** (`TEAM_BATTING_HR`) are examples of this. 

```{r fewOutliers, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 2, fig.width = 6, fig.align = 'center'}

set1 <-  train_set %>%
  select('Home Runs by batters' = TEAM_BATTING_HR, 
         'Strikeouts by batters' = TEAM_BATTING_SO,  
         'Home Runs allowed' = TEAM_PITCHING_HR) %>%
  gather("stat", "value") %>%
  filter(complete.cases(.) == TRUE)

vals <- ggplot(set1 , aes(x=stat, y=value)) + 
    geom_boxplot() +
  labs(title="",x="", y = "Value") +
  facet_wrap(~stat, scale="free")

vals

```
Some things to note about each of these statistics:

* **Home Runs allowed** (average = ~100/year) and **Home Runs by batters** (average = ~106/year) 
have a very similar mid-range distribution (50% of the data lies between ~50 and 150). 
The slight difference in average stats means that teams tend to have a higher number
of Home Runs than the opposition team. 
* The only thing that stands out about **Strikeouts by batters** (average = ~736/year) is 
how nearly perfectly normal it is. 50% of the data is between about 500 and 1000 and 
there are absolutely no outliers in the dataset! This means that there were no surprisingly
high or low seasons.   
  
  
Conversely, some of the stats have a very *high* number of outliers, indicating that there
are some seasons with some abnormally high or low values. **Errors** (`TEAM_FIELDING_E`), 
**Hits allowed** (`TEAM_PITCHING_H`), **Walks by batters** (`TEAM_BATTING_BB`), and 
**Stolen bases** (`TEAM_BASERUN_SB`) are examples of this. 

```{r manyOutliers, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 2, fig.width = 6, fig.align = 'center'}

set2 <-  train_set %>%
  select('Walks by batters' = TEAM_BATTING_BB, 
         'Stolen bases' = TEAM_BASERUN_SB,  
         'Hits allowed' = TEAM_PITCHING_H,
         'Errors' = TEAM_FIELDING_E) %>%
  gather("stat", "value") %>%
  filter(complete.cases(.) == TRUE)

vals2 <- ggplot(set2 , aes(x=stat, y=value)) + 
    geom_boxplot() +
  labs(title="",x="", y = "Value") +
  facet_wrap(~ stat, scale = 'free', ncol = 4)

vals2

```
Some things to note about each of these statistics: 

* All of the outliers for **Errors**, **Hits allowed**, and **Stolen bases** are above the upper 
tail of the data set. This is further illustrated by the mean and median values for these stats; in 
all instances, the mean per year (Errors = ~246/year, Hits allowed = ~1779/year, Stolen bases = ~125/year) 
are higher than the median per year (Errors = ~159/year, Hits allowed = ~1518/year, Stolen bases = 
~101/year). This means that some seasons with exceptionally high values skew the dataset. 
* There are a few *very* extreme outliers for **Hits allowed**. The maximum value is 30,132, which
is over 16 times the average number of hits allowed per season!
* There are outliers both above *and* below the tails of the data for the **Walks by batters** stat. 
This means that we have exceptionally low (min = 0!) and exceptionally high (max = 878) seasons. 
  
  
The remaining stats, **Walks allowed** (`TEAM_PITCHING_BB`), **Base Hits by batters** (`TEAM_BATTING_H`), 
**Caught stealing** (`TEAM_BASERUN_CS`), **Strikeouts by pitchers** (`TEAM_PITCHING_SO`), **Double plays** 
(`TEAM_FIELDING_DP`), **Triples by batters** (`TEAM_BATTING_3B`), and **Doubles by batters** 
(`TEAM_BATTING_2B`) have between 29 and 99 outliers. 

```{r otherOutliers, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 3.5, fig.width = 7, fig.align = 'center'}

set3 <-  train_set %>%
  select('Base Hits by batters' = TEAM_BATTING_H, 
         'Doubles by batters' = TEAM_BATTING_2B,  
         'Triples by batters' = TEAM_BATTING_3B,
         'Caught stealing' = TEAM_BASERUN_CS,
         'Walks allowed' = TEAM_PITCHING_BB,
         'Strikeouts by pitchers' = TEAM_PITCHING_SO,
         'Double plays' = TEAM_FIELDING_DP) %>%
  gather("stat", "value") %>%
  filter(complete.cases(.) == TRUE)

vals3 <- ggplot(set3 , aes(x=stat, y=value)) + 
    geom_boxplot() +
  labs(title="",x="", y = "Value") +
  facet_wrap(~ stat, scale = 'free', ncol = 4)

vals3

```

Some things to note: 

* All variables are very narrowly distributed, meaning that most of the data falls within
a small range. 
* **Strikeouts by pitchers** and **Walks allowed** have a few very extreme outliers; these 
represent seasons that have abnormally high values for the statistics. 
* The average number of pure **Base Hits** (1469/season) is greater than the average number
of Doubles (~241/season) and Triples (~55/season). This isn't at all surprising, but serves 
as a good gut check on the validity of the data. 


### Are stats correlated? 

We would expect that a few things in the dataset might be correlated: perhaps number of errors and
hits/homeruns allowed or the number of base hits by batters and doubles/triples/homeruns. We can
visualize the correlations between the statistics to determine if there is a significant relationship
between the them: blue dots represent positively correlated variables (as one increases,
so does the other) and red dots represent negatively correlated variables (as one increases,
the other decreases).

```{r, echo = FALSE, message = FALSE, warning = FALSE}

# Correlation matrix with significance levels (p-value)
res2 <- rcorr(as.matrix(train_set %>% select(-INDEX)))

# Insignificant correlation are crossed
corrplot(res2$r, type="upper", order="hclust", 
          tl.col = "black", p.mat = res2$P, sig.level = 0.01, insig = "blank")


```

Some noteworthy relationships (coincidental or not):

* **Errors** are highly, negatively correlated with walks by batters, strikeouts by batters, and
homeruns (both by batters and allowed). 
* **Triples by batters** are highly, negatively correlated with strikeouts by batters and homeruns
(both by batters and allowed).
* **Strikeouts by batters** are highly, positively correlated with homeruns (both by batters and
allowed). 
* **Homeruns by batters** and **Homeruns allowed** are both positively correlated with walks by 
batters.
* As expected, **basehits** are positively correlated with doubles and triples.

We can keep these correlations in mind when developing our models: if we have correlated statistics, 
there could be in-built redundancy in the features, and we may be able to create a simpler, more
accurate model by eliminating some.  



## DATA PREPARATION

Now we have a good idea of the data we are looking at we can take the next steps to 
prepare it for building a solid model.  We saw from the above that at least one 
variable should be omitted for low reporting of data. There are also outlier concerns 
for several of the variables, namely Team Pitching Strikeouts (`TEAM_PITCHING_SO`), 
Team Batting Hits (`TEAM_BATTING_H`) and Team Pitching Hits (`TEAM_PITCHING_H`). 

First, we will look into obvious outliers in the data set and remove them. There appears 
to be enough data in the training set to take a naive yet cautious approach to outlier removal. 


```{r load, echo=FALSE}
train_set <-
  train_set %>%
  select(-c(TEAM_BATTING_HBP, INDEX)) %>%
  mutate_if(is.integer, as.numeric)
```


### Isolate base hits. 

The variable Team Batting Hits (`TEAM_BATTING_H`) is a combination of all hits for 
the season. So we can cleanly use the Second base ('TEAM_BATTING_2B'), Third Base 
('TEAM_BATTING_3B') and Home Run ('TEAM_BATTING_HR') variables we create a First base variable
('TEAM_BATTING_1B') by subtracting the 2B, 3B, and HR respectively from the Team Batting Hits total. 

~Do we need a graph here?

```{r singles, echo=FALSE}
train_set<- 
    train_set %>%
    mutate(TEAM_BATTING_1B =
           TEAM_BATTING_H -
           (TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR)) 
  
```


### Outlier Removal

When performing the processes of outlier removal a cautious approach is always best.  Each 
outlier is evaluated to ensure its clearly from incorrectly or misentered data, the removal 
does not affect our later assumptions, and the outlier creates a significant association or relation 
at without it the relation is no longer present.   

There are some clear issues with two of the variables.  Since both appear with a heavy right skew, we 
will use median and IQR to remove the outliers.

* TEAM_PITCHING_SO
  +We discussed earlier Team Pitching Strikeouts.
* TEAM_PITCHING_H 
  + Team Pitching Hits has a value over 30,000 and two over 20,000
  + These values seem highly unlikely and will be adjusted


```{r outliers_1, echo=FALSE}
train_set %>%
  pivot_longer(everything(), 
               names_to ="Variable", 
               values_to="Value") %>%
  ggplot(aes(x=Variable, y=Value)) +
  geom_boxplot(na.rm = TRUE) + coord_flip()
```
  
```{r iqr_removal_SO, echo=FALSE}
train_set<-
train_set %>%
  na.omit() %>%
  summarise(iqr = IQR(TEAM_PITCHING_SO)) %>%
  bind_cols(
            train_set 
            )  %>%
  filter(
    TEAM_PITCHING_SO > quantile(TEAM_PITCHING_SO,
                                probs=c(0.25), 
                                na.rm = TRUE) - 1.5*iqr, 
    TEAM_PITCHING_SO < quantile(TEAM_PITCHING_SO, 
                                probs=c(0.75), 
                                na.rm=TRUE) + 1.5*iqr
        ) %>%
  select(-iqr)
```
  
```{r iqr_removal_H, echo=FALSE}
train_set<-
train_set %>%
  na.omit() %>%
  summarise(iqr = IQR(TEAM_PITCHING_H)) %>%
  bind_cols(
            train_set 
            )  %>%
  filter(
    TEAM_PITCHING_H > quantile(TEAM_PITCHING_H,
                                probs=c(0.25), 
                                na.rm = TRUE) - 1.5*iqr, 
    TEAM_PITCHING_H < quantile(TEAM_PITCHING_H, 
                                probs=c(0.75), 
                                na.rm=TRUE) + 1.5*iqr
        ) %>%
  select(-iqr)

```
  
```{r outlier_boxplot, echo=FALSE}
train_set %>%
  pivot_longer(everything(), 
               names_to ="Variable", 
               values_to="Value") %>%
  ggplot(aes(x=Variable, y=Value)) +
  geom_boxplot(na.rm = TRUE) + coord_flip()
```
  
This has eliminated what appears to be the most extreme outliers. There are 
still large outlier sets for Errors and stolen bases, but none that seem to 
dwarf the other variables. According to the box plot, things appear to be on a 
scale that seems logical given the data and source of information that we have. 

### "MICE" imputation method and Feature Engineering

From our early exploration of the data, the vast majority of the data is complete 
with only a few variables with missing values. We will use the "mice" imputation method, 
specifically predictive mean matching method.

MICE (Multivariate imputation by chained equations) is a principled method of dealing 
with missing data.  It creates multiple imputations, as opposed to single imputations 
and accounts for the statistical uncertainty in the imputations.  It creates predictive 
values for the mean instead of imputing the IQR values.  

[described here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/#:~:text=Missing%20data%20are%20a%20common,method%20of%20addressing%20missing%20data.).  
```{r impute_train,echo=FALSE, message=FALSE, warning=FALSE}
train_set <- complete(mice(data = train_set, 
                         method = "pmm", 
                         seed = 9450, 
                         print=FALSE), 3)
```

With outliers removed and missing values imputed via MICE, we can build a feature, 
specifically one for slugging. 

### Slugging Percentage

~Why are we building a feature?

Slugging is calculated in the following way

${\displaystyle \mathrm {SLG} ={\frac {({\mathit {1B}})+(2\times {\mathit {2B}})+(3\times {\mathit {3B}})+(4\times {\mathit {HR}})}{AB}}}$

We will take the same approximation for at-bats. 

${\displaystyle \mathrm {\sim SLG} ={\frac {({\mathit {1B}})+(2\times {\mathit {2B}})+(3\times {\mathit {3B}})+(4\times {\mathit {HR}})}{SO_{batting} + H + BB}}}$


```{r make_slg_obp, echo=FALSE}
train_set<-
train_set %>%
  mutate(AB = TEAM_BATTING_SO + TEAM_BATTING_BB + TEAM_BATTING_H,
         SLG = (TEAM_BATTING_1B + (2*TEAM_BATTING_2B)+ (3*TEAM_BATTING_3B)+ (4*TEAM_BATTING_HR))/AB) %>%
  # Removing AB and total hits
  select(-c(AB, TEAM_BATTING_H))
```


## BUILD MODELS

### Base Model
We will start with a simple linear model and select out features based on null hypothesis \
testing for non-zero slope. 

Below is the output of coefficients for backward selection model. 

```{r lm_first, echo=FALSE, eval=FALSE}
lm_reg = lm(data=train_set, TARGET_WINS ~ .)
summary(lm_reg)
```


```{r lm_selected, echo=FALSE}
lm_1<- lm(TARGET_WINS ~ TEAM_BATTING_2B +TEAM_BATTING_HR+
          TEAM_BATTING_BB+TEAM_BATTING_SO+ TEAM_BASERUN_SB+TEAM_BASERUN_CS+
          TEAM_PITCHING_HR+ TEAM_PITCHING_H+
          TEAM_FIELDING_E+ TEAM_FIELDING_DP+  SLG, data=train_set)
knitr::kable(lm_1$coefficients, digits = 3L)
```


## SELECT MODELS
