---
title: 'DATA 621---Business Analytics and Data Mining'
subtitle: 'Fall 2020---Group 2---Homework #1'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "9/27/2020"
output:
  pdf_document: default
  html_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loadData, include=FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(ggplot2)
library(corrplot)
library(data.table)
library(tidymodels)
library(mice)
library(psych)
library(Hmisc)
library(MASS)
library(dplyr)
library(caret)
train_set <- read.csv('https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/HW1/data/moneyball-training-data.csv')
eval_set <- read.csv('https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/HW1/data/moneyball-evaluation-data.csv')
set.seed(9450)
```


## DATA EXPLORATION

### How often does the team win?

We are given a data set of 2,276 records containing 15 seasonal statistics and the
total number of wins a team had in a given year. On average, about 50% of games played 
are won (81 games out of 162), with the best individual season having 146 wins and the worst 
season having 0 wins. The data is normally distributed and most years have between 49 
and 112 wins (blue lines below). The nature of the distribution means there aren't too 
many extreme seasons where wins are significantly higher or lower than usual. This serves 
as a good gut-check for our final predictions; if the predicted wins are too high or too 
low, we know something in our model is probably off. 

```{r wins, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 2.5, fig.width = 5, fig.align = 'center'}
avgWins <- mean(train_set$TARGET_WINS)
sdev <- sd(train_set$TARGET_WINS)
l1 <- mean(train_set$TARGET_WINS) - (2*sdev)
l2 <- mean(train_set$TARGET_WINS) + (2*sdev)
# histogram of wins
p <- ggplot(train_set, aes(x=TARGET_WINS)) + 
  geom_histogram() + 
  geom_vline(aes(xintercept= avgWins),
            color="red", linetype="dashed", size=1)+
  geom_vline(aes(xintercept=l1),
            color="blue", linetype="dashed", size=1) +
    geom_vline(aes(xintercept=l2),
            color="blue", linetype="dashed", size=1) +
  labs(title="Distribution of Wins",x="Number of wins", y = "Number of seasons")
p
```


### What's missing? 
A first look at the data shows that only about 8% of the records have a full set of 
information. The good news is that most of the missing values come from statistics 
that don't happen too often: hit-by-pitch (`TEAM_BATTING_HBP`, 92% missing!), caught 
stealing (`TEAM_BASERUN_CS`, 34% missing), and double plays (`TEAM_FIELDING_DP`, 13% 
missing). Since we have so little hit-by-pitch data, we expect that it doesn't contribute 
much to overall wins and will eliminate it from a few of the models we propose. The 
other two stats have less than half of the data missing, so we'll need to think of a 
clever way to fill in these values. The remaining missing information is from a 
combination of stolen bases and strikeouts by pitchers and batters (`TEAM_BASERUN_SB`,
`TEAM_PITCHING_SO`, `TEAM_BATTING_SO`). **It seems completely unreasonable** to have zero 
strike outs in a season, so this is something we'll most certainly have to impute. 

```{r pctNull, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 2, fig.width = 5, fig.align = 'center'}
pct_null <- data.frame(do.call("rbind", map(train_set %>% dplyr::select(-INDEX), ~mean(is.na(.)))))
colnames(pct_null) <- c('PCT_NULL')
totalNulls <- pct_null %>%
  mutate(VARIABLE = rownames(.)) %>%
  arrange(desc(PCT_NULL)) %>%
  filter(PCT_NULL > 0) %>%
  dplyr::select(VARIABLE, PCT_NULL)
ggplot(totalNulls, aes(x=reorder(VARIABLE, PCT_NULL), y=PCT_NULL, label = round(PCT_NULL, 2))) + 
  geom_text(vjust = 0.5, hjust = -0.05)+
  geom_bar(stat = "identity") +
  ggtitle("Variables with Missing Information") +
  xlab("Statistic") + ylab("Percent Missing") + 
  coord_flip() +
  expand_limits(y = 1)
```


### Do the individual stats affect winning?

**Stats with an expected negative impact: ** Intuitively, we expect that Caught stealing, 
Errors, Hits allowed, Home Runs allowed, Strikeouts by batters, and Walks allowed would 
all have a **negative** impact on the total wins. In other words, as these values increase, 
we expect that the team is less likely to win.  

  
```{r negFactors, echo = FALSE, message = FALSE, warning = FALSE}
negSet <- train_set %>%
  dplyr::select(TARGET_WINS, 
         'Errors' = TEAM_FIELDING_E, 
         'Hits allowed' = TEAM_PITCHING_H, 
         'Strikeouts by batters' = TEAM_BATTING_SO, 
         'Caught stealing' = TEAM_BASERUN_CS, 
         'Walks allowed' = TEAM_PITCHING_BB, 
         'Home Runs allowed' = TEAM_PITCHING_HR)
ggplot(data = negSet %>%
  gather(-TARGET_WINS, key = "STAT", value = "VALUE"), aes(x = VALUE, y = TARGET_WINS)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title="Number of wins with respect to recorded stats",x="", y = "Number of Wins") +
  facet_wrap(~ STAT, scales = "free")
```
  

When we take a closer look at the data, these negative relationships aren't obvious. In 
fact, only **Errors** and **Hits allowed** seem to have a negative impact on wins. **Caught**
**stealing** and **Strikeouts by batters** appear to be random; this means that whether the 
stat for a particular season is high or low doesn't affect the overall number of wins. 
  
Even more interestingly, **Home Runs allowed** and **Walks allowed** have the *opposite* 
effect; as these stats increase, so do the number of wins! 
  
  
**Stats with an expected positive impact: ** We can look at the same information for the 
stats that we expect to have a **positive** effect on wins: Base hits, Doubles, Triples, Home 
Runs, Walks, Batters getting hit by pitches, Stolen bases, Double Plays, and Strikeouts by 
pitchers.  
  

```{r posFactors, echo = FALSE, message = FALSE, warning = FALSE}
posSet <- train_set %>%
  dplyr::select(TARGET_WINS, 
         'Base hits' = TEAM_BATTING_H, 
         'Doubles' = TEAM_BATTING_2B, 
         'Triples' = TEAM_BATTING_3B, 
         'Home Runs' = TEAM_BATTING_HR, 
         'Walks' = TEAM_BATTING_BB, 
         'Batters hit' = TEAM_BATTING_HBP,
         'Stolen bases' = TEAM_BASERUN_SB,
         'Double plays' = TEAM_FIELDING_DP,
         'Strikeouts' = TEAM_PITCHING_SO)
ggplot(data = posSet %>%
  gather(-TARGET_WINS, key = "STAT", value = "VALUE"), aes(x = VALUE, y = TARGET_WINS)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title="Number of Wins with Respect to Recorded Stats",x="", y = "Number of Wins") +
  facet_wrap(~ STAT, scales = "free")
```

  
Many of these stats *do* seem to have an effect on the number of wins, most notably,
**Base hits** and **Walks**. We see weaker positive relationships for **Home runs**, 
**Doubles**, **Triples**, and **Stolen bases**. This makes sense when we think about it; 
these things tend to happen less often in games than pure base hits and walks, so they don't 
have as much of an effect on winning. Finally, **Double plays** and **Batters hit** don't 
appear to have any correlation with the number of wins. Once again, this intuitively makes 
sense because they are less likely to happen in a game. 
  
One thing to note is the number of strikeouts compared to the number of wins. We can see
that there are a few outliers (abnormally high numbers of strikeouts in a season). This
should be taken with caution, as they don't represent a typical season's stats. 


### Are some stats more skewed than others?

Before using any of the statistics in a model, we need to take a closer look at the variation 
in the data. We call out-of-the-ordinary values (exceptionally high or low values) **outliers**. 
We need to take these into account in our modeling because we want to make sure our predictions 
aren't skewed because of them.


```{r summaryInfo, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 3, fig.width = 5, fig.align = 'center'}
#summary(train_set)
totalOutliers <- data.frame(sapply(train_set %>% dplyr::select(-INDEX, -TARGET_WINS, - TEAM_BATTING_HBP), 
       function(x){length(boxplot.stats(x)$out)/nrow(train_set)}))
totalOutliers$VARIABLE_NM <- rownames(totalOutliers)
colnames(totalOutliers) <- c('PCT_OUTLIERS', 'VARIABLE_NM')
ggplot(totalOutliers , aes(x=reorder(VARIABLE_NM, PCT_OUTLIERS), y=PCT_OUTLIERS, label = round(PCT_OUTLIERS, 3))) + 
  geom_text(vjust = 0.5, hjust = -0.05)+
  geom_bar(stat = "identity") +
  ggtitle("Percentage of Outliers") +
  xlab("Statistic") + ylab("Percent of Data that is an Outlier") + 
  coord_flip() +
  expand_limits(y = 0.15)
```

As we can see, some of the provided statistics are well-balanced in the sense that there are very 
*few* (or no) extreme values. **Home Runs allowed** (`TEAM_PITCHING_HR`), **Strikeouts by batters** 
(`TEAM_BATTING_SO`), and **Home Runs by batters** (`TEAM_BATTING_HR`) are examples of this. 

```{r fewOutliers, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 2, fig.width = 6, fig.align = 'center'}
set1 <-  train_set %>%
  dplyr::select('Home Runs by batters' = TEAM_BATTING_HR, 
         'Strikeouts by batters' = TEAM_BATTING_SO,  
         'Home Runs allowed' = TEAM_PITCHING_HR) %>%
  gather("stat", "value") %>%
  filter(complete.cases(.) == TRUE)
vals <- ggplot(set1 , aes(x=stat, y=value)) + 
    geom_boxplot() +
  labs(title="",x="", y = "Value") +
  facet_wrap(~stat, scale="free")
vals
```
Some things to note about each of these statistics:

* **Home Runs allowed** (average = ~100/year) and **Home Runs by batters** (average = ~106/year) 
have a very similar mid-range distribution (50% of the data lies between ~50 and 150). 
The slight difference in average stats means that teams tend to have a higher number
of Home Runs than the opposition team. 
* The only thing that stands out about **Strikeouts by batters** (average = ~736/year) is 
how nearly perfectly normal it is. 50% of the data is between about 500 and 1000 and 
there are absolutely no outliers in the dataset! This means that there were no surprisingly
high or low seasons.   
  
  
Conversely, some of the stats have a very *high* number of outliers, indicating that there
are some seasons with some abnormally high or low values. **Errors** (`TEAM_FIELDING_E`), 
**Hits allowed** (`TEAM_PITCHING_H`), **Walks by batters** (`TEAM_BATTING_BB`), and 
**Stolen bases** (`TEAM_BASERUN_SB`) are examples of this. 

```{r manyOutliers, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 2, fig.width = 6, fig.align = 'center'}
set2 <-  train_set %>%
  dplyr::select('Walks by batters' = TEAM_BATTING_BB, 
         'Stolen bases' = TEAM_BASERUN_SB,  
         'Hits allowed' = TEAM_PITCHING_H,
         'Errors' = TEAM_FIELDING_E) %>%
  gather("stat", "value") %>%
  filter(complete.cases(.) == TRUE)
vals2 <- ggplot(set2 , aes(x=stat, y=value)) + 
    geom_boxplot() +
  labs(title="",x="", y = "Value") +
  facet_wrap(~ stat, scale = 'free', ncol = 4)
vals2
```
Some things to note about each of these statistics: 

* All of the outliers for **Errors**, **Hits allowed**, and **Stolen bases** are above the upper 
tail of the data set. This is further illustrated by the mean and median values for these stats; in 
all instances, the mean per year (Errors = ~246/year, Hits allowed = ~1779/year, Stolen bases = ~125/year) 
are higher than the median per year (Errors = ~159/year, Hits allowed = ~1518/year, Stolen bases = 
~101/year). This means that some seasons with exceptionally high values skew the dataset. 
* There are a few *very* extreme outliers for **Hits allowed**. The maximum value is 30,132, which
is over 16 times the average number of hits allowed per season!
* There are outliers both above *and* below the tails of the data for the **Walks by batters** stat. 
This means that we have exceptionally low (min = 0!) and exceptionally high (max = 878) seasons. 
  
  
The remaining stats, **Walks allowed** (`TEAM_PITCHING_BB`), **Base Hits by batters** (`TEAM_BATTING_H`), 
**Caught stealing** (`TEAM_BASERUN_CS`), **Strikeouts by pitchers** (`TEAM_PITCHING_SO`), **Double plays** 
(`TEAM_FIELDING_DP`), **Triples by batters** (`TEAM_BATTING_3B`), and **Doubles by batters** 
(`TEAM_BATTING_2B`) have between 29 and 99 outliers. 

```{r otherOutliers, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 3.5, fig.width = 7, fig.align = 'center'}
set3 <-  train_set %>%
  dplyr::select('Base Hits by batters' = TEAM_BATTING_H, 
         'Doubles by batters' = TEAM_BATTING_2B,  
         'Triples by batters' = TEAM_BATTING_3B,
         'Caught stealing' = TEAM_BASERUN_CS,
         'Walks allowed' = TEAM_PITCHING_BB,
         'Strikeouts by pitchers' = TEAM_PITCHING_SO,
         'Double plays' = TEAM_FIELDING_DP) %>%
  gather("stat", "value") %>%
  filter(complete.cases(.) == TRUE)
vals3 <- ggplot(set3 , aes(x=stat, y=value)) + 
    geom_boxplot() +
  labs(title="",x="", y = "Value") +
  facet_wrap(~ stat, scale = 'free', ncol = 4)
vals3
```

Some things to note: 

* All variables are very narrowly distributed, meaning that most of the data falls within
a small range. 
* **Strikeouts by pitchers** and **Walks allowed** have a few very extreme outliers; these 
represent seasons that have abnormally high values for the statistics. 
* The average number of pure **Base Hits** (1469/season) is greater than the average number
of Doubles (~241/season) and Triples (~55/season). This isn't at all surprising, but serves 
as a good gut check on the validity of the data. 


### Are stats correlated? 

We would expect that a few things in the dataset might be correlated: perhaps number of errors and
hits/homeruns allowed or the number of base hits by batters and doubles/triples/homeruns. We can
visualize the correlations between the statistics to determine if there is a significant relationship
between the them: blue dots represent positively correlated variables (as one increases,
so does the other) and red dots represent negatively correlated variables (as one increases,
the other decreases).

```{r corrPlot, echo = FALSE, message = FALSE, warning = FALSE}
# Correlation matrix with significance levels (p-value)
res2 <- rcorr(as.matrix(train_set %>% dplyr::select(-INDEX)))
# Insignificant correlation are crossed
corrplot(res2$r, type="upper", order="hclust", 
          tl.col = "black", p.mat = res2$P, sig.level = 0.01, insig = "blank")
```

Some noteworthy relationships (coincidental or not):

* **Errors** are highly, negatively correlated with walks by batters, strikeouts by batters, and
homeruns (both by batters and allowed). 
* **Triples by batters** are highly, negatively correlated with strikeouts by batters and homeruns
(both by batters and allowed).
* **Strikeouts by batters** are highly, positively correlated with homeruns (both by batters and
allowed). 
* **Homeruns by batters** and **Homeruns allowed** are both positively correlated with walks by 
batters.
* As expected, **basehits** are positively correlated with doubles and triples.

We can keep these correlations in mind when developing our models: if we have correlated statistics, 
there could be in-built redundancy in the features, and we may be able to create a simpler, more
accurate model by eliminating some.  



## DATA PREPARATION

Now we have a good idea of the data we are looking at we can take the next steps to 
prepare it for building a solid model. 

### Outlier Removal

As we saw in the data analysis, there are some outlier concerns for some of the variables,
so we will need to account for this in our modeling. When performing the processes of outlier 
removal, a cautious approach is always best. Each outlier is evaluated to ensure:

* It is clearly from incorrectly or mis-centered data.
* Its removal does not affect later assumptions.
* It creates a significant association/relation that is eliminated with its removal.

We can take another look at the outliers for each variable: 

```{r outliers_1, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 4, fig.width = 8, fig.align = 'center'}
train_set %>%
  dplyr::select(-TARGET_WINS, -INDEX) %>%
  pivot_longer(everything(), 
               names_to ="Variable", 
               values_to="Value") %>%
  ggplot(aes(x=Variable, y=Value)) +
  geom_boxplot(na.rm = TRUE) + 
  labs(title="Outliers for each Statistic",x="Variable", y = "Value") +
  coord_flip()
```
 
Off the bat, there are some clear issues with two of the variables: `TEAM_PITCHING_SO` and
`TEAM_PITCHING_H.` 

* **TEAM_PITCHING_SO**: The largest outlier is close to 20,000, which averages ~123 strikeouts
per game. The MLB team pitching strikeout record is 1,450 and set in 2014 (CLE). There are a 
minimum of 27 at-bats per game and most teams average ~35 at-bats, thus making 123 strikeouts
impossible. 
* **TEAM_PITCHING_H**: The largest outlier for this vaiable is over 30,000 which is highly 
unlikely. 

Since both appear with a heavy right skew, we will use median and IQR to remove the outliers.

```{r iqr_removal_SO, include=FALSE}
train_set<- train_set %>%
  na.omit() %>%
  summarise(iqr = IQR(TEAM_PITCHING_SO)) %>%
  bind_cols(
            train_set 
            )  %>%
  filter(
    TEAM_PITCHING_SO > quantile(TEAM_PITCHING_SO,
                                probs=c(0.25), 
                                na.rm = TRUE) - 1.5*iqr, 
    TEAM_PITCHING_SO < quantile(TEAM_PITCHING_SO, 
                                probs=c(0.75), 
                                na.rm=TRUE) + 1.5*iqr
        ) %>%
  dplyr::select(-iqr)
```
  
```{r iqr_removal_H, include=FALSE}
train_set<- train_set %>%
  na.omit() %>%
  summarise(iqr = IQR(TEAM_PITCHING_H)) %>%
  bind_cols(
            train_set 
            )  %>%
  filter(
    TEAM_PITCHING_H > quantile(TEAM_PITCHING_H,
                                probs=c(0.25), 
                                na.rm = TRUE) - 1.5*iqr, 
    TEAM_PITCHING_H < quantile(TEAM_PITCHING_H, 
                                probs=c(0.75), 
                                na.rm=TRUE) + 1.5*iqr
        ) %>%
  dplyr::select(-iqr)
```

We can take a look at the distributions of these variables after outlier removal: 

```{r outlier_boxplot, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 4, fig.width = 8, fig.align = 'center'}
train_set %>%
  dplyr::select(-TARGET_WINS, -INDEX) %>%
  pivot_longer(everything(), 
               names_to ="Variable", 
               values_to="Value") %>%
  ggplot(aes(x=Variable, y=Value)) +
  geom_boxplot(na.rm = TRUE) + 
  labs(title="Outliers for each Statistic",x="Variable", y = "Value") +
  coord_flip()
```
  
This has eliminated what appears to be the most extreme outliers. There are 
still large outlier sets for Errors and stolen bases, but none that seem to 
dwarf the other variables. According to the box plot, things appear to be on a 
scale that seems logical given the data and source of information that we have. 

### "MICE" imputation method 

From our early exploration of the data, the vast majority of the data is complete 
with only a few variables with missing values. We will use the "mice" imputation method, 
specifically predictive mean matching method.

MICE (Multivariate imputation by chained equations) is a principled method of dealing 
with missing data. It creates multiple imputations, as opposed to single imputations 
and accounts for the statistical uncertainty in the imputations. It creates predictive 
values for the mean instead of imputing the IQR values. 

```{r impute_train,echo=FALSE, message=FALSE, warning=FALSE}
train_set <- complete(mice(data = train_set, 
                         method = "pmm", 
                         seed = 9450, 
                         print=FALSE), 3)
```



### Feature Engineering: Single base hits 

With outliers removed and missing values imputed via MICE, we can create a few new 
variables. The first will be single base hits, and it will be derived from some of the
variables we do have. We know that Team Batting Hits (`TEAM_BATTING_H`) is a combination 
of *all* hits for the season, so we can create Single Base Hits as follows: 

$$Singles = Total\ Hits - (Doubles + Triples + Homeruns)$$

```{r singles, include=FALSE}
train_set<- train_set %>%
  dplyr::select(-INDEX) %>%
  mutate_if(is.integer, as.numeric) %>%
  mutate(TEAM_BATTING_1B =
           TEAM_BATTING_H -
           (TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR))
```


### Feature Engineering: Slugging Percentage

The second variable we will create is **Slugging Percentage**. Slugging is an offensive 
statistic that is a good predictor of winning and it tends to have better variance and
correlation than most other variables. It is composed of singles, doubles, triples, 
home runs and at-bats. 

Slugging is calculated in the following way:

$${\displaystyle \mathrm {SLG} ={\frac {({\mathit {1B}})+(2\times {\mathit {2B}})+(3\times {\mathit {3B}})+(4\times {\mathit {HR}})}{AB}}}$$

Because we don't have a statistic for at-bats, we will approximate it as follows:  

$${\displaystyle \mathrm {\sim SLG} ={\frac {({\mathit {1B}})+(2\times {\mathit {2B}})+(3\times {\mathit {3B}})+(4\times {\mathit {HR}})}{SO_{batting} + H + BB}}}$$


```{r make_slg_obp, include=FALSE}
train_set<- train_set %>%
  mutate(AB = TEAM_BATTING_SO + TEAM_BATTING_BB + TEAM_BATTING_H,
         SLG = (TEAM_BATTING_1B + (2*TEAM_BATTING_2B)+ (3*TEAM_BATTING_3B)+ (4*TEAM_BATTING_HR))/AB) 
```

As a gut-check, we can take a look at Slugging in comparison to Triples (`TEAM_BATTING_3B`) 
and Homeruns (`TEAM_BATTING_HR`). All of these variables should have a positive correlation 
with the total number of wins. 

```{r slugging_compare, echo = FALSE, message = FALSE, warning = FALSE, fig.height = 2.5, fig.width = 8, fig.align = 'center'}
modified_set <- train_set %>%
  dplyr::select(TARGET_WINS, SLG, TEAM_BATTING_3B, TEAM_BATTING_HR)
ggplot(data = modified_set %>%
  gather(-TARGET_WINS, key = "STAT", value = "VALUE"), aes(x = VALUE, y = TARGET_WINS)) + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title="Number of wins with respect to recorded stats",x="", y = "Number of Wins") +
  facet_wrap(~ STAT, scales = "free")
```

## BUILD MODELS

### Base Model
We will start with a simple linear model to serve as a baseline. This includes all variables 
in the dataset. 

```{r baseModel, include = FALSE}
lm_reg = lm(data=train_set, TARGET_WINS ~ .)
summary(lm_reg)
```

### Model 1
Model 1 includes all variables except Singles, Triples, Base Hits, Strikeouts by
pitchers, Walks allowed, and Batters Hit by Pitch. The final variables were chosen as 
a result of backwards dplyr::selection based on null hypothesis testing for non-zero slope.  
  
**Further data transformation: **  
We will use Cook's distance to remove outliers that are influencing the fit of the model above. We will use a cutoff of $\frac{4}{N}$. 

```{r cooks_distance, include=FALSE}
lm_1<- lm(TARGET_WINS ~ TEAM_BATTING_2B +TEAM_BATTING_HR+
          TEAM_BATTING_BB+TEAM_BATTING_SO+ TEAM_BASERUN_SB+TEAM_BASERUN_CS+
          TEAM_PITCHING_HR+ TEAM_PITCHING_H+
          TEAM_FIELDING_E+ TEAM_FIELDING_DP+  SLG, data=train_set)
cooks_dis<- cooks.distance(lm_1)
influential<- as.numeric(names(cooks_dis)[(cooks_dis > (4/nrow(train_set)))])
train_df2<- train_set[-influential, ]
lm_1<- lm(TARGET_WINS ~ TEAM_BATTING_2B +TEAM_BATTING_HR+
          TEAM_BATTING_BB+TEAM_BATTING_SO+ TEAM_BASERUN_SB+TEAM_BASERUN_CS+
          TEAM_PITCHING_HR+ TEAM_PITCHING_H+
          TEAM_FIELDING_E+ TEAM_FIELDING_DP+  SLG, data=train_df2)

mod_sum1 = summary(lm_1)  
pred_lm1 = predict(lm_1, newdata = train_df2)
knitr::kable(summary(lm_1)$coefficients, digits = 3L, caption = 'Backwards Model Regression Output')
```

**Coefficient Discussion: **
First, we are keeping the coefficient for `TEAM_BATTING_HR` since it's p-value is marginally above the general threshold and knowledge of the game suggests it is important. There are some counter intuitive results, which is expected given that baseball is a messy, imprecise game that has evolved over time. To that end, we should expect some seemingly strange results from algorithmic regression. We used a combination of domain knowledge and data analysis to justify retaining features. 

Fielding double plays and batting doubles both appear to have negative impacts on wins even though they _should_ be positive impacts. Turning double plays, while a good for the defensive team, may suggest a larger, negative issue. Namely, weak pitching that leads to runners on base. Similarly, batting doubles, leaves runners open to double plays. Allowed hits by pitching, caught stealing, and batting strike-outs are all counter intuitive results as well, but they seem to be small contributors and these are events that happen regularly in every game. 

Slugging as an approximation is the major predictor in this regression, by far. The other predictors other than the intercept are orders of magnitude less in predictive value. It should be noted that slugging alone is not a good predictor for wins overall. 


### Model 2
This is a model allowing pairwise interactions within the data types of baserunning,
batting, pitching, and fielding, fit using a forward and backwards stepwise regression.

```{r model2, include = FALSE}
crazyModel <- lm(TARGET_WINS ~
  (TEAM_BASERUN_CS + TEAM_BASERUN_SB) ^ 2 + (TEAM_BATTING_2B + TEAM_BATTING_3B +
  TEAM_BATTING_BB + TEAM_BATTING_H + TEAM_BATTING_HR + TEAM_BATTING_SO) ^ 2 +
  (TEAM_FIELDING_DP + TEAM_FIELDING_E) ^ 2 + (TEAM_PITCHING_BB +
  TEAM_PITCHING_H + TEAM_PITCHING_HR + TEAM_PITCHING_SO) ^ 2, data = train_set)
stepModel <- stepAIC(crazyModel, scope = list(upper = ~
  (TEAM_BASERUN_CS + TEAM_BASERUN_SB) ^ 2 + (TEAM_BATTING_2B + TEAM_BATTING_3B +
  TEAM_BATTING_BB + TEAM_BATTING_H + TEAM_BATTING_HR + TEAM_BATTING_SO) ^ 2 +
  (TEAM_FIELDING_DP + TEAM_FIELDING_E) ^ 2 + (TEAM_PITCHING_BB +
  TEAM_PITCHING_H + TEAM_PITCHING_HR + TEAM_PITCHING_SO) ^ 2, lower = ~ 1),
  direction = 'both')
sstm <- summary(stepModel)
stmr2 <- sstm$r.squared
stmar2 <- sstm$adj.r.squared
stmAIC <- prettyNum(AIC(stepModel), digits = 2L, big.mark = ',')
stmRMSE <- sqrt(mean(stepModel$residuals ^ 2))
stmF <- prettyNum(sstm$fstatistic, digits = 5L)
stmFp <- pf(sstm$fstatistic[[1]], sstm$fstatistic[[2]], sstm$fstatistic[[3]],
            lower.tail = FALSE)
```


**Coefficient Discussion: **
```{r model2_discuss, echo = FALSE, message = FALSE, warning = FALSE,}
knitr::kable(sstm$coefficients, digits = 3L,
             caption = 'Step Model with Pairwise Class Interactions Output')
```


Most of the coefficients are reasonable within the context of the game of
baseball. These two statements are axiomatic:
  
  * The only way to win is to have the higher score at the end of the game.
  * The only way to score is for a baserunner to cross home plate.

With those in mind, we can make the following observations about the linear
non-interactive coefficients.

 * Reasonable
   * Caught stealing removes baserunners; negative coefficient makes sense.
   * Stolen bases get a runner closer to home plate; positive coefficient makes
   sense.
   * Triples get a runner very close to home plate; positive coefficient makes
   sense.
   * Walks are free baserunners; positive coefficient makes sense.
   * Hits increase the number of baserunners; positive coefficient makes sense.
   * Hitting home runs directly increase the score; positive coefficient makes
   sense.
   * Striking out reduces the number of baserunners; negative coefficient makes
   sense.
   * Errors allow the other team free baserunners; negative coefficient makes
   sense.
   * Giving up home runs gives the opponent scores; negative coefficient makes
   sense.
   * Getting strikeouts reduces the opponents baserunners; positive coefficient
   makes sense.
 * Curious
   * Hitting doubles get runners on base; why negative?
   * Turning double plays reduce baserunners; why negative?
   * Giving up walks allows the other team free baserunners; negative
   coefficient makes sense but why insignificant?
   * Giving up hits allows the other team baserunners; why positive although
   insignificant?

An absolutely fascinating observation. In the first draft of this exercise,
`TEAM_BATTING_HR` was removed due to its high correlation with
`TEAM_PITCHING_HR`. Note the coefficients, batting is almost +12 and pitching is
-9. However, this is such as strong indicator, that in the first run, pitching
was given a *positive* coefficient of around +3. In hindsight this is because
*it was being used as an indicator for* ***batting!!*** The correlation allowed
the use of pitching HRs as an indicator for the hidden batting HRs! Once both
were restored to the model, the logical coefficients surfaced. Another reason
why models should not be trusted out of the box, but all model results should
be reviewed for sanity and sense!!

The curiosities above can *possibly* be resolved by looking at the interaction
terms. The interaction between giving up hits and strikeouts,
`TEAM_PITCHING_H:TEAM_PITCHING_SO `, is highly negative and significant. It
probably captures most of the giving up hits information making the singleton
less relevant.

A possible explanation for the negative coefficient for getting double plays, is
that double plays require at least two people on base. That means that the
opponent has a lot of base runners, which is very highly correlated with
scoring.

The behavior of doubles remains confusing. Unless its hiding something like
a team's propensity to strand runners on base. It would be interesting to see
a breakdown between the American and National leagues, as the latter tends to be
somewhat better at "small ball" and moving the runners along.


### Model 3
The final model is a Stepwise Regression with Repeated k-fold Cross-Validation, and higher order polynomials variables were introduced into the full model.
  
A stepwise variable selection model is conducted to determine what are the variables that can help predict the number of wins for the team. The method allows variables to be added one at a time to the model, as long as the F-statistic is below the specified $\alpha$, in this case $\alpha = 0.05$. However, variables already in the model do not necessarily stay in. The steps evaluate all of the variables already included in the model and remove any variable that has an insignificant F-statistic. Only after this test ends, is the best model found, that is when none of the variables can be excluded and every variable included in the model is significant. 

Here, the dependent variable is the continuous variable, `TARGET_WINS`, and the independent variables are the full model to identify the most contributing predictors. In addition, a robust method for estimating the accuracy of a model, the k-fold cross-validation method, was performed evaluate the model performance on different subset of the training data and then calculate the average prediction error rate.

```{r model3, include = FALSE}
train_df = read.csv('https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/HW1/data/moneyball-training-data.csv')
train_df = subset(train_df, select = -c(INDEX, TEAM_BATTING_HBP))
model = lm(TARGET_WINS ~ ., data = train_df)
# Remove Outlier and more cleaning
cooksd = cooks.distance(model)
influential = as.numeric(names(cooksd)[(cooksd > 4*mean(cooksd, na.rm = TRUE))])
train_df = train_df[-c(influential), ]
train_df.clean = complete(mice(data = train_df, method = "pmm", maxit = 5, 
                               seed = 525, print = FALSE), 3)
train_df.clean$TEAM_BATTING_1B = train_df.clean$TEAM_BATTING_H - 
  (train_df.clean$TEAM_BATTING_2B + train_df.clean$TEAM_BATTING_3B + 
     train_df.clean$TEAM_BATTING_HR)
train_df.clean = subset(train_df.clean, select = -c(TEAM_BATTING_H, TEAM_PITCHING_HR))
# Data Transformation & Multicollinearity Diagnostic Measures
stepdata = cbind(train_df.clean,sapply(train_df.clean[2:length(train_df.clean)], 
                                       function(x) x^2))
names(stepdata) = make.unique(names(stepdata), sep = "_")
# Set up repeated k-fold cross-validation
set.seed(525)
train.control = trainControl(method = "cv", number = 10)
# Train the model
step.model = train(TARGET_WINS ~ ., data = stepdata, method = "lmStepAIC", 
                   trControl = train.control, trace = FALSE)
# Final model coefficients
step.model$finalModel 
# Summary of the model
mod_sum = summary(step.model$finalModel)
# Predictions on train data
pred_step.model = predict(step.model, newdata = stepdata)
```

**Coefficient Discussion: **
```{r model3_discuss, echo=FALSE}
mod_sum[["coefficients"]] %>% knitr::kable(digits = 3L, caption = 'K-fold Step Model with Higher Order Polynomials Output')
```

Studying the coefficients of the model suggest that winning is in favor if the team batting hits more doubles, triples and home runs. Moreover, increase in the number of stolen bases, and a decrease in caught steals, double plays, error, and walks allowed would all lead to a win for the batting team. It is noteworthy that the model suggests that a decrease in single hits by batter and an increase in strikeouts by batters which seems counter intuitive. But these variables were kept because when a batter steps to the plate, the player is more likely to strike out than to get a hit. Trying to hit the ball out of the park will come with strikeouts but it will also increase the chances of hitting home runs (even 1B, 2B, 3B), and that is pretty good exchange that most teams are willing carry out.

## SELECTING A MODEL

In order to select the best model to make predictions, we looked at some measurements that tells us how well each model fits the training. These include the 1) $R^2$, which represents the proportion of the variance explained by a model; 2) $adj R^2$, which is a modified version of $R^2$; 3) *Root Mean Squared Error* (RMSE), which is the square root of the mean squared error; and 4) *Akaike Information Criterion* (AIC), which is an estimator of out-of-sample prediction error. 

When comparing the three models, it was interesting to select which would be our best model given that their performance statistics and error measurements were not significantly different from each other. It is apparent that Model #1: Backwards accounts for nearly 44% of the variation in the dependent variable with the independent variables, which is acceptable as a good model. But it's explanatory power based on an $adj R^2 = 0.414$ suggests it is no different from Model #3: K-fold. In addition, Model #1: Backwards has a highest RSME of all the models, which ranks as one of the major criteria we are using to decide on a model. Now, Model #2: Pairwise resulted in the $R^2 = 0.427$, $adjR^2 = 0.417$, and the lowest $RSME = 10.193$. Because the RMSE and adjusted $R^2$ statistics already include a minor adjustment for the number of coefficients estimated, to evaluate the model complexity, we compared the AIC. While a smaller AIC is deemed less complex, Model #1: Backwards would have been the way to go, however, it was deemed that the AIC differences among the models are quite insignificant. Thus, it is unanimous that Model #2: Pairwise is our final model.

```{r performance, echo=FALSE, message=FALSE, warning=FALSE}
# Model accuracy
results = data.frame(Models = c("Model #1: Backwards" , "Model #2: Pairwise", "Model #3: K-fold"),
                R.squared = c(mod_sum1[["r.squared"]], stmr2, mod_sum[["r.squared"]]), 
                adj.R.squared = c(mod_sum[["adj.r.squared"]], stmar2, mod_sum[["adj.r.squared"]]),
                AIC = c(AIC(lm_1), AIC(stepModel), AIC(step.model$finalModel)),
                RSME = c(sqrt(mean((train_df2$TARGET_WINS - pred_lm1)^2)), stmRMSE,
                         sqrt(mean((stepdata$TARGET_WINS - pred_step.model)^2))))
results %>% knitr::kable(digits = 3L, caption = 'Performance Statistics & Error Measure of Models' )
```


## PREDICTIONS

Using the test data and the selected final model, Model #2: Pairwise, a comparison in the prediction statistic was conducted. 


```{r predictions, echo=FALSE, message=FALSE, warning=FALSE}
# Cleaning test set for Model#1
test_df1<-  eval_set %>% dplyr::select(-c(TEAM_BATTING_HBP, INDEX)) %>%
  mutate(TEAM_BATTING_1B =
         TEAM_BATTING_H -
         (TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR)) %>%
  mutate_if(is.integer, as.numeric)
test_df1 <- complete(mice(data = test_df1,
                         method = "pmm",
                         seed = 9450,
                         print=FALSE), 3)
  
test_df1<-
  test_df1 %>%  mutate(AB = TEAM_BATTING_SO + TEAM_BATTING_BB + TEAM_BATTING_H,
         SLG = (TEAM_BATTING_1B + (2*TEAM_BATTING_2B)+ (3*TEAM_BATTING_3B)+ (4*TEAM_BATTING_HR))/AB) %>% dplyr::select(-c(AB, TEAM_BATTING_H))

# Cleaning test set for Model #2
BxCox <- function(x, l) {
  ifelse(l == 0, log(x), (x ^ l - 1) / l)
}
BCL <- data.table(Metric = c('TEAM_PITCHING_H', 'TEAM_FIELDING_E',
                             'TEAM_BASERUN_SB', 'TEAM_PITCHING_HR',
                             'TEAM_BATTING_HR', 'TEAM_PITCHING_BB',
                             'TEAM_PITCHING_SO', 'TEAM_BATTING_3B',
                             'TEAM_BASERUN_CS'),
                  Lambda = c(-2.5, -0.7, 0, 0.6, 0.6, -0.1, -0.35, 0, 0.5))
setkey(BCL, Metric)
MBT <- setDT(eval_set)
#MBT[, `:=`(TEAM_BATTING_HBP = NULL)]
MBTlng <- melt(MBT, id.vars = 'INDEX', variable.name = 'Metric',
               value.name = 'Value', variable.factor = FALSE)
MBTlng[, Value := as.double(Value)]
MBTlng <- BCL[MBTlng, on = 'Metric']
MBTlng[!is.na(Lambda), Value := BxCox(Value, Lambda)]
MBTlng[, Lambda := NULL]
MBTlngSC <- copy(MBTlng)
# Remove infinities
MBTlngSC[, Value := ifelse(is.infinite(Value), NA, Value)]
# Impute, Center, and Scale
MBTlngSC[, Value := replace(Value, is.na(Value), mean(Value, na.rm = TRUE)),
       keyby = Metric]
MBTlngSC[, Value := scale(Value, center = TRUE, scale = TRUE), keyby = Metric]
MBTSC <- dcast(MBTlngSC, INDEX ~ Metric, value.var = 'Value')

# Cleaning test set for Model #3
test_df = complete(mice(data = eval_set, method = "pmm", maxit = 5, seed = 525, print = FALSE), 3)
test_df$TEAM_BATTING_1B = test_df$TEAM_BATTING_H - 
  (test_df$TEAM_BATTING_2B + test_df$TEAM_BATTING_3B + test_df$TEAM_BATTING_HR)
test_df = cbind(test_df, sapply(test_df[1:length(test_df)], function(x) x^2))
names(test_df) = make.unique(names(test_df), sep = "_")
```

```{r echo=FALSE}
# Model #1
predictions = predict(lm_1, newdata = test_df1)
results = cbind(dataset = c('Training Data', 'Test Prediction'), 
                rbind(psych::describe(train_set$TARGET_WINS), psych::describe(predictions))) 
rownames(results) = NULL
results[,-c(2,8,11)] %>% knitr::kable(digits = 2L, caption = 'Prediction Comparison with Model #1: Backwards')

# Model #2
predictions <- predict(stepModel, MBTSC)
results = cbind(dataset = c('Training Data', 'Test Prediction'), 
                rbind(psych::describe(train_set$TARGET_WINS), psych::describe(predictions))) 
rownames(results) = NULL
results[,-c(2,8,11)] %>% knitr::kable(digits = 2L, caption = 'Prediction Comparison with Model #2: Pairwise')

# Model #3
predictions = predict(step.model$finalModel, newdata = test_df)
results = cbind(dataset = c('Training Data', 'Test Prediction'), 
                rbind(psych::describe(train_set$TARGET_WINS), psych::describe(predictions))) 
rownames(results) = NULL
results[,-c(2,8,11)] %>% knitr::kable(digits = 2L, caption = 'Prediction Comparison with Model #3: K-fold')
```

## CONCLUSIONS


## CODE APPENDIX

The code chunks below represent the R code called in order during the analysis.
They are reproduced in the appendix for review and comment.
