---
title: "Data 621---HW1---Regression Model---Version 2"
author: "Group 2"
date: "9/8/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```
```{r loadPackages}
library(MASS)
library(corrplot)
library(RColorBrewer)
library(ggplot2)
library(scales)
library(data.table)
```

# Data Exploration
## Task
Describe the size and the variables in the moneyball training data set. Consider
that too much detail will cause a manager to lose interest while too little
detail will make the manager consider that you aren’t doing your job. Some
suggestions are given below. Please do NOT treat this as a check list of things
to do to complete the assignment. You should have your own thoughts on what to
tell the boss. These are just ideas.

  a. Mean / Standard Deviation / Median
  b. Bar Chart or Box Plot of the data
  c. Is the data correlated to the target variable (or to other variables?)
  d. Are any of the variables missing and need to be imputed “fixed”?
  
## Analysis
```{r readData}
MB <- fread('./data/moneyball-training-data.csv')
n <- dim(MB)[[1]]
k <- dim(MB)[[2]]

# Melting data from wide to narrow for munging & ggplot
MBmelt <- melt(MB, id.vars = 'INDEX', variable.name = 'Metric',
               value.name = 'Value', variable.factor = FALSE)
```

### Description
The data is composed of `r n` observations of `r k` fields, all of which are
discrete numerical variables.

### Missing Observations
The table below shows all variables with missing values:

```{r missingTable}
missingObs <- MBmelt[, .(Missing = sum(is.na(Value)),
                         Percentage = sum(is.na(Value)) / length(Value)),
                     keyby = Metric][Missing > 0][order(-Missing)]
knitr::kable(missingObs, digits = 3L,
caption = 'Training Data with Missing Observations')
```

Most variables are present in their entirety. Some have a few observations
missing that may be addressed through imputation. Unfortunately,
`TEAM_BATTING_HBP`---the metric representing batters getting first base awarded
due to being hit by a pitch---is missing almost all observations. It is prudent
to remove that variable from the analysis. It is also probably prudent to
remove `TEAM_BASERUN_CS` as it is missing more than one-third of the time. The
variables missing less than 10% of the time may be imputed using median or
k-means clustering. Whether `TEAM_FIELDING_DP` will be removed or imputed will
be a decision made later on in the analysis.

### Data Distribution
The table and plots below give an overview as to the distribution of the
variables.

```{r summaryTable}
# Removing 'TEAM_BATTING_HBP' and index from wide data. Easier to calculate
# correlations later on using wide data.
MB[, `:=`(TEAM_BATTING_HBP = NULL)]

# Removing 'TEAM_BATTING_HBP' from tall data
MBmelt <- MBmelt[Metric != 'TEAM_BATTING_HBP']

# Casting to double since medians and quartiles may return non-integer values.
MBmelt[, Value := as.double(Value)]

statTable <- MBmelt[, .(Min = min(Value, na.rm = TRUE),
                        Q1 = quantile(Value, 0.25, na.rm = TRUE),
                        Median = median(Value, na.rm = TRUE),
                        Q3 = round(quantile(Value, 0.75, na.rm = TRUE)),
                        Max = max(Value, na.rm = TRUE),
                        Mean = mean(Value, na.rm = TRUE),
                        StDev = sd(Value, na.rm = TRUE),
                        IQR = IQR(Value, na.rm = TRUE)), keyby = Metric]
knitr::kable(statTable, digits = 2L,
             caption = 'Training Data Summary Statistics')
```

```{r boxPlot1, fig.width = 7}
ggplot(MBmelt, aes(x = Value, y = Metric)) + geom_boxplot() +
  scale_x_continuous(labels = comma)
```

One immediate observation from both the table and the boxplots, is that the
range for `TEAM_PITCHING_H` is one to two orders of magnitude greater than that
of the other variables. Below are boxplots of the remaining variables for a
better view, followed by a histogram of `TEAM_PITCHING_H`.

```{r boxPlot2, fig.width = 7}
ggplot(MBmelt[Metric != 'TEAM_PITCHING_H'], aes(x = Value, y = Metric)) +
  geom_boxplot() + scale_x_continuous(labels = comma)

# Use the Freedman-Diaconis rule for histogram bin widths instead of ggplot
# defaults

FreedmanDiaconis <- function(x) {
  2 * IQR(x, na.rm = TRUE) / (length(x) ^ (1 / 3))
}

ggplot(MBmelt[Metric == 'TEAM_PITCHING_H'], aes(x = Value)) +
  geom_histogram(binwidth = FreedmanDiaconis) +
  scale_x_continuous(labels = comma)
```

It is very clear that `TEAM_PITCHING_H` is extremely right tailed and may be a
prime subject for a transformation.

### Correlations
A quick way to identify correlations between variables is through a
corrgram.[^1] In the corrgram below, blue represents positive correlation and
red represents negative correlation, as shown in the color bar. The shape of the
images in each cell reflect the scatterplots of the features with each other.
The main diagonal, therefore, are all upward sloping blue lines, as each feature
is perfectly correlated with itself.

[^1]:This is the proper name for a multicolored matrix representation of a
correlation matrix (Friendly 2002). Correlelograms refer to the time-series
plots used to check for auto-correlation. The terms have been conflated, though.

```{r corrgram, fig.height=6}
MBcor <- cor(MB[, -1L], method = 'pearson', use = 'pairwise.complete.obs')
corrplot(MBcor, 'ellipse', type = 'lower', order = 'hclust',
         col=brewer.pal(n=8, name="RdYlBu"))
```

Most of the ellipses are pretty wide, so there is not *much* correlation. There
is one notable exception. `TEAM_BATTING_HR` is about **97%** correlated with
`TEAM_PITCHING_HR`, as the individual HR directly affects the single determinant
of both wins and losses---the score! There is no other correlation greater than
67%. That one is `TEAM_FIELDING_E` with `TEAM_PITCHING_H` which stands to reason,
as one can only be charged with an error if the ball is in play, which is also
the only way to get a hit.

There are some interesting clusters. For example, `TEAM_FIELDING_E` and 
`TEAM_BATTING_3B` are both negatively correlated with `TEAM_BATTING_SO` and
`TEAM_BATTING_HR`. As regards `TEAM_FIELDING_E`, a hypothesis is that if batters
are either hitting home runs or striking out, *they are* **not**
*getting on base*. Therefore there are fewer opportunity for fielding errors.
Why triples are negatively correlated with home runs or strikeouts is not as
logical. There is a positive correlation between triples and fielding errors of
around 51%. Perhaps triples are more a function of fielding than batting?

Initially, our thought was to only use `TEAM_PITCHING_HR` as that has a slightly
higher correlation with target wins, but after running the model, it was clear
that there was valid information captured in both variables so both were left in.

# Data Preperation
## Task
Describe how you have transformed the data by changing the original variables or
creating new variables. If you did transform the data or create new variables,
discuss why you did this. Here are some possible transformations:

  a. Fix missing values (maybe with a Mean or Median value)
  b. Create flags to suggest if a variable was missing
  c. Transform data by putting it into buckets
  d. Mathematical transforms such as log or square root(or use Box-Cox)
  e. Combine variables (such as ratios or adding or multiplying) to create new
  variables
  
## Analysis
The `caret` package for R is the gold-standard catch-all interface for machine
learning techniques for those not using the tidyverse. Otherwise, the same
author's `tidymodels` package may be used as well. When using `caret`, almost
all of the following imputation and transformation is done as pre-processing
to the model. For the purposes of this homework, they are being performed
manually to demonstrate understanding.

### Training and Test Sets
As we are performing this analysis manually, we will break up the supplied data
into two sets, one for training and one for testing. This needs to be done
before any normalization or imputation, otherwise the testing set is polluted
with information from the training set. Please note that observations prior to
this point were on the entirety of the data set.

```{r testTrain}
set.seed(811247)
trainObs <- sample(x = n, size = 0.75 * n, replace = FALSE)
MBtrn <- MB[trainObs]
MBtst <- MB[!trainObs]
ntrn <- dim(MBtrn)[[1]]
ntst <- dim(MBtst)[[1]]
MBtrnlng <- melt(MBtrn, id.vars = 'INDEX', variable.name = 'Metric',
                 value.name = 'Value', variable.factor = FALSE)
MBtstlng <- melt(MBtst, id.vars = 'INDEX', variable.name = 'Metric',
                 value.name = 'Value', variable.factor = FALSE)

# Casting to double since medians and quartiles may return non-integer values.
MBtrnlng[, Value := as.double(Value)]
MBtstlng[, Value := as.double(Value)]
```

### Imputation
Since we are restricting ourselves to the world of the simple linear regression
model, it makes sense to use a Z-transformation to standardize the models.
Prior to this, the missing variables should be imputed. Looking at the summary
statistics table, the median is not that far off of the mean in most cases with
the exceptions of `TEAM_BASERUN_SB` and `TEAM_BATTING_SO`. Therefore, for an
initial imputation, we will use the mean so that the overall mean is unaffected
for the Z-transform. This **will** artificially reduce the SD, however. We could
address that by calculating the mean and SD, *prior* to imputation. We will not
do so now for simplicity. 

```{r imputeMean}
MBtrnlng[, Value := replace(Value, is.na(Value), mean(Value, na.rm = TRUE)),
       keyby = Metric]
```

### Transformations
#### Centering and Scaling
The first transform applied will be centering and scaling, as discussed above.
We will store the means and standard deviations so that we can transform the
predictions back to their respective distributions.

```{r centerScale}
MBtrnlngSC <- copy(MBtrnlng)
MBtrnlngUnscale <- MBtrnlngSC[, .(Mean = mean(Value), SD = sd(Value)),
                              keyby = Metric]
MBtrnlngSC[, Value := scale(Value, center = TRUE, scale = TRUE), keyby = Metric]
knitr::kable(MBtrnlngUnscale, digits = 2L, caption = 'Data Means and SDs')
```
```{r plotScaled, fig.width=7}
ggplot(MBtrnlng, aes(x = Value)) + geom_density() +
  facet_wrap(~ Metric, scales = 'free') +
  ggtitle(label = 'Kernel-Smoothed Densities of Raw Data')
ggplot(MBtrnlngSC, aes(x = Value)) + geom_density() +
  facet_wrap(~ Metric, scales = 'free') +
  ggtitle(label = 'Kernel-Smoothed Densities of Centered and Scaled Data')
```

As expected, centering and scaling did not change the shape of the
distributions. As an aside, the relationship we saw between `TEAM_PITCHING_HR`
and `TEAM_BATTING_HR` above remains rather clear in the plots.

Some of the variables, like `TEAM_BATTING_2B` or `TEAM_BATTING_SO` show
Gaussian-like behavior, with some allowances for the discrete nature of the
input data (those wiggles at the crest of `TEAM_BATTING_SO` for example). Others
are classic right tailed (exponential/gamma-esque) such as `TEAM_FIELDING_E`
or `TEAM_BATTING_3B`.

Still others are clearly multimodal. Variables like `TEAM_BATTING_HR` and
`TEAM_PITCHING_HR` indicate the existence of some clustering of better and worse
players. With these two, As said above, the "wiggles" in `TEAM_BATTING_SO` and
`TEAM_BASERUN_SB` may be more artifacts of discrete data than indications of
multimodality.

Then there is `TEAM_BASERUN_CS` which has an extreme spike at one point and then
some outliers on either side.

All of this indicates that there will need to be further processing.

#### Box-Cox transformations
One of classic data transformations to induce normality is the Box-Cox
transform. It is defined as:
\[
y(\lambda) =
\begin{cases}
\frac{y^\lambda - 1}{\lambda}, &\quad \lambda \neq 0\\
\ln{y}, &\quad \lambda = 0
\end{cases}
\]

with \(\lambda\) usually allowed to range between -5 and 5. Unfortunately, this
is only defined on positive data. Therefore, the unscaled data will need to be
used and then subsequently rescaled.

As mentioned in the introduction, most of the actual computation of transforms
is handled by the software, such as the `caret` package. For the purposes of
pedagogy, we will make some hand-selected transforms.

```{r boxCox}
BxCox <- function(x, l) {
  ifelse(l == 0, log(x), (x ^ l - 1) / l)
}

# All of these values determined by visual inspection of resulting density
# functions. A better approach would be some kind of optimization to normality,
# perhaps minimizing the square of the skewness and *excess* kurtosis, both of
# which are 0 in the standard normal distribution.

BCL <- data.table(Metric = c('TEAM_PITCHING_H', 'TEAM_FIELDING_E',
                             'TEAM_BASERUN_SB', 'TEAM_PITCHING_HR',
                             'TEAM_BATTING_HR', 'TEAM_PITCHING_BB',
                             'TEAM_PITCHING_SO', 'TEAM_BATTING_3B',
                             'TEAM_BASERUN_CS'),
                  Lambda = c(-2.5, -0.7, 0, 0.6, 0.6, -0.1, -0.35, 0, 0.5))
setkey(BCL, Metric)
knitr::kable(BCL, digits = 2L, caption = 'Hand-selected Box-Cox Lambdas')

# Left join MBtrnlng to BCL. 
MBtrnlng <- BCL[MBtrnlng, on = 'Metric']

# Transform data where there are selected lambdas
MBtrnlng[!is.na(Lambda), Value := BxCox(Value, Lambda)]

# Remove Lambda
MBtrnlng[, Lambda := NULL]

# Plot results
ggplot(MBtrnlng, aes(x = Value)) + geom_density() +
  facet_wrap(~ Metric, scales = 'free') +
  ggtitle(label = 'Kernel-Smoothed Densities of Box-Cox Transformed Data')
```

Most of the variables are now closer to a Gaussian distribution. Now the data
needs to be rescaled and recentered.

```{r reScale}
MBtrnlngSC <- copy(MBtrnlng)
MBtrnlngUnscale <- MBtrnlngSC[, .(Mean = mean(Value), SD = sd(Value)),
                          keyby = Metric]

MBtrnlngSC[, Value := scale(Value, center = TRUE, scale = TRUE), keyby = Metric]
knitr::kable(MBtrnlngUnscale, digits = 2L,
             caption = 'Post-Transform Data Means and SDs')
```

Unfortunately, the scaling introduces infinities, as most of the Box-Cox lambdas
are non-positive, which transforms 0 into infinity. These were converted to NAs
for the purposes of imputation, after which scaling and centering will be done
again. This is the danger of too many transforms. Each one induces a "stress" on
the relationship between the independent and dependent variables. At what point
does that relationship fracture?

```{r reImputeThenScale}
MBtrnlngSC <- copy(MBtrnlng)

# Remove infinities
MBtrnlngSC[, Value := ifelse(is.infinite(Value), NA, Value)]
# Center and Scale
MBtrnlngSC[, Value := replace(Value, is.na(Value), mean(Value, na.rm = TRUE)),
       keyby = Metric]
MBtrnlngUnscale <- MBtrnlngSC[, .(Mean = mean(Value), SD = sd(Value)),
                          keyby = Metric]
MBtrnlngSC[, Value := scale(Value, center = TRUE, scale = TRUE), keyby = Metric]
knitr::kable(MBtrnlngUnscale, digits = 2L,
             caption = 'Final Post-Transform Data Means and SDs')
ggplot(MBtrnlngSC, aes(x = Value)) + geom_density() +
  facet_wrap(~ Metric, scales = 'free') +
  ggtitle(label = 'Kernel-Smoothed Densities of Scaled Box-Cox Transformed Data')
```

# Build Models
## Task
Using the training data set, build at least three different multiple linear
regression models, using different variables (or the same variables with
different transformations). Since we have not yet covered automated variable
selection methods, you should select the variables manually (unless you
previously learned Forward or Stepwise selection, etc.). Since you manually
selected a variable for inclusion into the model or exclusion into [*sic*] the
model, indicate why this was done. Discuss the coefficients in the models, do
they make sense? For example, if a team hits a lot of Home Runs, it would be
reasonably expected that such a team would win more games. However, if the
coefficient is negative (suggesting that the team would lose more games), then
that needs to be discussed. Are you keeping the model even though it is counter
intuitive? Why? The boss needs to know.

## Analysis
Normally, we would use a statistical package like `caret`, which not only
simplifies model construction but also takes care of cross-validation. For the
purposes of this homework, and to demonstrate understanding of the underlying
decisions involved in predictive modeling, we will use calls to base R and the
`MASS` package.

A null model is once where there are no explanatory variables other than the
grand mean. A saturated model is one where every variable appears. 

At this point we have a set of transformed independent variables on which we
can build a model. The reasoning for the variable selection and transformations
are in previous sections. For ease, we will cast the dataset we were using from
tall back to wide first. Furthermore, the target variable, `TARGET_WINS` does
not need to be scaled.

### Linear Models
We will show three models:

 1. The null model: This is the model with only the intercept, the grand mean.
 2. The saturated model: this is the model with every term included.
 3. A model allowing pairwise interactions within the data types of baserunning,
 batting, pitching, and fielding, fit using a forward and backwards stepwise
 regression.

```{r lm, include=FALSE}
MBtrnSC <- dcast(MBtrnlngSC, INDEX ~ Metric, value.var = 'Value')

# Rescale TARGET_WINS to its standard value.
MBtrnSC[, TARGET_WINS := TARGET_WINS * 
          MBtrnlngUnscale[Metric == 'TARGET_WINS', SD] +
          MBtrnlngUnscale[Metric == 'TARGET_WINS', Mean]]

# Removing INDEX for convenience. Could have used "-INDEX" in the formula call.
MBtrnSC[, INDEX := NULL]

nullModel <- lm(TARGET_WINS ~ 1, data = MBtrnSC)
snm <- summary(nullModel)
nmr2 <- prettyNum(snm$r.squared, digits = 3L)
nmar2 <- prettyNum(snm$adj.r.squared, digits = 3L)
nmAIC <- prettyNum(AIC(nullModel), digits = 2L, big.mark = ',')
nmRMSE <- prettyNum(sqrt(mean(nullModel$residuals ^ 2)), digits = 5L)

saturatedModel <- lm(TARGET_WINS ~ ., data = MBtrnSC)
ssm <- summary(saturatedModel)
smr2 <- prettyNum(ssm$r.squared, digits = 3L)
smar2 <- prettyNum(ssm$adj.r.squared, digits = 3L)
smAIC <- prettyNum(AIC(saturatedModel), digits = 2L, big.mark = ',')
smRMSE <- prettyNum(sqrt(mean(saturatedModel$residuals ^ 2)), digits = 5L)
smF <- prettyNum(ssm$fstatistic, digits = 5L)
smFp <- pf(ssm$fstatistic[[1]], ssm$fstatistic[[2]], ssm$fstatistic[[3]],
            lower.tail = FALSE)

crazyModel <- lm(TARGET_WINS ~
  (TEAM_BASERUN_CS + TEAM_BASERUN_SB) ^ 2 + (TEAM_BATTING_2B + TEAM_BATTING_3B +
  TEAM_BATTING_BB + TEAM_BATTING_H + TEAM_BATTING_HR + TEAM_BATTING_SO) ^ 2 +
  (TEAM_FIELDING_DP + TEAM_FIELDING_E) ^ 2 + (TEAM_PITCHING_BB +
  TEAM_PITCHING_H + TEAM_PITCHING_HR + TEAM_PITCHING_SO) ^ 2, data = MBtrnSC)

stepModel <- stepAIC(crazyModel, scope = list(upper = ~
  (TEAM_BASERUN_CS + TEAM_BASERUN_SB) ^ 2 + (TEAM_BATTING_2B + TEAM_BATTING_3B +
  TEAM_BATTING_BB + TEAM_BATTING_H + TEAM_BATTING_HR + TEAM_BATTING_SO) ^ 2 +
  (TEAM_FIELDING_DP + TEAM_FIELDING_E) ^ 2 + (TEAM_PITCHING_BB +
  TEAM_PITCHING_H + TEAM_PITCHING_HR + TEAM_PITCHING_SO) ^ 2, lower = ~ 1),
  direction = 'both')

sstm <- summary(stepModel)
stmr2 <- prettyNum(sstm$r.squared, digits = 3L)
stmar2 <- prettyNum(sstm$adj.r.squared, digits = 3L)
stmAIC <- prettyNum(AIC(stepModel), digits = 2L, big.mark = ',')
stmRMSE <- prettyNum(sqrt(mean(stepModel$residuals ^ 2)), digits = 5L)
stmF <- prettyNum(sstm$fstatistic, digits = 5L)
stmFp <- pf(sstm$fstatistic[[1]], sstm$fstatistic[[2]], sstm$fstatistic[[3]],
            lower.tail = FALSE)
```
```{r nullOutput}
knitr::kable(snm$coefficients, digits = 3L,
             caption = 'Null Model Output')
```
The null model has an \(R^2\) of `r nmr2`, an adjusted \(R^2\) of `r nmar2`, an
AIC of `r nmAIC`, and an RMSE on the training set of `r nmRMSE`. There is no
F-statistic for the entirety of the null model as there is only one axis for
degrees of freedom.

```{r satOutput}
knitr::kable(ssm$coefficients, digits = 3L,
             caption = 'Saturated Model Output')
```
The saturated model has an \(R^2\) of `r smr2`, an adjusted \(R^2\) of
`r smar2`, an AIC of `r smAIC`, and an RMSE on the training set of `r smRMSE`.
The p-value for the F-test with a critical value of `r smF[[1]]` with
`r smF[[2]]` and `r smF[[3]]` degrees of freedom is below machine error
(`r smFp`).

```{r stepOutput}
knitr::kable(sstm$coefficients, digits = 3L,
             caption = 'Step Model with Pairwise Class Interactions Output')
```

The step model allowing pairwise interactions within the four classes of
metric: base running, batting, fielding, and pitching, has an \(R^2\) of
`r stmr2`, an adjusted \(R^2\) of `r stmar2`, an AIC of `r stmAIC`, and an RMSE
on the training set of `r stmRMSE`. The p-value for the F-test with a critical
value of `r stmF[[1]]` with `r stmF[[2]]` and `r stmF[[3]]` degrees of freedom
is below machine error (`r stmFp`).

### Discussion
The first item for discussion is the observation that while the intercept does
not change between the null model and the saturated model, it does decrease for
the model allowing within class pairwise interactions. There are 162 games in a
baseball season. According to all the models, given no other information, a team
will lose slightly more than half its games. Apparently, interactions in the
stepwise model unearth somewhat more information about why teams win.

#### Statistics
The stepwise model has the best \(R^2\), adjusted  \(R^2\), and AIC of the three
models. Using accepted rules of thumb, a difference of more than 10 indicates no
support for the model with the higher AIC. The magnitude is irrelevant; it is a
function of the number of observations. The difference between the AIC of the
saturated model and the step model is
`r prettyNum(AIC(saturatedModel) - AIC(stepModel), digits = 2L)`. This shows
that the saturated model, and obviously the null model, should be completely
ignored in favor of the stepwise model with pairwise within class interactions.

```{r trainStatTable}
knitr::kable(data.frame(Model = c("Null", "Saturated", "Stepwise Pairwise"),
                        Rsq = c(snm$r.squared, ssm$r.squared, sstm$r.squared),
                        AdjRsq = c(snm$adj.r.squared, ssm$adj.r.squared,
                                   sstm$adj.r.squared),
                        RMSE = c(sqrt(mean(nullModel$residuals ^ 2)),
                                 sqrt(mean(saturatedModel$residuals ^ 2)),
                                 sqrt(mean(stepModel$residuals ^ 2)))),
             digits = 3L, caption = 'Train Set Results')
```

#### Residual Plots

```{r residPlots, fig.height=7}
par(mfrow = c(2, 2))
plot(saturatedModel$fitted.values, studres(saturatedModel),
     main = 'Saturated Model', xlab = 'Predicted Values',
     ylab = 'Studentized Residuals', xlim = c(0, 160), ylim = c(-6, 6))
plot(stepModel$fitted.values, studres(stepModel),
     main = 'Stepwise Pairwise Model', xlab = 'Predicted Values',
     ylab = 'Studentized Residuals', xlim = c(0, 160), ylim = c(-6, 6))
plot(nullModel$fitted.values, studres(nullModel),
     main = 'Null Model', xlab = 'Predicted Values',
     ylab = 'Studentized Residuals', xlim = c(0, 160), ylim = c(-6, 6))
par(mfrow = c(1, 1))
```

Both the saturated and the stepwise models look to have a basically normal
distribution of their residuals. The null model returns a single value,
explaining the bizarreness of the graph.

The **actual** model to be used for the prediction will be the model which
returns the best RMSE on the test set. Unfortunately, MAPE cannot be used as
there are observations which are 0.

#### Explanation of Selected Model
Most of the coefficients are reasonable within the context of the game of
baseball. These two statements are axiomatic:
  
  * The only way to win is to have the higher score at the end of the game.
  * The only way to score is for a baserunner to cross home plate.

With those in mind, we can make the following observations about the linear
non-interactive coefficients.

 * Reasonable
   * Caught stealing removes baserunners; negative coefficient makes sense.
   * Stolen bases get a runner closer to home plate; positive coefficient makes
   sense.
   * Triples get a runner very close to home plate; positive coefficient makes
   sense.
   * Walks are free baserunners; positive coefficient makes sense.
   * Hits increase the number of baserunners; positive coefficient makes sense.
   * Hitting home runs directly increase the score; positive coefficient makes
   sense.
   * Striking out reduces the number of baserunners; negative coefficient makes
   sense.
   * Errors allow the other team free baserunners; negative coefficient makes
   sense.
   * Giving up home runs gives the opponent scores; negative coefficient makes
   sense.
   * Getting strikeouts reduces the opponents baserunners; positive coefficient
   makes sense.
 * Curious
   * Hitting doubles get runners on base; why negative?
   * Turning double plays reduce baserunners; why negative?
   * Giving up walks allows the other team free baserunners; negative
   coefficient makes sense but why insignificant?
   * Giving up hits allows the other team baserunners; why positive although
   insignificant?

An absolutely fascinating observation. In the first draft of this exercise,
`TEAM_BATTING_HR` was removed due to its high correlation with
`TEAM_PITCHING_HR`. Note the coefficients, batting is almost +12 and pitching is
-9. However, this is such as strong indicator, that in the first run, pitching
was given a *positive* coefficient of around +3. In hindsight this is because
*it was being used as an indicator for* ***batting!!*** The correlation allowed
the use of pitching HRs as an indicator for the hidden batting HRs! Once both
were restored to the model, the logical coefficients surfaced. Another reason
why models should not be trusted out of the box, but all model results should
be reviewed for sanity and sense!!

The curiosities above can *possibly* be resolved by looking at the interaction
terms. The interaction between giving up hits and strikeouts,
`TEAM_PITCHING_H:TEAM_PITCHING_SO `, is highly negative and significant. It
probably captures most of the giving up hits information making the singleton
less relevant.

A possible explanation for the negative coefficient for getting double plays, is
that double plays require at least two people on base. That means that the
opponent has a lot of base runners, which is very highly correlated with
scoring.

The behavior of doubles remains confusing. Unless its hiding something like
a team's propensity to strand runners on base. It would be interesting to see
a breakdown between the American and National leagues, as the latter tends to be
somewhat better at "small ball" and moving the runners along.

# Build Models
## Task
Decide on the criteria for selecting the best multiple linear regression model.
Will you select a model with slightly worse performance if it makes more sense
or is more parsimonious? Discuss why you selected your model. For the multiple
linear regression model, will you use a metric such as Adjusted \(R^2\), RMSE,
etc.? Be sure to explain how you can make inferences from the model,
discuss multi-collinearity issues (if any), and discuss other relevant model
output.

Using the training data set, evaluate the multiple linear regression model based
on (a) mean squared error, (b) \(R^2\), (c) F-statistic, and (d) residual plots.
Make predictions using the evaluation data set.

## Analysis
At this point, to properly fulfill this portion, cross-validation should be used
in model selection. That is not possible here given the model selection above.
That being said, given that the purpose of this exercise is **not** to
demonstrate facility with tuning, the three models developed above will be used
as the representatives. 

### Criteria
As a devout follower of Burnham & Anderson (2002), I prefer to use AIC whenever
possible. This includes inherent recognition of parsimony. As discussed in the
previous section, for an AIC difference of 10 or greater, the model with the
higher AIC has effectively no support. The stepwise model with within-class
pairwise interactions had the lowest AIC by hundreds. The various statistics and
plots requested for the training data are also in the previous section.

General inferences about the nature of the coefficients is in the previous
section. The fascinating example of how multi-collinearity can override logic is
also discussed in the previous section.

In order to make inferences from the model, the incoming data needs to be
processed in the exact same way:

 1. Mean imputation
 2. Box-Cox with the selected lambdas
 3. Mean imputation again to remove `Inf` issues
 4. Centering and scaling
 
### Model Evaluations
In the table below are the \(R^2\), adjusted \(R^2\), and RMSEfor each of the
three models developed above when applied to the **testing** set.

```{r buildTestSet}
MBtstlng <- BCL[MBtstlng, on = 'Metric']
MBtstlng[!is.na(Lambda), Value := BxCox(Value, Lambda)]
MBtstlng[, Lambda := NULL]
MBtstlngSC <- copy(MBtstlng)

# Remove infinities
MBtstlngSC[, Value := ifelse(is.infinite(Value), NA, Value)]

# Center and Scale
MBtstlngSC[, Value := replace(Value, is.na(Value), mean(Value, na.rm = TRUE)),
       keyby = Metric]
MBtstlngUnscale <- MBtstlngSC[, .(Mean = mean(Value), SD = sd(Value)),
                          keyby = Metric]
MBtstlngSC[, Value := scale(Value, center = TRUE, scale = TRUE), keyby = Metric]
MBtstSC <- dcast(MBtstlngSC, INDEX ~ Metric, value.var = 'Value')
MBtstSC[, TARGET_WINS := TARGET_WINS * 
          MBtstlngUnscale[Metric == 'TARGET_WINS', SD] +
          MBtstlngUnscale[Metric == 'TARGET_WINS', Mean]]
nmP <- predict(nullModel, MBtstSC)
nmPR <- (MBtstSC$TARGET_WINS - nmP)
smP <- predict(saturatedModel, MBtstSC)
smPR <- (MBtstSC$TARGET_WINS - smP)
stmP <- predict(stepModel, MBtstSC)
stmPR <- (MBtstSC$TARGET_WINS - stmP)
SST <- sum((MBtstSC$TARGET_WINS - mean(MBtstSC$TARGET_WINS)) ^ 2)
nmPR2 <- 1 - sum(nmPR ^ 2) / SST
nmPaR2 <- 1 - (1 - nmPR2) * (snm$df[[1]] + snm$df[[2]] - 1) / (snm$df[[2]])
nmPRMSE <- sqrt(mean(nmPR ^ 2))

smPR2 <- 1 - sum(smPR ^ 2) / SST
smPaR2 <- 1 - (1 - smPR2) * (ssm$df[[1]] + ssm$df[[2]] - 1) / (ssm$df[[2]])
smPRMSE <- sqrt(mean(smPR ^ 2))

stmPR2 <- 1 - sum(stmPR ^ 2) / SST
stmPaR2 <- 1 - (1 - stmPR2) * (sstm$df[[1]] + sstm$df[[2]] - 1) / (sstm$df[[2]])
stmPRMSE <- sqrt(mean(stmPR ^ 2))
knitr::kable(data.frame(Model = c("Null", "Saturated", "Stepwise Pairwise"),
                        Rsq = c(nmPR2, smPR2, stmPR2),
                        AdjRsq = c(nmPaR2, smPaR2, stmPaR2),
                        RMSE = c(nmPRMSE, smPRMSE, stmPRMSE)),
             digits = 3L, caption = 'Test Set Results')
```

The stepwise model allowing for pairwise interactions is the winner in all
cases, as was expected. The metrics are all worse than those on the training
set. This is a classic example of overfitting, which may have been helped by
k-fold cross-validation.

Also, this is one of those rare times where the actual \(R^2\) can be negative.
The null model prediction is a constant which is not equal to the grand mean of
the testing set, so it actually **adds** variance.

### Predictions on Evaluation Set
We will use the stepwise-pairwise model to predict `TARGET_WINS` for the
separately provided evaluation set by following the same preprocessing used for
the training and testing sets.

```{r predictions}
MBT <- fread('./data/moneyball-evaluation-data.csv')
MBT[, `:=`(TEAM_BATTING_HBP = NULL)]
MBTlng <- melt(MBT, id.vars = 'INDEX', variable.name = 'Metric',
               value.name = 'Value', variable.factor = FALSE)
MBTlng[, Value := as.double(Value)]
MBTlng <- BCL[MBTlng, on = 'Metric']
MBTlng[!is.na(Lambda), Value := BxCox(Value, Lambda)]
MBTlng[, Lambda := NULL]
MBTlngSC <- copy(MBTlng)

# Remove infinities
MBTlngSC[, Value := ifelse(is.infinite(Value), NA, Value)]

# Impute, Center, and Scale
MBTlngSC[, Value := replace(Value, is.na(Value), mean(Value, na.rm = TRUE)),
       keyby = Metric]
MBTlngSC[, Value := scale(Value, center = TRUE, scale = TRUE), keyby = Metric]
MBTSC <- dcast(MBTlngSC, INDEX ~ Metric, value.var = 'Value')
predictions <- predict(stepModel, MBTSC)
predictions
```

# References

 * Burnham, K.P. and Anderson, D.R. (2002) "Model Selection and Inference: A
 Practical Information-Theoretic Approach". 2nd Edition, Springer-Verlag,
 New York. http://dx.doi.org/10.1007/b97636
 * Friendly, Michael (2002) Corrgrams: Exploratory displays for correlation
 matrices, *The American Statistician*, 56:4, 316-324, DOI: 10.1198/000313002533,
 http://euclid.psych.yorku.ca/datavis/papers/corrgram.pdf

# Code Appendix
The code chunks below represent the code called in order during the analysis.
They are reproduced in the appendix for review and comment.

```{r loadPackages, echo=TRUE, eval=FALSE}
```
```{r readData, echo=TRUE, eval=FALSE}
```
```{r missingTable, echo=TRUE, eval=FALSE}
```
```{r summaryTable, echo=TRUE, eval=FALSE}
```
```{r boxPlot1, echo=TRUE, eval=FALSE}
```
```{r boxPlot2, echo=TRUE, eval=FALSE}
```
```{r corrgram, echo=TRUE, eval=FALSE}
```
```{r testTrain, echo=TRUE, eval=FALSE}
```
```{r imputeMean, echo=TRUE, eval=FALSE}
```
```{r centerScale, echo=TRUE, eval=FALSE}
```
```{r plotScaled, echo=TRUE, eval=FALSE}
```
```{r boxCox, echo=TRUE, eval=FALSE}
```
```{r reScale, echo=TRUE, eval=FALSE}
```
```{r reImputeThenScale, echo=TRUE, eval=FALSE}
```
```{r lm, echo=TRUE, eval=FALSE}
```
```{r nullOutput, echo=TRUE, eval=FALSE}
```
```{r satOutput, echo=TRUE, eval=FALSE}
```
```{r stepOutput, echo=TRUE, eval=FALSE}
```
```{r trainStatTable, echo=TRUE, eval=FALSE}
```
```{r residPlots, echo=TRUE, eval=FALSE}
```
```{r buildTestSet, echo=TRUE, eval=FALSE}
```
```{r predictions, echo=TRUE, eval=FALSE}
```