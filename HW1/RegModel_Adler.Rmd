---
title: "Data 621---HW1---Regression Model---Version 2"
author: "Group 2"
date: "9/8/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```
```{r loadPackages}
library(caret)
library(corrplot)
library(RColorBrewer)
library(ggplot2)
library(scales)
library(data.table)
```

# Data Exploration
## Task
Describe the size and the variables in the moneyballtraining data set. Consider
that too much detail will cause a manager to lose interest while too little
detail will make the manager consider that you aren’t doing your job. Some
suggestions are given below. Please do NOT treat this as a check list of things
to do to complete the assignment. You should have your own thoughts on what to
tell the boss. These are just ideas.

  a. Mean / Standard Deviation / Median
  b. Bar Chart or Box Plot of the data
  c. Is the data correlated to the target variable (or to other variables?)
  d. Are any of the variables missing and need to be imputed “fixed”?
  
## Analysis
```{r readData}
MB <- fread('./data/moneyball-training-data.csv')
n <- dim(MB)[[1]]
k <- dim(MB)[[2]]

# Meltng data from wide to narrow for munging & ggplot
MBmelt <- melt(MB, id.vars = 'INDEX', variable.name = 'Metric',
               value.name = 'Value', variable.factor = FALSE)
```

### Description
The data is composed of `r n` observations of `r k` fields, all of which are
discrete numerical variables.

### Missing Observations
The table below shows all variables with missing values:

```{r missingTable}
missingObs <- MBmelt[, .(Missing = sum(is.na(Value)),
                         Percentage = sum(is.na(Value)) / length(Value)),
                     keyby = Metric][Missing > 0][order(-Missing)]
knitr::kable(missingObs, digits = 3L,
caption = 'Training Data with Missing Observations')
```

Most variables are present in their entirety. Some have a few observations
missing that may be addressed through imputation. Unfortunately,
`TEAM_BATTING_HBP`---the metric representing batters getting first base awarded
due to being hit by a pitch---is missing almost all observations. It is prudent
to remove that variable from the analysis. It is also probably prudent to
remove `TEAM_BASERUN_CS` as it is missing more than one-third of the time. The
variables missing less than 10% of the time may be imputed using median or
k-means clustering. Whether `TEAM_FIELDING_DP` will be removed or imputed will
be a decision made later on in the analysis.

### Data Distribution
The table and plots below give an overview as to the distribution of the
variables.

```{r summaryTable}
# Removing 'TEAM_BATTING_HBP' and index from wide data. Easier to calculate
# correlations later on using wide data.
MB[, `:=`(TEAM_BATTING_HBP = NULL)]

# Removing 'TEAM_BATTING_HBP' from tall data
MBmelt <- MBmelt[Metric != 'TEAM_BATTING_HBP']

# Casting to double since medians and quartiles may return non-integer values.
MBmelt[, Value := as.double(Value)]

statTable <- MBmelt[, .(Min = min(Value, na.rm = TRUE),
                        Q1 = quantile(Value, 0.25, na.rm = TRUE),
                        Median = median(Value, na.rm = TRUE),
                        Q3 = round(quantile(Value, 0.75, na.rm = TRUE)),
                        Max = max(Value, na.rm = TRUE),
                        Mean = mean(Value, na.rm = TRUE),
                        StDev = sd(Value, na.rm = TRUE),
                        IQR = IQR(Value, na.rm = TRUE)), keyby = Metric]
knitr::kable(statTable, digits = 2L,
             caption = 'Training Data Summary Statistics')
```

```{r boxPlot1, fig.width = 7}
ggplot(MBmelt, aes(x = Value, y = Metric)) + geom_boxplot() +
  scale_x_continuous(labels = comma)
```

One immediate observation from both the table and the boxplots, is that the
range for `TEAM_PITCHING_H` is one to two orders of magnitude greater than that
of the other variables. Below are boxplots of the remaining variables for a
better view, followed by a histogram of `TEAM_PITCHING_H`.

```{r boxPlot2, fig.width = 7}
ggplot(MBmelt[Metric != 'TEAM_PITCHING_H'], aes(x = Value, y = Metric)) +
  geom_boxplot() + scale_x_continuous(labels = comma)

# Use the Freedman-Diaconis rule for historgram bin widths instead of ggplot
# defaults

FreedmanDiaconis <- function(x) {
  2 * IQR(x, na.rm = TRUE) / (length(x) ^ (1 / 3))
}

ggplot(MBmelt[Metric == 'TEAM_PITCHING_H'], aes(x = Value)) +
  geom_histogram(binwidth = FreedmanDiaconis) +
  scale_x_continuous(labels = comma)
```

It is very clear that `TEAM_PITCHING_H` is extremely right tailed and may be a
prime subject for a transformation.

### Correlations
A quick way to identify correlations between variables is through a
corrgram.[^1] In the corrgram below, blue represents positive correlation and
red represents negative correlation, as shown in the color bar. The shape of the
images in each cell reflect the scatterplots of the features with each other.
The main diagonal, therefore, are all upward sloping blue lines, as each feature
is perfectly correlated with itself.

[^1]:This is the proper name for a multicolored matrix representation of a
correlation matrix (Friendly 2002). Correlelograms refer to the time-series
plots used to check for auto-correlation. The terms have been conflated, though.

```{r corrgram, fig.height=6}
MBcor <- cor(MB[, -1L], method = 'pearson', use = 'pairwise.complete.obs')
corrplot(MBcor, 'ellipse', type = 'lower', order = 'hclust',
         col=brewer.pal(n=8, name="RdYlBu"))
```

Most of the ellipses are pretty wide, so there is not *much* correlation. There
is one notable exception. `TEAM_BATTING_HR` is about **97%** correlated with
`TEAM_PITCHING_HR`, as the individual HR directly affects the single determinant
of both wins and losses---the score! There is no other correlation greater than
67%. That one is `TEAM_FIELDING_E` with `TEAM_PITCHING_H` which stands to reason,
as one can only be charged with an error if the ball is in play, which is also
the only way to get a hit.

There are some interesting clusters. For example, `TEAM_FIELDING_E` and 
`TEAM_BATTING_3B` are both negatively correlated with `TEAM_BATTING_SO` and
`TEAM_BATTING_HR`. As regards `TEAM_FIELDING_E`, a hypothesis is that if batters
are either hitting home runs or striking out, *they are* **not**
*getting on base*. Therefore there are fewer opportunity for fielding errors.
Why triples are negatively correlated with home runs or strikeouts is not as
logical. There is a positive correlation between triples and fielding errors of
around 51%. Perhaps triples are more a function of fielding than batting?

Our suggestion would be to use only `TEAM_PITCHING_HR` as that has a slightly
higher correlation with target wins, and leave all the other variables in the
analysis.

# Data Preperation
## Task
Describe how you have transformed the data by changing the original variables or
creating new variables. If you did transform the data or create new variables,
discuss why you did this. Here are some possible transformations:

  a. Fix missing values (maybe with a Mean or Median value)
  b. Create flags to suggest if a variable was missing
  c. Transform data by putting it into buckets
  d. Mathematical transforms such as log or square root(or use Box-Cox)
  e. Combine variables (such as ratios or adding or multiplying) to create new
  variables
  
## Analysis
The `caret` package for R is the gold-standard catch-all interface for machine
learning techniques for those not using the tidyverse. Otherwise, the same
author's `tidymodels` package may be used as well. When using `caret`, almost
all of the following imputation and transformation is done as pre-processing
to the model. For the purposes of this homework, they are being performed
manually to demonstrate understanding.

### Imputation
Since we are restricting ourselves to the world of the simple linear regression
model, it makes sense to use a Z-transformation to standardize the models.
Prior to this, the missing variables should be imputed. Looking at the summary
statistics table, the median is not that far off of the mean in most cases with
the exceptions of `TEAM_BASERUN_SB` and `TEAM_BATTING_SO`. Therefore, for an
initial imputation, we will use the mean so that the overall mean is unaffected
for the Z-transform. This **will** artificially reduce the SD, however. We could
address that by calculating the mean and SD, *prior* to imputation. We will not
do so now for simplicity. 

```{r imputeMean}
MBmelt[, Value := replace(Value, is.na(Value), mean(Value, na.rm = TRUE)),
       keyby = Metric]
```

### Transformations
#### Centering and Scaling
The first transform applied will be centering and scaling, as discussed above.
We will store the means and standard deviations so that we can transform the
predictions back to their respective distributions.

```{r centerScale}
MBmeltSC <- copy(MBmelt)
MBmeltUnscale <- MBmeltSC[, .(Mean = mean(Value), SD = sd(Value)),
                          keyby = Metric]
MBmeltSC[, Value := scale(Value, center = TRUE, scale = TRUE), keyby = Metric]
knitr::kable(MBmeltUnscale, digits = 2L, caption = 'Data Means and SDs')
```
```{r plotScaled, fig.width=7}
ggplot(MBmelt, aes(x = Value)) + geom_density() +
  facet_wrap(~ Metric, scales = 'free') +
  ggtitle(label = 'Kernel-Smoothed Densities of Raw Data')
ggplot(MBmeltSC, aes(x = Value)) + geom_density() +
  facet_wrap(~ Metric, scales = 'free') +
  ggtitle(label = 'Kernel-Smoothed Densities of Centered and Scaled Data')
```

As expected, centering and scaling did not change the shape of the
distributions. As an aside, the relationship we saw between `TEAM_PITCHING_HR`
and `TEAM_BATTING_HR` above remains rather clear in the plots.

Some of the variables, like `TEAM_BATTING_2B` or `TEAM_BATTING_SO` show
Gaussian-like behavior, with some allowances for the discrete nature of the
input data (those wiggles at the crest of `TEAM_BATTING_SO` for example). Others
are classic right tailed (exponential/gamma-esque) such as `TEAM_FIELDING_E`
or `TEAM_BATTING_3B`.

Still others are clearly multimodal. Variables like `TEAM_BATTING_HR` and
`TEAM_PITCHING_HR` indicate the existence of some clustering of better and worse
players. With these two, As said above, the "wiggles" in `TEAM_BATTING_SO` and
`TEAM_BASERUN_SB` may be more artifacts of discrete data than indications of
multimodality.

Then there is `TEAM_BASERUN_CS` which has an extreme spike at one point and then
some outliers on either side.

All of this indicates that there will need to be further processing.

#### Box-Cox transformations
One of classic data transformations to induce normality is the Box-Cox
transform. It is defined as:
\[
y(\lambda) =
\begin{cases}
\frac{y^\lambda - 1}{\lambda}, &\quad \lambda \neq 0\\
\ln{y}, &\quad \lambda = 0
\end{cases}
\]

with \(\lambda\) usually allowed to range between -5 and 5. Unfortunately, this
is only defined on positive data. Therefore, the unscaled data will need to be
used and then subsequently rescaled.

As mentioned in the introduction, most of the actual computation of transforms
is handled by the software, such as the `caret` package. For the purposes of
pedagogy, we will make some hand-selected transforms.

```{r boxCox}
BxCox <- function(x, l) {
  ifelse(l == 0, log(x), (x ^ l - 1) / l)
}

# All of these values determined by visual inspection of resulting density
# functions. A better approach would be some kind of optimization to normality,
# perhaps minimizing the square of the skewness and *excess* kurtosis, both of
# which are 0 in the standard normal distribution.

BCL <- data.table(Metric = c('TEAM_PITCHING_H', 'TEAM_FIELDING_E',
                             'TEAM_BASERUN_SB', 'TEAM_PITCHING_HR',
                             'TEAM_PITCHING_BB', 'TEAM_PITCHING_SO',
                             'TEAM_BATTING_3B'),
                  Lambda = c(-2.5, -0.5, 0, 0.6, -0.15, -0.35, 0))
setkey(BCL, Metric)
knitr::kable(BCL, digits = 2L, caption = 'Hand-selected Box-Cox Lambdas')

# Left join MBmelt to BCL. 
MBmelt <- BCL[MBmelt]

# Transform data where there are selected lambdas
MBmelt[!is.na(Lambda), Value := BxCox(Value, Lambda)]

# Remove TEAM_BATTING_HR

MBmelt <- MBmelt[Metric != 'TEAM_BATTING_HR']

# Remove Lambda
MBmelt[, Lambda := NULL]

# Plot results
ggplot(MBmelt, aes(x = Value)) + geom_density() +
  facet_wrap(~ Metric, scales = 'free') +
  ggtitle(label = 'Kernel-Smoothed Densities of Box-Cox Transformed Data')
```

Most of the variables are now closer to a Gaussian distribution. Also note we
have removed `TEAM_BATTING_HR` from the data. Now the data needs to be rescaled
and recentered.

```{r reScale}
MBmeltSC <- copy(MBmelt)
MBmeltUnscale <- MBmeltSC[, .(Mean = mean(Value), SD = sd(Value)),
                          keyby = Metric]

MBmeltSC[, Value := scale(Value, center = TRUE, scale = TRUE), keyby = Metric]
knitr::kable(MBmeltUnscale, digits = 2L,
             caption = 'Post-Transform Data Means and SDs')
```

Unfortunately, the scaling introduces infinities, as most of the Box-Cox lamdas
are non-positive, which transforms 0 into infinity. These were converted to NAs
for the purposes of imputation, after which scaling and centering will be done
again. This is the danger of too many transforms. Each one induces a "stress" on
the relationship between the independent and dependent variables. At what point
does that relationship fracture?

```{r reImputeThenScale}
MBmeltSC <- copy(MBmelt)
MBmeltSC[, Lambda := NULL]
# Remove infinities
MBmeltSC[, Value := ifelse(is.infinite(Value), NA, Value)]
# Center and Scale
MBmeltSC[, Value := replace(Value, is.na(Value), mean(Value, na.rm = TRUE)),
       keyby = Metric]
MBmeltUnscale <- MBmeltSC[, .(Mean = mean(Value), SD = sd(Value)),
                          keyby = Metric]
MBmeltSC[, Value := scale(Value, center = TRUE, scale = TRUE), keyby = Metric]
knitr::kable(MBmeltUnscale, digits = 2L,
             caption = 'Final Post-Transform Data Means and SDs')
ggplot(MBmeltSC, aes(x = Value)) + geom_density() +
  facet_wrap(~ Metric, scales = 'free') +
  ggtitle(label = 'Kernel-Smoothed Densities of Scaled Box-Cox Transformed Data')
```

# Build Models
## Task
Using the training data set, build at least three different multiple linear
regression models, using different variables (or the same variables with
different transformations). Since we have not yet covered automated variable
selection methods, you should select the variables manually (unless you
previously learned Forward or Stepwise selection, etc.). Since you manually
selected a variable for inclusion into the model or exclusion into [*sic*] the
model, indicate why this was done. Discuss the coefficients in the models, do
they make sense? For example, if a team hits a lot of Home Runs, it would be
reasonably expected that such a team would win more games. However, if the
coefficient is negative (suggesting that the team would lose more games), then
that needs to be discussed. Are you keeping the model even though it is counter
intuitive? Why? The boss needs to know.

## Analysis
At this point we have a set of transformed independent variables on which we
can build a model. The target variable, `TARGET_WINS` does not need to be
scaled. Normally, we would have thrown everything into `caret`, but for the
purposes of this homework, we will use the `stepAIC` function from the `MASS`
package to perform a backwards-stepwise linear regression using AIC as our
goodness of fit metric. For ease, we will cast the dataset we were using from
tall back to wide first.

```{r lm1}
MBsc <- dcast(MBmeltSC, INDEX ~ Metric, value.var = 'Value')
```



# References

 * Friendly, Michael (2002) Corrgrams: Exploratory displays for correlation
 matrices, *The American Statistician*, 56:4, 316-324, DOI: 10.1198/000313002533,
 http://euclid.psych.yorku.ca/datavis/papers/corrgram.pdf

# Appendix
The code chunks below represent the code called in order during the analysis.
They are reproduced in the appendix for review and comment.

```{r loadPackages, echo=TRUE, eval=FALSE}
```
```{r readData, echo=TRUE, eval=FALSE}
```
```{r missingTable, echo=TRUE, eval=FALSE}
```
```{r summaryTable, echo=TRUE, eval=FALSE}
```
```{r boxPlot1, echo=TRUE, eval=FALSE}
```
```{r boxPlot2, echo=TRUE, eval=FALSE}
```
```{r corrgram, echo=TRUE, eval=FALSE}
```
```{r imputeMean, echo=TRUE, eval=FALSE}
```
```{r centerScale, echo=TRUE, eval=FALSE}
```
```{r plotScaled, echo=TRUE, eval=FALSE}
```
```{r boxCox, echo=TRUE, eval=FALSE}
```
```{r reScale, echo=TRUE, eval=FALSE}
```
```{r reImputeThenScale, echo=TRUE, eval=FALSE}
```