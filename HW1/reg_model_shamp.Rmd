---
title: "HW! - Regression Model - 621"
author: "Jeff Shamp"
date: "9/8/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(mice)
library(psych)
```


## Data Preparation

We saw from the above that at least one variable should be omitted for low reporting of data. There are also outlier concerns for several of the variables, namely `TEAM_PITCHING_SO`. 

First, we will look into obvious outliers in the data set and remove them. There appears to be enough data in the training set to naive yet cautious approach to outlier removal. 

```{r load, echo=FALSE}
set.seed(9450)
train_df<- 
  read.csv("./data/moneyball-training-data.csv")
train_df <-
  train_df %>%
  select(-c(TEAM_BATTING_HBP, INDEX)) %>%
  mutate_if(is.integer, as.numeric)
```



### Isolate base hits. 

The variable `TEAM_BATTING_H` is all hits. We will separate out singles from the rest. 

```{r singles, echo=FALSE}
train_df<- 
    train_df %>%
    mutate(TEAM_BATTING_1B =
           TEAM_BATTING_H -
           (TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR)) 
  
```


### Outlier Removal

Again, we will take a cautious approach to removing outliers at first. Removing values that appear to be so far out from the normal that is it seems likely that there is some kind of data input issue. 

```{r outliers_1, echo=FALSE}
train_df %>%
  pivot_longer(everything(), 
               names_to ="Variable", 
               values_to="Value") %>%
  ggplot(aes(x=Variable, y=Value)) +
  geom_boxplot(na.rm = TRUE) + coord_flip()
```

There are some clear issues with hitting singles and some that we know about with strikeouts. We will remove the `SO` outliers first and see what needs to happen with singles hitting. These appear to right skewed so we will use median and IQR to remove outliers. 

```{r iqr_removal_SO, echo=FALSE}
train_df<-
train_df %>%
  na.omit() %>%
  summarize(iqr = IQR(TEAM_PITCHING_SO)) %>%
  bind_cols(
            train_df 
            )  %>%
  filter(
    TEAM_PITCHING_SO > quantile(TEAM_PITCHING_SO,
                                probs=c(0.25), 
                                na.rm = TRUE) - 1.5*iqr, 
    TEAM_PITCHING_SO < quantile(TEAM_PITCHING_SO, 
                                probs=c(0.75), 
                                na.rm=TRUE) + 1.5*iqr
        ) %>%
  select(-iqr)

train_df %>%
  pivot_longer(everything(), 
               names_to ="Variable", 
               values_to="Value") %>%
  ggplot(aes(x=Variable, y=Value)) +
  geom_boxplot(na.rm = TRUE) + coord_flip()
```

We will take a similar approach to removing outliers for pitching hitting allowed. At least one seems unrealisticly extreme. 


```{r iqr_removal_H, echo=FALSE}
train_df<-
train_df %>%
  na.omit() %>%
  summarize(iqr = IQR(TEAM_PITCHING_H)) %>%
  bind_cols(
            train_df 
            )  %>%
  filter(
    TEAM_PITCHING_H > quantile(TEAM_PITCHING_H,
                                probs=c(0.25), 
                                na.rm = TRUE) - 1.5*iqr, 
    TEAM_PITCHING_H < quantile(TEAM_PITCHING_H, 
                                probs=c(0.75), 
                                na.rm=TRUE) + 1.5*iqr
        ) %>%
  select(-iqr)

```


```{r outlier_boxplot, echo=FALSE}
train_df %>%
  pivot_longer(everything(), 
               names_to ="Variable", 
               values_to="Value") %>%
  ggplot(aes(x=Variable, y=Value)) +
  geom_boxplot(na.rm = TRUE) + coord_flip()
```

This has eliminated what appears to be the most extreme outliers. There are still large outlier sets for Errors and stolen bases, but none that seem to dwarf the other variables. Basically, things appear to be on a scale that seems logical given the data and source of information that we have. 

## Imputation of Values

From the `missing_value_eda.Rmd` the vast majority of the data is complete with only a few variables with missing values. We will use the "mice" imputation method [described here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/#:~:text=Missing%20data%20are%20a%20common,method%20of%20addressing%20missing%20data.).

We will use the predictive mean matching method. 

```{r impute_train,echo=FALSE, message=FALSE, warning=FALSE}
train_df <- complete(mice(data = train_df, 
                         method = "pmm", 
                         seed = 9450, 
                         print=FALSE), 3)
```


## Feature Engineering

With outliers removed and missing values imputed via MICE, we can add a few features. We will take the obvious and approximate the slugging feature.


### Slugging Percentage

Slugging is calculated in the following way

${\displaystyle \mathrm {SLG} ={\frac {({\mathit {1B}})+(2\times {\mathit {2B}})+(3\times {\mathit {3B}})+(4\times {\mathit {HR}})}{AB}}}$

We will take the same approximation for at-bats. 

${\displaystyle \mathrm {\sim SLG} ={\frac {({\mathit {1B}})+(2\times {\mathit {2B}})+(3\times {\mathit {3B}})+(4\times {\mathit {HR}})}{SO_{batting} + H + BB}}}$



```{r make_slg_obp, echo=FALSE}
train_df<-
train_df %>%
  mutate(AB = TEAM_BATTING_SO + TEAM_BATTING_BB + TEAM_BATTING_H,
         SLG = (TEAM_BATTING_1B + (2*TEAM_BATTING_2B)+ (3*TEAM_BATTING_3B)+ (4*TEAM_BATTING_HR))/AB) %>%
  # Removing AB and total hits
  select(-c(AB, TEAM_BATTING_H))
```



## Modeling

We will start with a simple linear model and select out features based on null hypothesis testing for non-zero slope. 

Below is the output of coefficients for backward selection model. 

```{r lm_first, echo=FALSE, eval=FALSE}
lm_reg = lm(data=train_df, TARGET_WINS ~ .)
summary(lm_reg)
```


```{r lm_selected, echo=FALSE}
lm_1<- lm(TARGET_WINS ~ TEAM_BATTING_2B +TEAM_BATTING_HR+
          TEAM_BATTING_BB+TEAM_BATTING_SO+ TEAM_BASERUN_SB+TEAM_BASERUN_CS+
          TEAM_PITCHING_HR+ TEAM_PITCHING_H+
          TEAM_FIELDING_E+ TEAM_FIELDING_DP+  SLG, data=train_df)
knitr::kable(lm_1$coefficients, digits = 3L)
```

### Model Coefficient Discussion

First, we are keeping the coefficient for `TEAM_BATTING_HR` since it's p-value is marginally above the general threshold and knowledge of the game suggests it is important. There are some counter intuitive results, which is expected given that baseball is a messy, imprecise game that has evolved over time. To that end, we should expect some seemingly strange results from algorithmic regression. We used a combination of domain knowledge and data analysis to justify retaining features. 

Fielding double plays and batting doubles both appear to have negative impacts on wins even though they _should_ be postive impacts. Turning double plays, while a good for the defensive team, may suggest a larger, negative issue. Namely, weak pitching that leads to runners on base. Similarly, batting doubles, leaves runners open to double plays. Allowed hits by pitching, caught stealing, and batting strike-outs are all counter intuitive results as well, but they seem to be small contributors and these are events that happen regularly in every game. 

Slugging as an approximation is the major predictor in this regression, by far. The other predictors other than the intercept are orders of magnitude less in predictive value. It should be noted that slugging alone is not a good predictor for wins overall. 

### Remove Outliers that Influence Fit

We need further outlier treatment. We will use Cook's distance to remove outliers that are influencing the fit of the model above. We will use a cutoff of $\frac{4}{N}$. 

```{r cooks_distance, echo=FALSE}
cooks_dis<- cooks.distance(lm_1)
influential<- as.numeric(names(cooks_dis)[(cooks_dis > (4/nrow(train_df)))])
train_df<- train_df[-influential, ]
```

### Model Evaluation

Let's take this backward selection process and use it in a resampling scheme and see how our R values hold up. 

```{r resample, echo=FALSE}
folds<- vfold_cv(train_df,
                 v = 10)

lm_model<- 
  linear_reg() %>%
  set_engine(engine="lm")

lm_wf<- 
  workflow() %>%
  add_formula(
    TARGET_WINS ~ TEAM_BATTING_2B +TEAM_BATTING_HR+
          TEAM_BATTING_BB+TEAM_BATTING_SO+ TEAM_BASERUN_SB+TEAM_BASERUN_CS+
          TEAM_PITCHING_HR+ TEAM_PITCHING_H+
          TEAM_FIELDING_E+ TEAM_FIELDING_DP+ SLG
    ) %>%
  add_model(lm_model)

lm_resample<- 
  fit_resamples(
    lm_wf,
    folds, 
    control = control_resamples(save_pred = TRUE)
  )

lm_resample %>%
  collect_metrics() %>%
  knitr::kable(digits = 3L)
```

The metrics hold up with cross validation, that's good. Let's fit the entire training dataset one last time and look at $R^{2}$ and RMSE. 

```{r last_fit, echo=FALSE}
results_train<-
  lm_wf %>%
  fit(train_df) %>%
  predict(new_data = train_df) %>%
  mutate(
    truth = train_df$TARGET_WINS,
    model = "lm"
  )

multi_metric<- metric_set(rmse, rsq)

results_train %>%
  multi_metric(truth=truth, estimate = .pred) %>%
  knitr::kable(digits = 3L)

```

## Test Data

Let's prep, fit, and evaluate the testing data. 

```{r test_prep, echo=FALSE, warning=FALSE, message=FALSE}
test_df<- 
  read.csv("./data/moneyball-evaluation-data.csv")
test_df<- 
  test_df %>% 
  select(-c(TEAM_BATTING_HBP, INDEX)) %>%
  mutate(TEAM_BATTING_1B =
         TEAM_BATTING_H -
         (TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR)) %>%
  mutate_if(is.integer, as.numeric)

test_df <- complete(mice(data = test_df,
                         method = "pmm",
                         seed = 9450,
                         print=FALSE), 3)
  
test_df<-
  test_df %>%
  mutate(AB = TEAM_BATTING_SO + TEAM_BATTING_BB + TEAM_BATTING_H,
         SLG = (TEAM_BATTING_1B + (2*TEAM_BATTING_2B)+ (3*TEAM_BATTING_3B)+ (4*TEAM_BATTING_HR))/AB) %>%
  select(-c(AB, TEAM_BATTING_H))
```


```{r make_predictions, echo=FALSE}
test_pred<-
  lm_wf %>%
  fit(train_df) %>%
  predict(new_data=test_df) %>%
  rename(predictions = .pred)

knitr::kable(summary(test_pred))
```


```{r prediction, echo=FALSE}
test_pred %>%
  filter(predictions >= 116 | predictions <=0) %>%
  arrange(predictions) %>%
  knitr::kable()
```

Three predictions are unreasonably large, but not bad given the metrics. One prediction is above the MLB reccord, but only by one win. With an RMSE of 9.53 and $R^{2}=0.443$, and $R^{2}_{ADJ} =0.43$ we have a model that explains 44% of the variance in the data and can predict wins within $\pm 10$ wins for a given season. That's pretty good at predicting wins relative to seasoned baseball veterans. 




