---
title: "DATA 621 - Business Analytics and Data Mining"
subtitle: 'Fall 2020 - Group 2 - Homework #4'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "11/01/2020"
output:
  pdf_document:
    toc: TRUE
    toc_depth: 4
urlcolor: purple
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE)
```

```{r loadData, include=FALSE}
# Load necessary libraries
library(corrplot)
library(ggplot2)
library(scales)
library(knitr)
library(caret)
library(RANN)
library(data.table)
# Set master seed
set.seed(71554)

# Set filepaths for data ingestion
urlRemote  = "https://raw.githubusercontent.com/"
pathGithub = "aadler/DT621_Fall2020_Group2/master/HW4/data/"
fileTrain = "insurance_training_data.csv"
fileTest = "insurance-evaluation-data.csv"

# Read training file
DT <- fread(paste0(urlRemote, pathGithub, fileTrain),
            colClasses = c(rep('integer', 2L), 'double', rep('integer', 4L),
                           'double', 'factor', 'double', rep('factor', 4L),
                           'integer', 'factor', 'double', 'integer',
                           rep('factor', 2L), 'double', 'integer', 'factor',
                           rep('integer', 2L), 'factor'))

# Convert character currencies to doubles
DT[, `:=`(INCOME = as.double(gsub('[$,]', '', INCOME)),
          HOME_VAL = as.double(gsub('[$,]', '', HOME_VAL)),
          BLUEBOOK = as.double(gsub('[$,]', '', BLUEBOOK)),
          OLDCLAIM = as.double(gsub('[$,]', '', OLDCLAIM)))]

# Read and process the test/evaluation file
eval <- fread(paste0(urlRemote, pathGithub, fileTest),
              colClasses = c(rep('integer', 2L), 'double', rep('integer', 4L),
                             'double', 'factor', 'double', rep('factor', 4L),
                             'integer', 'factor', 'double', 'integer',
                             rep('factor', 2L), 'double', 'integer', 'factor',
                             rep('integer', 2L), 'factor'))
eval[, `:=`(INCOME = as.double(gsub('[$,]', '', INCOME)),
          HOME_VAL = as.double(gsub('[$,]', '', HOME_VAL)),
          BLUEBOOK = as.double(gsub('[$,]', '', BLUEBOOK)),
          OLDCLAIM = as.double(gsub('[$,]', '', OLDCLAIM)))]

# Number of training observations
ntrobs <- dim(DT)[[1]]

# Get the names of the variables and which ones are numeric
nmtrn <- names(DT)
nmtrnNUM <- names(DT[, .SD, .SDcols = sapply(DT, is.numeric)])
nmtrnDUB <- names(DT[, .SD, .SDcols = sapply(DT, is.double)])
nmtrnFAC <- setdiff(nmtrn, nmtrnNUM)
# Drop INDEX and target variablesfrom name list
nmtrnNUM <- nmtrnNUM[-(1:3)]
```

# Introduction
The assignment for HW4 is to analyze and model a dataset containing
approximately 8000 records representing customers of an auto insurance company.
Each record has two response variables. The first response variable,
`TARGET_FLAG`, is a 1 or a 0. where a `1` means that the person was in a car
and a `0` means that the person was not in a car crash. The second response
variable is `TARGET_AMT`. This value is 0 if the person did not crash their car.
However, if they did crash their car, this number will be a value greater than
zero.

The objective of the assignment is to build multiple linear regression and
binary logistic regression models on the training data to predict **both** the
probability that a person will crash their car and the amount of money it will
cost given a crash. Only the variables in the dataset, or variables directly
derived from them, may be used.

# DATA EXPLORATION
## Variables
The data is composed of the following variables:

|VARIABLE NAME|DEFINITION|THEORETICAL EFFECT|
|-|-|-|
|INDEX|Identification Variable (do not use)|None|
|TARGET_FLAG|Was Car in a crash? 1=YES 0=NO|None|
|TARGET_AMT|If car was in a crash, what was the cost|None|
|AGE|Age of Driver|Very young people tend to be risky. Maybe very old people also.|
|BLUEBOOK|Value of Vehicle|Unknown effect on probability of collision, but probably effect the payout if there is a crash|
|CAR_AGE|Vehicle Age|Unknown effect on probability of collision, but probably effect the payout if there is a crash|
|CAR_TYPE|Type of Car|Unknown effect on probability of collision, but probably effect the payout if there is a crash|
|CAR_USE|Vehicle Use|Commercial vehicles are driven more, so might increase probability of collision|
|CLM_FREQ|# Claims (Past 5 Years)|The more claims you filed in the past, the more you are likely to file in the future|
|EDUCATION|Max Education Level|Unknown effect, but in theory more educated people tend to drive more safely|
|HOMEKIDS|# Children at Home|Unknown effect|
|HOME_VAL|Home Value|In theory, home owners tend to drive more responsibly|
|INCOME|Income|In theory, rich people tend to get into fewer crashes|
|JOB|Job Category|In theory, white collar jobs tend to be safer|
|KIDSDRIV|# Driving Children|When teenagers drive your car, you are more likely to get into crashes|
|MSTATUS|Marital Status|In theory, married people drive more safely|
|MVR_PTS|Motor Vehicle Record Points|If you get lots of traffic tickets, you tend to get into more crashes|
|OLDCLAIM|Total Claims (Past 5 Years)|If your total payout over the past five years was high, this suggests future payouts will be high|
|PARENT1|Single Parent|Unknown effect|
|RED_CAR|A Red Car|Urban legend says that red cars (especially red sports cars) are more risky. Is that true?|
|REVOKED|License Revoked (Past 7 Years)|If your license was revoked in the past 7 years, you probably are a more risky driver.|
|SEX|Gender|Urban legend says that women have less crashes then men. Is that true?|
|TIF|Time in Force|People who have been customers for a long time are usually more safe.|
|TRAVTIME|Distance to Work|Long drives to work usually suggest greater risk|
|URBANICITY|Home/Work Area|Unknown|
|YOJ|Years on Job|People who stay at a job for a long time are usually more safe|

There are `r ntrobs` observations of `r length(nmtrnNUM) - 2` numeric predictor
variables and `r length(nmtrnFAC)` factor predictor variables.

## Missing Data
There are missing observations. Specifically, the following variables have
missing values coded as `NA`:

```{r missingVal}
# Check for NAs in all variables
missingV <- DT[, lapply(.SD, function(x) sum(is.na(x)))]
# Only keep those whose count is > 0
missingV <- missingV[, .SD, .SDcols = sapply(missingV, function (x) x > 0)]
kable(missingV, caption = "Variables with Count of Missing Values > 0")
```

Another concern is that the following variables should not have significant
probabilities of 0 because they represent values which *should* be positive:

  * AGE
  * YOJ
  * INCOME
  * HOME_VAL
  * TRAVTIME
  * BLUEBOOK
  * TIF
  * CAR_AGE

```{r zeroVal}
# Check for 0 value for the above variables
zeroV <- DT[, lapply(.SD, function(x) sum(x == 0, na.rm = TRUE)),
            .SDcols = c('AGE', 'YOJ', 'INCOME', 'HOME_VAL', 'TRAVTIME',
                        'BLUEBOOK', 'TIF', 'CAR_AGE')]
# Only keep those whose count of zero values is > 0
zeroV <- zeroV[, .SD, .SDcols = sapply(zeroV, function (x) x > 0)]
kable(zeroV, caption = "Variables with Count of Missing Values > 0")
```

The few missing `CAR_AGE` values should be easy to impute. The other heavy-0
observations are of more concern. Over 25% of the `HOME_VAL` observations are 0!
Now it may be that those with `HOME_VAL` of 0 do not own homes but rent, and it
may be that those with `YOJ` and `INCOME` of 0 were unemployed at the time of
the data collection. There are `r DT[YOJ == 0 & INCOME == 0, .N]` observations
with both `YOJ` and `INCOME` of 0, meaning there are a few dozen of each which
have values in one class and not the other. Handling this will be addressed in
the DATA PREPARATION section.

## Summary Statistics and Graphs
### Numeric Predictors
The numeric predictor variables have the following summary statistics, ignoring
missing values:

```{r statsN}
# Isolate numeric only predictors
numDT <- DT[, .SD, .SDcols = nmtrnNUM]

# Melt them from wide to long format
numDTM <- melt(numDT, variable.name = 'metric', value.name = 'value')

# Calculate summary statistics
statsN <- numDTM[, .(Mean = mean(value, na.rm = TRUE),
                     SD = sd(value, na.rm = TRUE),
                     Min = min(value, na.rm = TRUE),
                     Q1 = quantile(value, prob = 0.25, na.rm = TRUE),
                     Median = median(value, na.rm = TRUE),
                     Q3 = quantile(value, prob = 0.75, na.rm = TRUE),
                     Max = max(value, na.rm = TRUE),
                     IQR = IQR(value, na.rm = TRUE)), keyby = 'metric']

# Print the table
kable(statsN, digits = 3L, align = 'r',
      caption = "Summary Statistitics for Numeric Variables")
```

A visual depiction of the distributions of the numeric predictors follows. The
blue graphs are on a normal scale and the red ones are on a \(\log_{10}\) scale.

```{r graphsN}
# Freedman-Diaconis rule for bin widths
FDbin <- function(x) {
  result <- 2 * IQR(x, na.rm = TRUE) / (length(x) ^ (1 / 3))
  return(ifelse(result == 0, 0.5, result))
}
ggplot(numDTM[!is.na(metric)], aes(x = value)) +
  geom_histogram(binwidth = FDbin, fill = 'lightblue3') +
  facet_wrap( ~ metric, scales = 'free')

ggplot(numDTM, aes(x = value)) +
  geom_histogram(binwidth = FDbin, fill = 'indianred4') +
  facet_wrap( ~ metric, scales = 'free') + scale_x_log10()
```

What the above visualizations show is that there are two sets of numeric
variables. There are those which are clearly discrete, taking one of a few
integral values, and there are those which are more continuous. Of the
continuous variables, some like `AGE` look rather Gaussian at first glance,
while others, such as `HOME_VAL` or `INCOME` are skewed. However, `HOME_VAL`
looks lognormal, as its histogram on the log scale is near-Gaussian.

A variable such as `OLDCLAIM` is expected to have a mass point at 0. Every
observation which had fewer than 2 claims clearly did not have a prior claim!

### Factor Predictors
While tabular depiction of factor variables is usually of little value, visual
representation of their distributions is of use.

```{r graphsF, fig.width=8, fig.height=8}
# Isolate factor only predictors
facDT <- DT[, .SD, .SDcols = nmtrnFAC]

# Melt them from wide to long format (need to explicitly call measure.vars since
# these are all factors)
facDTM <- melt(facDT, measure.vars = nmtrnFAC, variable.name = 'metric',
               value.name = 'value')
ggplot(facDTM, aes(x = value)) + geom_bar(fill = 'darkolivegreen') +
  facet_wrap( ~ metric, nrow = 5L, scales = 'free') + coord_flip()
```

In order to use regression-based tools, we are going to have to convert these
factors to dummy variables. Another important observation is that there are
rows for which `JOB` is blank and some strange value names for some of the
factors. Cleaning this up will be handled in the DATA PREPARATION section.

### Correlations
The corrgram below graphically represents the correlations between the numeric
predictor variables, when ignoring the missing variables.

```{r corrgram, fig.width=6, fig.height=4}
corrplot(cor(DT[, ..nmtrnNUM], use = 'complete.obs'),
         method = 'ellipse', type = 'lower')
```

Most of the numeric variables are uncorrelated with one another. A few
exceptions exist:

 * `HOMEKIDS` with `KIDSDRIV`
   * This is self-explanatory
 * `HOME_VAL` with `INCOME`
   * Both are very strongly tied to wealth and net worth
 * `BLUEBOOK` and `INCOME`
   * People with more disposable income can afford more expensive vehicles
 * `CLM_FREQ` with `OLDCLAIM`
   * More claims implies a greater total aggregate payment
 * `CAR_AGE` and `INCOME`
   * This is interesting. It could be that people with higher disposable incomes
   buy more expensive cars and keep them for longer, but that's a guess.
 * `MVR_PTS` is *lightly* correlated with `CLM_FREQ` and `OLDCLAIM`
   * Probably when one is in a lot of accidents, the chances of them being at
   some level of fault rises
 * `HOMEKIDS` **negatively** correlated with `AGE`
   * As people age, their children age as well and eventually move out, we hope!

# DATA PREPARATION
## Data Value Cleanup
The first step will be to clean up the strange values and blank `JOB` entries.

```{r dataClean}
# Clean up data values. Using setattr for fast data.table setting. Equivalent
# to levels(DT$X) <- c(a, b) but faster since changes by reference.
setattr(DT$MSTATUS, 'levels', c('Yes', 'No'))
setattr(DT$SEX, 'levels', c('M', 'F'))
setattr(DT$EDUCATION, 'levels', c('<High School', 'Bachelors', 'Masters', 'PhD',
                                  'High School'))
setattr(DT$JOB, 'levels', c('Unknown', 'Clerical', 'Doctor', 'Home Maker',
                            'Lawyer', 'Manager', 'Professional', 'Student',
                            'Blue Collar'))
setattr(DT$CAR_TYPE, 'levels', c('Minivan', 'Panel Truck', 'Pickup',
                                 'Sports Car', 'Van', 'SUV'))

setattr(DT$URBANICITY, 'levels', c('Urban', 'Rural'))

# Relevel some of the variables to have the default make more sense
# Set default marital status to unmarried
set(DT, NULL, 'MSTATUS', relevel(DT$MSTATUS, 'No'))
# Set default car usage to private
set(DT, NULL, 'CAR_USE', relevel(DT$CAR_USE, 'Private'))
```

## Training & Testing Split
While the following discussion more properly occurs under BUILD MODELS, as it
factors into how the data is prepared, it will happen here. All the models will
be trained on the same approximately 70% of the training set, reserving 30% for
validation of which model to select for the frequency and severity estimation on
the supplied evaluation set.

However, there will need to be two sets of training and testing, as the severity
model must perforce be trained on data for which a claim occurred. To maintain
integrity, the split of all the data for frequency will be honored for severity.
The 70/30 split will be honored as all claims with a `TARGET_FLAG` of 1 will
have non-zero `TARGET_AMT`, However, there will be no guarantee that the
*distribution* of the loss conditional on event in the validation set will be
similar to the training set as a whole.

```{r trainTestSplit}
set.seed(642)
trnIDX <- createDataPartition(DT$TARGET_FLAG, p = 0.7)
trnX <- DT[trnIDX$Resample1, ]
tstX <- DT[!trnIDX$Resample1, ]
```

## Models Set 1
### Missing Data
The issues with factor data were handled universally for all models through
resetting the levels. What remains is handling the missing numeric values. For
`CAR_AGE` simple k-means imputation will be used. The remaining three problem
variables, `YOJ`, `INCOME`, and `HOME_VAL` have both `NA` and 0 issues. 

For `YOJ` and `INCOME`, it is not unreasonable to consider 0 to be realistic:
unemployed or employed less than half a year. As such, we will leave the 0's and
impute the NAs.

We will take a different approach with `HOME_VAL`. As seen above when the 0
values are removed, `HOME_VAL` has a nice lognormal shape. It is also reasonable
to consider that people responding with 0 for `HOME_VAL` are non-owners as
opposed to it being missing, although we would want to investigate this further
were we allowed. Therefore, we are going to add a "Own" variable which will be
`Yes` for all positive `HOME_VAL` and `No` otherwise, and allow an interaction
between this value and the actual value. This would be much more effective in a
decision tree framework, but that is not available.

```{r model1addVars}
# Add OWN variable for model set 1
trnX[, OWN := factor(ifelse(HOME_VAL > 0, 1, 0),
                     levels = c(0, 1), labels = c('No', 'Yes'))]
tstX[, OWN := factor(ifelse(HOME_VAL > 0, 1, 0),
                     levels = c(0, 1), labels = c('No', 'Yes'))]
```

# BUILD MODELS
## Model Set 1
### Dummy Variables
As there are many factor variables, dummy variables will be created. The target
variables will be removed and held to the side so as not to contaminate the
fitting.

### Frequency Model
The frequency model will be a binary logistic on the prepared data. It will
start with the complete model and allow for some interactions. It will use both
forward and backward steps to identify which variables to include, using AIC as
the test metric. Using this approach does not allow for cross-validation. The
initial interactions considered will be:

 * `OWN` with `HOME_VAL` as discussed in the missing data section
 * `KIDSDRIV` with `HOMEKIDS`
 * A three-way interaction between `HOME_VAL`, `INCOME`, and `BLUEBOOK`
 * `CLM_FREQ` with `OLDCLAIM`
 * `MSTATUS` with `SEX`
   * One sex may be more affected by marriage than the other
 
```{r model1DummyFreq}
trnFreq <- factor(trnX$TARGET_FLAG, levels = c(0, 1),
                  labels = c("NoClaim", "Claim"))
trnDumFreq <- dummyVars(TARGET_FLAG ~ .  + OWN * HOME_VAL +
                          KIDSDRIV * HOMEKIDS + HOME_VAL * INCOME * BLUEBOOK +
                          CLM_FREQ * OLDCLAIM + MSTATUS * SEX - INDEX -
                          TARGET_AMT,
                        fullRank = TRUE, data = trnX)
trnXFD <- predict(trnDumFreq, newdata = trnX)
```

#### Training
At this point the code will be set up to preprocess the data and train the
model. The two pre-processing steps which will be done will be to check for
near-zero variance between the predictors and then use kNN imputations for
missing values. The actual code may be found in the CODE APPENDIX.

```{r model1RunFreq, cache = 2}
trC <- trainControl(method = 'none',            # Using StepAIC
                    classProbs = TRUE,          # Classification
                    summaryFunction = prSummary # Precision/Recall)
)
set.seed(487)
m1FreqFit <- train(x = trnXFD, y = trnFreq, preProcess = c('nzv', 'knnImpute'),
                   trControl = trC, method = 'glmStepAIC',
                   family = binomial(link = 'logit'), direction = 'both',
                   trace = 1)
kable(summary(m1FreqFit)$coefficients, digits = 3L,
      caption = "Model 1 Frequency Regression Output")
```

#### Coefficient Discussion
Positive coefficients represent an **increased** risk of a claim over the
baseline and negative coefficients represent a **decreased** risk over the
baseline. Most of the coefficients are of the expected direction. For example,
having one's license revoked or living in an urban area is a *positive* risk
factor for claims while higher levels of wealth or education are *negative* risk
factors.

Some interesting observations. `SEX` is not a significant predictor, even at the
10% level, but removing it does not increase the AIC, so it remains. The
hypothesized interaction between `MSTATUS` and `SEX` does lower the deviance,
but not enough to justify the additional parameter.

The predictors with the greatest effect on risk are `URBANICITY` and `INCOME`.
Having the three-way interaction between `HOME_VAL`, `INCOME`, and `BLUEBOOK`,
together with `INCOME` and `BLUEBOOK` alone, obviated the need for stand-alone
`HOME_VAL` and the other two possible interactions.

In terms of variable importance, it can be estimated from the the last trace of
the stepping procedure shown below. The further down the table the variable is,
the more dramatic its inclusion or exclusion will be. The top 10 variables in
order would be: `URBANICITY.Rural`, `REVOKED.Yes`, `CAR_USE.Commercial`,
`TRAVTIME`, `CAR_TYPE.Sports Car`, `CLM_FREQ`, `MVR_PTS`, `TIF`, `CAR_TYPE.SUV`,
and `JOB.Blue Collar`.

```
                             Df Deviance    AIC
<none>                            5164.6 5224.6
+ HOMEKIDS                    1   5162.7 5224.7
+ AGE                         1   5163.1 5225.1
- SEX.F                       1   5167.4 5225.4
+ EDUCATION.Masters           1   5163.4 5225.4
+ JOB.Manager                 1   5163.7 5225.7
+ OLDCLAIM                    1   5164.0 5226.0
+ EDUCATION.PhD               1   5164.0 5226.0
+ `HOME_VAL:BLUEBOOK`         1   5164.2 5226.2
+ CAR_AGE                     1   5164.2 5226.2
+ `MSTATUSYes:SEXF`           1   5164.5 5226.5
+ OWN.Yes                     1   5164.5 5226.5
+ YOJ                         1   5164.6 5226.6
+ RED_CAR.yes                 1   5164.6 5226.6
+ `EDUCATION.High School`     1   5164.6 5226.6
+ `INCOME:HOME_VAL:BLUEBOOK`  1   5164.6 5226.6
- `INCOME:BLUEBOOK`           1   5168.8 5226.8
- `INCOME:HOME_VAL`           1   5170.0 5228.0
- JOB.Lawyer                  1   5170.6 5228.6
- JOB.Student                 1   5171.4 5229.4
- `CAR_TYPE.Panel Truck`      1   5173.5 5231.5
- MSTATUS.Yes                 1   5174.8 5232.8
- CAR_TYPE.Van                1   5179.5 5237.5
- BLUEBOOK                    1   5179.9 5237.9
- `OLDCLAIM:CLM_FREQ`         1   5180.0 5238.0
- `JOB.Home Maker`            1   5180.2 5238.2
- HOME_VAL                    1   5180.7 5238.7
- EDUCATION.Bachelors         1   5181.4 5239.4
- INCOME                      1   5182.3 5240.3
- PARENT1.Yes                 1   5182.6 5240.6
- CAR_TYPE.Pickup             1   5183.2 5241.2
- JOB.Professional            1   5186.1 5244.1
- JOB.Clerical                1   5195.4 5253.4
- KIDSDRIV                    1   5197.8 5255.8
- `JOB.Blue Collar`           1   5201.2 5259.2
- CAR_TYPE.SUV                1   5202.5 5260.5
- TIF                         1   5204.4 5262.4
- MVR_PTS                     1   5204.8 5262.8
- CLM_FREQ                    1   5205.4 5263.4
- `CAR_TYPE.Sports Car`       1   5208.4 5266.4
- TRAVTIME                    1   5211.7 5269.7
- CAR_USE.Commercial          1   5230.2 5288.2
- REVOKED.Yes                 1   5233.0 5291.0
- URBANICITY.Rural            1   5612.1 5670.1
```

### Severity Model
In the insurance world, severity is not necessarily fit via linear models, but
is more often fit through comparison of the maximum likelihood fits of various
distributional families to the size of the claims, but the task here is to fit a
linear regression model to estimate future claim size.

#### Training
The observations in the training set which have claim amounts will be extracted
and used to fit the linear regression. Similar to frequency, the model will be
selected via a forward and backward stepwise AIC algorithm starting with every
relevant predictive variable and some selected interactions.

```{r model1DummySev}
trnSev <- trnX$TARGET_AMT[trnX$TARGET_AMT > 0]
trnDumSev <- dummyVars(TARGET_AMT ~ .  + OWN * HOME_VAL +
                          KIDSDRIV * HOMEKIDS + HOME_VAL * INCOME * BLUEBOOK +
                          CLM_FREQ * OLDCLAIM + MSTATUS * SEX - INDEX -
                          TARGET_FLAG,
                        fullRank = TRUE, data = trnX[TARGET_AMT > 0])
trnXSD <- predict(trnDumFreq, newdata = trnX[TARGET_AMT > 0])
```

```{r runModel1Sev}
trC <- trainControl(method = 'none')
set.seed(878)
m1SevFit <- train(x = trnXSD, y = trnSev, preProcess = c('nzv', 'knnImpute'),
                   trControl = trC, method = 'glmStepAIC',
                   family = gaussian, direction = 'both',
                   trace = 0)
kable(summary(m1SevFit)$coefficients, digits = 3L,
      caption = "Model 1 Severity Regression Output")
```

#### Coefficient Disucssion
Hi!

```
                             Df   Deviance   AIC
<none>                          9.5466e+10 31329
- REVOKED.Yes                 1 9.5614e+10 31329
+ JOB.Manager                 1 9.5368e+10 31330
- MSTATUS.Yes                 1 9.5625e+10 31330
+ CAR_AGE                     1 9.5379e+10 31330
- `OLDCLAIM:CLM_FREQ`         1 9.5645e+10 31330
+ `MSTATUSYes:SEXF`           1 9.5392e+10 31330
- CAR_USE.Commercial          1 9.5650e+10 31330
+ SEX.F                       1 9.5399e+10 31330
+ JOB.Professional            1 9.5400e+10 31330
+ CAR_TYPE.Pickup             1 9.5410e+10 31330
+ CAR_TYPE.Van                1 9.5412e+10 31330
+ MVR_PTS                     1 9.5413e+10 31330
+ HOMEKIDS                    1 9.5415e+10 31330
+ CLM_FREQ                    1 9.5431e+10 31331
- `INCOME:HOME_VAL`           1 9.5688e+10 31331
+ RED_CAR.yes                 1 9.5440e+10 31331
- HOME_VAL                    1 9.5694e+10 31331
+ JOB.Student                 1 9.5444e+10 31331
+ EDUCATION.Masters           1 9.5445e+10 31331
+ PARENT1.Yes                 1 9.5452e+10 31331
- OLDCLAIM                    1 9.5709e+10 31331
+ JOB.Clerical                1 9.5455e+10 31331
+ `CAR_TYPE.Panel Truck`      1 9.5455e+10 31331
+ `EDUCATION.High School`     1 9.5456e+10 31331
+ OWN.Yes                     1 9.5457e+10 31331
+ TRAVTIME                    1 9.5458e+10 31331
+ `INCOME:HOME_VAL:BLUEBOOK`  1 9.5460e+10 31331
+ `JOB.Home Maker`            1 9.5460e+10 31331
+ URBANICITY.Rural            1 9.5463e+10 31331
+ `INCOME:BLUEBOOK`           1 9.5464e+10 31331
+ AGE                         1 9.5464e+10 31331
+ `HOME_VAL:BLUEBOOK`         1 9.5465e+10 31331
+ `CAR_TYPE.Sports Car`       1 9.5465e+10 31331
+ YOJ                         1 9.5465e+10 31331
+ `JOB.Blue Collar`           1 9.5466e+10 31331
+ JOB.Lawyer                  1 9.5466e+10 31331
+ KIDSDRIV                    1 9.5466e+10 31331
+ EDUCATION.Bachelors         1 9.5466e+10 31331
+ CAR_TYPE.SUV                1 9.5466e+10 31331
+ `KIDSDRIV:HOMEKIDS`         1 9.5466e+10 31331
+ TIF                         1 9.5466e+10 31331
+ INCOME                      1 9.5466e+10 31331
+ EDUCATION.PhD               1 9.5466e+10 31331
- BLUEBOOK                    1 9.6124e+10 31337
```
# SELECT MODELS
## Model Selection Criteria
For that *classification* algorithm, we will look at accuracy, AUC, and F1, and
select the model which performs best two out of three in those metrics. For the
*regression* algorithm we will look at RMSE, AIC. and MAE (since the values must
be positive) and select the model which performs best two out of three in those
metrics.

### Frequency Models

```{r model1Test}

```

# PREDICTIONS


# CODE APPENDIX
```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```
```{r loadData}
```
```{r missingVal}
```
```{r zeroVal}
```
```{r statsN}
```
```{r graphsN}
```
```{r graphsF}
```
```{r corrgram}
```
```{r dataClean}
```
```{r trainTestSplit}
```
```{r model1addVars}
```
```{r model1DummyFreq}
```
```{r model1RunFreq}
```
```{r model1DummySev}
```
```{r model1RunSev}
```