---
title: "DATA 621 - Business Analytics and Data Mining"
subtitle: 'Fall 2020 - Group 2 - Homework #4'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "11/01/2020"
output:
  pdf_document:
    toc: TRUE
    toc_depth: 3
urlcolor: purple
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE)
```

```{r loadData, include=FALSE}
# Load necessary libraries
library(knitr)
library(caret)
library(data.table)
# Set master seed
set.seed(71554)

# Set filepaths for data ingestion
urlRemote  = "https://raw.githubusercontent.com/"
pathGithub = "aadler/DT621_Fall2020_Group2/master/HW4/data/"
fileTrain = "insurance_training_data.csv"
fileTest = "insurance-evaluation-data.csv"

# Read training file
DT <- fread(paste0(urlRemote, pathGithub, fileTrain),
            colClasses = c(rep('integer', 2L), 'double', rep('integer', 4L),
                           'double', 'factor', 'double', rep('factor', 4L),
                           'integer', 'factor', 'double', 'integer',
                           rep('factor', 2L), 'double', 'integer', 'factor',
                           rep('integer', 2L), 'factor'))

# Convert character currencies to doubles
DT[, `:=`(INCOME = as.double(gsub('[$,]', '', INCOME)),
          HOME_VAL = as.double(gsub('[$,]', '', HOME_VAL)),
          BLUEBOOK = as.double(gsub('[$,]', '', BLUEBOOK)),
          OLDCLAIM = as.double(gsub('[$,]', '', OLDCLAIM)))]

# Read and process the test/evaluation file
eval <- fread(paste0(urlRemote, pathGithub, fileTest),
              colClasses = c(rep('integer', 2L), 'double', rep('integer', 4L),
                             'double', 'factor', 'double', rep('factor', 4L),
                             'integer', 'factor', 'double', 'integer',
                             rep('factor', 2L), 'double', 'integer', 'factor',
                             rep('integer', 2L), 'factor'))
eval[, `:=`(INCOME = as.double(gsub('[$,]', '', INCOME)),
          HOME_VAL = as.double(gsub('[$,]', '', HOME_VAL)),
          BLUEBOOK = as.double(gsub('[$,]', '', BLUEBOOK)),
          OLDCLAIM = as.double(gsub('[$,]', '', OLDCLAIM)))]

# Number of training observations
ntrobs <- dim(DT)[[1]]

# Get the names of the variables and which ones are numeric
nmtrn <- names(DT)
nmtrnNUM <- names(DT[, .SD, .SDcols = sapply(DT, is.numeric)])
nmtrnDUB <- names(DT[, .SD, .SDcols = sapply(DT, is.double)])
nmtrnFAC <- setdiff(nmtrn, nmtrnNUM)
# Drop INDEX and target variablesfrom name list
nmtrnNUM <- nmtrnNUM[-(1:3)]
```

# Introduction
The assignment for HW4 is to analyze and model a dataset containing
approximately 8000 records representing customers of an auto insurance company.
Each record has two response variables. The first response variable,
`TARGET_FLAG`, is a 1 or a 0. where a `1` means that the person was in a car
and a `0` means that the person was not in a car crash. The second response
variable is `TARGET_AMT`. This value is 0 if the person did not crash their car.
However, if they did crash their car, this number will be a value greater than
zero.

The objective of the assignment is to build multiple linear regression and
binary logistic regression models on the training data to predict **both** the
probability that a person will crash their car and the amount of money it will
cost given a crash. Only the variables in the dataset, or variables directly
derived from them, may be used.

# DATA EXPLORATION
## Variables
The data is composed of the following variables:

|VARIABLE NAME|DEFINITION|THEORETICAL EFFECT|
|-|-|-|
|INDEX|Identification Variable (do not use)|None|
|TARGET_FLAG|Was Car in a crash? 1=YES 0=NO|None|
|TARGET_AMT|If car was in a crash, what was the cost|None|
|AGE|Age of Driver|Very young people tend to be risky. Maybe very old people also.|
|BLUEBOOK|Value of Vehicle|Unknown effect on probability of collision, but probably effect the payout if there is a crash|
|CAR_AGE|Vehicle Age|Unknown effect on probability of collision, but probably effect the payout if there is a crash|
|CAR_TYPE|Type of Car|Unknown effect on probability of collision, but probably effect the payout if there is a crash|
|CAR_USE|Vehicle Use|Commercial vehicles are driven more, so might increase probability of collision|
|CLM_FREQ|# Claims (Past 5 Years)|The more claims you filed in the past, the more you are likely to file in the future|
|EDUCATION|Max Education Level|Unknown effect, but in theory more educated people tend to drive more safely|
|HOMEKIDS|# Children at Home|Unknown effect|
|HOME_VAL|Home Value|In theory, home owners tend to drive more responsibly|
|INCOME|Income|In theory, rich people tend to get into fewer crashes|
|JOB|Job Category|In theory, white collar jobs tend to be safer|
|KIDSDRIV|# Driving Children|When teenagers drive your car, you are more likely to get into crashes|
|MSTATUS|Marital Status|In theory, married people drive more safely|
|MVR_PTS|Motor Vehicle Record Points|If you get lots of traffic tickets, you tend to get into more crashes|
|OLDCLAIM|Total Claims (Past 5 Years)|If your total payout over the past five years was high, this suggests future payouts will be high|
|PARENT1|Single Parent|Unknown effect|
|RED_CAR|A Red Car|Urban legend says that red cars (especially red sports cars) are more risky. Is that true?|
|REVOKED|License Revoked (Past 7 Years)|If your license was revoked in the past 7 years, you probably are a more risky driver.|
|SEX|Gender|Urban legend says that women have less crashes then men. Is that true?|
|TIF|Time in Force|People who have been customers for a long time are usually more safe.|
|TRAVTIME|Distance to Work|Long drives to work usually suggest greater risk|
|URBANICITY|Home/Work Area|Unknown|
|YOJ|Years on Job|People who stay at a job for a long time are usually more safe|

There are `r ntrobs` observations of `r length(nmtrnNUM) - 2` numeric predictor
variables and `r length(nmtrnFAC)` factor predictor variables.

## Missing Data
There are missing observations. Specifically, the following variables have
missing values coded as `NA`:

```{r missingVal}
# Check for NAs in all variables
missingV <- DT[, lapply(.SD, function(x) sum(is.na(x)))]
# Only keep those whose count is > 0
missingV <- missingV[, .SD, .SDcols = sapply(missingV, function (x) x > 0)]
kable(missingV, caption = "Variables with Count of Missing Values > 0")
```

Another concern is that the following variables should not have significant
probabilities of 0 because they represent values which *should* be positive:

  * AGE
  * YOJ
  * INCOME
  * HOME_VAL
  * TRAVTIME
  * BLUEBOOK
  * TIF
  * CAR_AGE

```{r zeroVal}
# Check for 0 value for the above variables
zeroV <- DT[, lapply(.SD, function(x) sum(x == 0, na.rm = TRUE)),
            .SDcols = c('AGE', 'YOJ', 'INCOME', 'HOME_VAL', 'TRAVTIME',
                        'BLUEBOOK', 'TIF', 'CAR_AGE')]
# Only keep those whose count of zero values is > 0
zeroV <- zeroV[, .SD, .SDcols = sapply(zeroV, function (x) x > 0)]
kable(zeroV, caption = "Variables with Count of Missing Values > 0")
```

The few missing `CAR_AGE` values should be easy to impute. The other heavy-0
observations are of more concern. Over 25% of the `HOME_VAL` observations are 0!
Now it may be that those with `HOME_VAL` of 0 do not own homes but rent, and it
may be that those with `YOJ` and `INCOME` of 0 were unemployed at the time of
the data collection. There are `r DT[YOJ == 0 & INCOME == 0, .N]` observations
with both `YOJ` and `INCOME` of 0, meaning there are a few dozen of each which
have values in one class and not the other. Handling this will be addressed in
the DATA PREPARATION section.

## Summary Statistics and Graphs
### Numeric Predictors
The numeric predictor variables have the following summary statistics, ignoring
missing values:

```{r statsN}
# Isolate numeric only predictors
numDT <- DT[, .SD, .SDcols = nmtrnNUM]

# Melt them from wide to long format
numDTM <- melt(numDT, variable.name = 'metric', value.name = 'value')

# Calculate summary statistics
statsN <- numDTM[, .(Mean = mean(value, na.rm = TRUE),
                     SD = sd(value, na.rm = TRUE),
                     Min = min(value, na.rm = TRUE),
                     Q1 = quantile(value, prob = 0.25, na.rm = TRUE),
                     Median = median(value, na.rm = TRUE),
                     Q3 = quantile(value, prob = 0.75, na.rm = TRUE),
                     Max = max(value, na.rm = TRUE),
                     IQR = IQR(value, na.rm = TRUE)), keyby = 'metric']

# Print the table
kable(statsN, digits = 3L, align = 'r',
      caption = "Summary Statistitics for Numeric Variables")
```

A visual depiction of the distributions of the numeric predictors follows. The
blue graphs are on a normal scale and the red ones are on a \(\log_{10}\) scale.

```{r graphsN}
# Freedman-Diaconis rule for bin widths
FDbin <- function(x) {
  result <- 2 * IQR(x, na.rm = TRUE) / (length(x) ^ (1 / 3))
  return(ifelse(result == 0, 0.5, result))
}
ggplot(numDTM[!is.na(metric)], aes(x = value)) +
  geom_histogram(binwidth = FDbin, fill = 'lightblue3') +
  facet_wrap( ~ metric, scales = 'free')

ggplot(numDTM, aes(x = value)) +
  geom_histogram(binwidth = FDbin, fill = 'indianred4') +
  facet_wrap( ~ metric, scales = 'free') + scale_x_log10()
```

What the above visualizations show is that there are two sets of numeric
variables. There are those which are clearly discrete, taking one of a few
integral values, and there are those which are more continuous. Of the
continuous variables, some like `AGE` look rather Gaussian at first glance,
while others, such as `HOME_VAL` or `INCOME` are skewed. However, `HOME_VAL`
looks lognormal, as its histogram on the log scale is near-Gaussian.

A variable such as `OLDCLAIM` is expected to have a mass point at 0. Every
observation which had fewer than 2 claims clearly did not have a prior claim!

### Factor Predictors

# CODE APPENDIX
```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```
```{r loadData}
```
```{r missingVal}
```
```{r zeroVal}
```
```{r statsN}
```
```{r graphsN}
```