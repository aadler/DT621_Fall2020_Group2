---
title: "DATA 621 - Business Analytics and Data Mining"
subtitle: 'Fall 2020 - Group 2 - Homework #5'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "11/22/2020"
output:
  pdf_document:
  toc: TRUE
toc_depth: 4
urlcolor: purple
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE)
```

```{r loadData, include=FALSE}
# Load necessary libraries
library(ggplot2)
library(scales)
library(knitr)
library(caret)
library(corrplot)
library(data.table)

# Set master seed
set.seed(65408)

# Set filepaths for data ingestion
urlRemote  = "https://raw.githubusercontent.com/"
pathGithub = "aadler/DT621_Fall2020_Group2/master/HW5/data/"
fileTrain = "wine-training-data.csv"
fileTest = "wine-evaluation-data.csv"

# Read training file
DT <- fread(paste0(urlRemote, pathGithub, fileTrain))

# Number of training observations
ntrnobs <- dim(DT)[[1]]

# Get the names of the predictor variables
nmtrn <- names(DT)[-(1:2)]
nmtrnINT <- c('AcidIndex', 'LabelAppeal', 'STARS')
nmtrnDUB <- setdiff(nmtrn, nmtrnINT)
```

# ASSIGNMENT
The assignment for HW5 is to analyze and model a dataset containing
approximately 12,000 records representing commercially available wines. The
`TARGET` response variable represents the number of sample cases of wine
purchased by wine distribution companies after sampling that wine. These cases
would be used to provide tasting samples to restaurants and wine stores around
the United States. The more sample cases purchased, the more likely is a wine to
be sold at a high end restaurant.A large wine manufacturer is studying the data
in order to predict the number of wine cases ordered based upon the wine
characteristics. If the wine manufacturer can predict the number of cases, then
that manufacturer will be able to adjust their wine offering to maximize sales.

The objective of this assignment is to build a count regression model to predict
the number of cases of wine that will be sold given certain properties of the
wine. *HINT:* Sometimes, the fact that a variable is missing is actually
predictive of the target. You can only use the variables given to you (or
variables that you derive from the variables provided).

# DATA EXPLORATION
## Variables
The data is composed of the following variables:

|VARIABLE NAME|DEFINITION|THEORETICAL EFFECT|
|--|--|--|
|INDEX|Identification Variable (do not use)|None|
|TARGET|Number of Cases Purchased|None|
|AcidIndex|Proprietary method of testing total acidity of wine by using a weighted average||
|Alcohol|Alcohol Content||
|Chlorides|Chloride content of wine||
|CitricAcid|Citric Acid Content||
|Density|Density of Wine||
|FixedAcidity|Fixed Acidity of Wine||
|FreeSulfurDioxide|Sulfur Dioxide content of wine||
|LabelAppeal|Marketing Score indicating the appeal of label design for consumers. High numbers suggest customers like the label design. Negative numbers suggest customers don't like the design.|Many consumers purchase based on the visual appeal of the wine label design. Higher numbers suggest better sales.|
|ResidualSugar|Residual Sugar of wine||
|STARS|Wine rating by a team of experts. 4 Stars = Excellent, 1 Star = Poor|A high number of stars suggests high sales|
|Sulphates|Sulfate content of wine||
|TotalSulfurDioxide|Total Sulfur Dioxide of Wine||
|VolatileAcidity|Volatile Acid content of wine||
|pH|pH of wine||

There are `r ntrnobs` observations. All of these predictors are numeric, 
although `LabelAppeal`, `AcidIndex` and `STARS` all appear to be ordinal factors
and not true numerics. For the purpose of this assignment, we can treat them
as integers.

## Missing Data
There are a lot of missing values for some of the predictors.

```{r missingVal}
missingPreds <- transpose(DT[, lapply(.SD, function(x) {sum(is.na(x))}),
                             .SDcols = nmtrn],
                          keep.names = 'Predictors')
setnames(missingPreds, 'V1', 'Missing')
missingPreds[, Percentage := Missing / ntrnobs * 100]
setorder(missingPreds, -Missing)
kable(missingPreds, digits = 2L, caption = 'Predictors with Missing Observations')
```

`STARS` is especially sparse, but as noted in the assignment, this may be an
indication in and of its own. It is reasonable to assume that the vinters of a
good wine want it to be rated. If they do not submit it for rating, that may be
an indication of their lack of faith in the wine. How to factor this into the
analysis will be decided individually by the modelers in this assignment. The
other missing variables are all less than 10%, and their handling via imputation
or otherwise will be done on a model-by-model basis as well.

## Summary Statistics and Graphs
As all of the data are numeric, we can investigate the distributions of the
predictors both tabularly and graphically.

The numeric predictor variables have the following summary statistics, ignoring
missing values:

```{r statsN}
# Isolate numeric only predictors
predictorDT <- DT[, .SD, .SDcols = nmtrn]

# Melt them from wide to long format
predictorDTM <- melt(predictorDT, variable.name = 'metric',
                     value.name = 'value',  variable.factor = FALSE,
                     id.vars = NULL)

# Calculate summary statistics
statsN <- predictorDTM[, .(Mean = mean(value, na.rm = TRUE),
                           SD = sd(value, na.rm = TRUE),
                           Min = min(value, na.rm = TRUE),
                           Q1 = quantile(value, prob = 0.25, na.rm = TRUE),
                           Median = median(value, na.rm = TRUE),
                           Q3 = quantile(value, prob = 0.75, na.rm = TRUE),
                           Max = max(value, na.rm = TRUE),
                           IQR = IQR(value, na.rm = TRUE)), keyby = 'metric']

# Print the table
kable(statsN, digits = 3L, align = 'r',
      caption = "Summary Statistitics for Numeric Variables")
```

A kernel-smoothed density plot of the distributions of the non-integer numeric
predictors is below, followed by a histogram of the integral-valued predictors.

```{r graphsD, fig.height=5, fig.width=7}
# Using epanechnikov kernel
ggplot(predictorDTM[!is.na(metric) & metric %chin% nmtrnDUB], aes(x = value)) +
  geom_density(kernel = 'epanechnikov') +
  facet_wrap( ~ metric, scales = 'free') +
  ggtitle('Kernel-smoothed Density of Numeric Predictors')
```

The numeric predictors all look to be basically symmetrical, but non-Gaussian in
that there is a strong spike near the median, but the tails on either side are
thicker than would be for a normal distribution with that low of a standard
deviation.

```{r graphsH, fig.height=2, fig.width=7}
# Freedman-Diaconis rule for bin widths
FDbin <- function(x) {
  result <- 2 * IQR(x, na.rm = TRUE) / (length(x) ^ (1 / 3))
  return(ifelse(result == 0, 0.5, result))
}
ggplot(predictorDTM[!is.na(metric) & metric %chin% nmtrnINT], aes(x = value)) +
  geom_histogram(binwidth = FDbin, fill = 'indianred4') +
  facet_wrap( ~ metric, scales = 'free') +
  ggtitle('Histogram of Integral Predictors')
```

Of these three, `LabelAppeal` seems to be the most "normal" of the lot; `STARS`
and `AcidIndex` look to be more Poisson in shape.

### Correlations
The corrgram below graphically represents the correlations between the numeric
predictor variables, when ignoring the missing variables.

```{r corrgram, fig.width=6, fig.height=4}
corrplot::corrplot(cor(DT[, ..nmtrn], use = 'complete.obs'),
         method = 'ellipse', type = 'lower', order = 'hclust',
         hclust.method = 'ward.D2')
```

There is very little correlation between the variables. The only pairs with some
level correlation are:

  * `STARS` being positively correlated with `LabelAppeal`
    * This is interesting. Could it be that wine connoisseurs are impacted by
    the visual appearance of the label and not just the flavor?
  * `AcidIndex` having some positive correlation with `FixedAcidity`
    * One may have suspected a higher correlation, to be frank.
  * `AcidIndex` having some *negative* correlation with `STARS`
    * Wine reviewers may not like too much acidity, it seems.
    
# DATA PREPARATION
The different methods of imputation and feature generation will be discussed in
each models BUILD section. The only preparation performed here will be that
which will apply to all models equally.


# SELECT MODELS
## Model Selection Criteria
We will look at RMSE and AIC as our metrics and select the model which performs
best on both, Ties will be broken by \(R^2\).

# CODE APPENDIX
```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```
```{r loadData}
```
```{r missingVal}
```
```{r statsN}
```
```{r graphsD}
```
```{r graphsH}
```
```{r corrgram}
```
```{r dataClean}
```
```{r trainTestSplit}
```
```{r model1addVars}
```
```{r m2clean}
```
```{r m3clean}
```
```{r model1DummyFreq}
```
```{r model1RunFreq}
```
```{r model1DummySev}
```
```{r model1RunSev}
```
```{r model2RunFreq}
```
```{r model2RunSev}
```
```{r model3RunBaseLog}
```
```{r model3RunFreq}
```
```{r model3RunBaseLin}
```
```{r model3RunSev}
```
```{r setUpCompTables}
```
```{r validateM1F}
```
```{r validateM1S}
```
```{r validateM2F}
```
```{r validateM2S}
```
```{r validateM3F}
```
```{r validateM3S}
```
```{r freqComp}
```
```{r sevComp}
```
```{r badSev}
```
```{r evalProcessClean}
```
```{r evalPredictFreq}
```
```{r evalPredictSev}
```