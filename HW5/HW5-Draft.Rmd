---
title: "DATA 621 - Business Analytics and Data Mining"
subtitle: 'Fall 2020 - Group 2 - Homework #5'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "11/22/2020"
output:
  pdf_document:
  toc: TRUE
toc_depth: 4
urlcolor: purple
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE)
```

```{r loadData, include=FALSE}
# Load necessary libraries
library(ggplot2)
library(scales)
library(knitr)
library(caret)
library(corrplot)
library(data.table)

# Set master seed
set.seed(65408)

# Set filepaths for data ingestion
urlRemote  = "https://raw.githubusercontent.com/"
pathGithub = "aadler/DT621_Fall2020_Group2/master/HW5/data/"
fileTrain = "wine-training-data.csv"
fileTest = "wine-evaluation-data.csv"

# Read training file
DT <- fread(paste0(urlRemote, pathGithub, fileTrain))

# Number of training observations
ntrnobs <- dim(DT)[[1]]

# Get the names of the predictor variables
nmtrn <- names(DT)[-(1:2)]
nmtrnINT <- c('AcidIndex', 'LabelAppeal', 'STARS')
nmtrnDUB <- setdiff(nmtrn, nmtrnINT)
```

# ASSIGNMENT
The assignment for HW5 is to analyze and model a dataset containing
approximately 12,000 records representing commercially available wines. The
`TARGET` response variable represents the number of sample cases of wine
purchased by wine distribution companies after sampling that wine. These cases
would be used to provide tasting samples to restaurants and wine stores around
the United States. The more sample cases purchased, the more likely is a wine to
be sold at a high end restaurant.A large wine manufacturer is studying the data
in order to predict the number of wine cases ordered based upon the wine
characteristics. If the wine manufacturer can predict the number of cases, then
that manufacturer will be able to adjust their wine offering to maximize sales.

The objective of this assignment is to build a count regression model to predict
the number of cases of wine that will be sold given certain properties of the
wine. *HINT:* Sometimes, the fact that a variable is missing is actually
predictive of the target. You can only use the variables given to you (or
variables that you derive from the variables provided).

# DATA EXPLORATION
## Variables
The data is composed of the following variables:

|VARIABLE NAME|DEFINITION|THEORETICAL EFFECT|
|--|--|--|
|INDEX|Identification Variable (do not use)|None|
|TARGET|Number of Cases Purchased|None|
|AcidIndex|Proprietary method of testing total acidity of wine by using a weighted average||
|Alcohol|Alcohol Content||
|Chlorides|Chloride content of wine||
|CitricAcid|Citric Acid Content||
|Density|Density of Wine||
|FixedAcidity|Fixed Acidity of Wine||
|FreeSulfurDioxide|Sulfur Dioxide content of wine||
|LabelAppeal|Marketing Score indicating the appeal of label design for consumers. High numbers suggest customers like the label design. Negative numbers suggest customers don't like the design.|Many consumers purchase based on the visual appeal of the wine label design. Higher numbers suggest better sales.|
|ResidualSugar|Residual Sugar of wine||
|STARS|Wine rating by a team of experts. 4 Stars = Excellent, 1 Star = Poor|A high number of stars suggests high sales|
|Sulphates|Sulfate content of wine||
|TotalSulfurDioxide|Total Sulfur Dioxide of Wine||
|VolatileAcidity|Volatile Acid content of wine||
|pH|pH of wine||

There are `r ntrnobs` observations. All of these predictors are numeric, 
although `LabelAppeal`, `AcidIndex` and `STARS` all appear to be ordinal factors
and not true numerics. For the purpose of this assignment, we can treat them
as integers.

## Missing Data
There are a lot of missing values for some of the predictors.

```{r missingVal}
missingPreds <- transpose(DT[, lapply(.SD, function(x) {sum(is.na(x))}),
                             .SDcols = nmtrn],
                          keep.names = 'Predictors')
setnames(missingPreds, 'V1', 'Missing')
missingPreds[, Percentage := Missing / ntrnobs * 100]
setorder(missingPreds, -Missing)
kable(missingPreds, digits = 2L, caption = 'Predictors with Missing Observations')
```

`STARS` is especially sparse, but as noted in the assignment, this may be an
indication in and of its own. It is reasonable to assume that the vinters of a
good wine want it to be rated. If they do not submit it for rating, that may be
an indication of their lack of faith in the wine. How to factor this into the
analysis will be decided individually by the modelers in this assignment. The
other missing variables are all less than 10%, and their handling via imputation
or otherwise will be done on a model-by-model basis as well.

## Summary Statistics and Graphs
As all of the data are numeric, we can investigate the distributions of the
predictors both tabularly and graphically.

The numeric predictor variables have the following summary statistics, ignoring
missing values:

```{r statsN}
# Isolate numeric only predictors
predictorDT <- DT[, .SD, .SDcols = nmtrn]

# Melt them from wide to long format
predictorDTM <- melt(predictorDT, variable.name = 'metric',
                     value.name = 'value',  variable.factor = FALSE,
                     measure.vars = nmtrn)

# Calculate summary statistics
statsN <- predictorDTM[, .(Mean = mean(value, na.rm = TRUE),
                           SD = sd(value, na.rm = TRUE),
                           Min = min(value, na.rm = TRUE),
                           Q1 = quantile(value, prob = 0.25, na.rm = TRUE),
                           Median = median(value, na.rm = TRUE),
                           Q3 = quantile(value, prob = 0.75, na.rm = TRUE),
                           Max = max(value, na.rm = TRUE),
                           IQR = IQR(value, na.rm = TRUE)), keyby = 'metric']

# Print the table
kable(statsN, digits = 3L, align = 'r',
      caption = "Summary Statistitics for Numeric Variables")
```

A kernel-smoothed density plot of the distributions of the non-integer numeric
predictors is below, followed by a histogram of the integral-valued predictors.

```{r graphsD, fig.height=5, fig.width=7}
# Using epanechnikov kernel to generate kernel-smoothed densities
ggplot(predictorDTM[!is.na(metric) & metric %chin% nmtrnDUB], aes(x = value)) +
  geom_density(kernel = 'epanechnikov') +
  facet_wrap( ~ metric, scales = 'free') +
  ggtitle('Kernel-smoothed Density of Numeric Predictors')
```

The numeric predictors all look to be basically symmetrical, but non-Gaussian in
that there is a strong spike near the median, but the tails on either side are
thicker than would be for a normal distribution with that low of a standard
deviation. This is also why a boxplot would be a poor graphical indicator, as
the spike at the medians would lead to a compressed inter-quartile range and a
plethora of outliers.

```{r graphsH, fig.height=2, fig.width=7}
# Freedman-Diaconis rule for bin widths
FDbin <- function(x) {
  result <- 2 * IQR(x, na.rm = TRUE) / (length(x) ^ (1 / 3))
  return(ifelse(result == 0, 0.5, result))
}
# Generate histogram
ggplot(predictorDTM[!is.na(metric) & metric %chin% nmtrnINT], aes(x = value)) +
  geom_histogram(binwidth = FDbin, fill = 'indianred4') +
  facet_wrap( ~ metric, scales = 'free') +
  ggtitle('Histogram of Integral Predictors')
```

Of these three, `LabelAppeal` seems to be the most "normal" of the lot; `STARS`
and `AcidIndex` look to be more Poisson in shape.

## Correlations
The corrgram below graphically represents the correlations between the numeric
predictor variables, when ignoring the missing variables.

```{r corrgram, fig.width=5, fig.height=6.5}
# Create corrgram
corrplot::corrplot(cor(DT[, ..nmtrn], use = 'complete.obs'),
         method = 'ellipse', type = 'lower', order = 'hclust',
         hclust.method = 'ward.D2')
```

There is very little correlation between the variables. The only pairs with some
level correlation are:

  * `STARS` being positively correlated with `LabelAppeal`
    * This is interesting. Could it be that wine connoisseurs are impacted by
    the visual appearance of the label and not just the flavor?
  * `AcidIndex` having some positive correlation with `FixedAcidity`
    * One may have suspected a higher correlation, to be frank.
  * `AcidIndex` having some *negative* correlation with `STARS`
    * Wine reviewers may not like too much acidity, it seems.
    
## TARGET variable
Lastly, the complete dataset exhibits an average of
`r prettyNum(mean(DT$TARGET), digits = 3L)` cases being bought, with the
distribution below:

```{r targetH}
ggplot(DT[, .(TARGET)], aes(x = TARGET)) +
  geom_histogram(binwidth = FDbin, fill = 'darkolivegreen4')
```
    
# DATA PREPARATION
First, preparation necessary for all the models equally will be performed. The
subsequent imputation and feature generation, if necessary will be discussed in
a separate subsection for each modeler.

## Training & Testing Split
All the models will be trained on the same approximately 70% of the training
set, reserving 30% for validation of which model to select for the count
estimation on the supplied evaluation set.

```{r trainTestSplit}
# Create training and testing split
set.seed(1004)
trnIDX <- createDataPartition(DT$TARGET, p = 0.7)
trnX <- DT[trnIDX$Resample1, ]
tstX <- DT[!trnIDX$Resample1, ]
```

## Poisson Model 1 & Negative Binomial Model 1
### Missing Data
The missing data can be split into two groups: `STARS` and the rest. For all the
others, I do not think there is signal encoded in the "missingness" and thus
will use some form of imputation. `STARS` is different as discussed in the DATA
EXPLORATION section. Therefore, I will create a new feature called `Rated` which
will be true if `STARS` is missing, and then only use this variable and its
interaction with `STARS` in the model, and not `STARS` by itself. 

```{r model1addVars}
# Copy the training set to leave a pristine version for others
m1trnX <- copy(trnX)

# Add the "rated" factor variable
m1trnX[, Rated := factor(ifelse(is.na(STARS), 'Unrated', 'Rated'),
                         levels = c('Unrated', 'Rated'),
                         labels = c('Unrated', 'Rated'))]
```

### Imputation
Imputation will be handled through bagging. Instead of looking at a nearest
neighbors approach, which also requires centering and scaling, once can create a
set of bagged trees. As per Kuhn (2019):

>*For each predictor in the data, a bagged tree is created using all of the*
*other predictors in the training set. When a new sample has a missing*
*predictor value, the bagged model is used to predict the value. While, in*
*theory, this is a more powerful method of imputing, the computational costs*
*are much higher than the nearest neighbor technique.*

The bagged trees will use `STARS` as a predictor, as that contains valuable
information, but prior to the imputation, the missing `STARS` values will be
replaced by 0 so as not to be imputed, as per the explanation above. The
preprocessing will also check for near-zero value predictors.

```{r model1impute}
# Create bagged-tree imputation model
set.seed(89)
m1trnXi <- preProcess(m1trnX, method =c('nzv', 'bagImpute'))

# Replace missing STARS with 0s
m1trnX[is.na(STARS), STARS := 0L]

# Impute the remaining NAs
m1trnXimp <- predict(m1trnXi, newdata = m1trnX)
```

It will be instructive to compare the shapes of the distributions after
imputation to those before imputation.

```{r m1postImputeD, fig.height=5, fig.width=7}
# Isolate numeric only predictors
m1predictorDT <- m1trnXimp[, .SD, .SDcols = nmtrn]

# Melt them from wide to long format
m1predictorDTM <- melt(m1predictorDT, variable.name = 'metric',
                     value.name = 'value',  variable.factor = FALSE,
                     measure.vars = nmtrn)

# Using epanechnikov kernel to generate kernel-smoothed densities
ggplot(predictorDTM[metric %chin% nmtrnDUB], aes(x = value)) +
  geom_density(kernel = 'epanechnikov') +
  facet_wrap( ~ metric, scales = 'free') +
  ggtitle('Kernel-smoothed Density of Numeric Predictors')
```

Thankfully, none of the shapes appears to have changed significantly, which
implies the imputation was in line with the original distributions. Now for the
histograms.

```{r m1postImputeH, fig.height=2, fig.width=7}
# Generate histogram
ggplot(m1predictorDTM[metric %chin% nmtrnINT], aes(x = value)) +
  geom_histogram(binwidth = FDbin, fill = 'indianred4') +
  facet_wrap( ~ metric, scales = 'free') +
  ggtitle('Histogram of Integral Predictors')
```

Here too, `AcidIndex` and `LabelAppeal` look very similar to their
pre-imputation distribution. What stands out is `STARS`. Both the Poisson and
the negative binomial models are either unimodal or have to consecutive values
sharing the mode, due to their inherent nature as discrete disctibutions. What
is seen here is that 1 star is empirically less common than 0 (unrated) or 2
stars. On the one hand, this could simply be noise (process variance) or
parameter error (finite sample size). On the other, it could indicate the need
for a zero-inflated version of the counting distributions.

# BUILD MODELS
## Poisson Model 1 & Negative Binomial Model 1
The same person is building these two models, and the model setups will be
similar. Each model will follow a similar format, with only the family changing.
Each will start with the saturated model including all the individual predictors
***except*** for `STARS`. There will also be the following interactions:

  * `Rated` and `STARS`
    * As discussed above, by having `Rating` on its own and this interaction
    the effect of both having a rating and the effect of the rating level can be
    measured, without penalizing unrated wines a second time for a 0.
  * The full four-way interaction between `AcidIndex`, `FixedAcidity`,
  `VolatileAcidity`, and `pH`
    * There are **four** separate predictors relating to acidity. Despite their
    Pearson product-moment correlations being low, I am suspicious of too much
    weight being given to acidity. By including their interactions, effects can
    be tempered, or magnified, as necessary. `CitricAcid` is not included here
    as is not merely an acid but a flavor provider as well. Also, five-way
    interactions lead to migraines.
  * `FreeSulfurDioxide` and `TotalSulfurDioxide`
    * Again, a case of multiple theoretically related predictors
  * `LabelAppeal` with `Rated` and `LabelAppeal` with `Rated:STARS`
    * This is a later addition. The top three predictors in magnitude were
    `LabelAppeal`, `Rated`, and `Rated:STARS`. With all three being so strong,
    there may be interactions between them. However, as `STARS` alone has been
    removed from contention, only the interactions between the two others will
    be considered.

### Poisson Model # 1
For the Poisson model, a forward and backwards stepwise procedure based on AIC
will be used, and the model with the lowest AIC on the training set will be the
selected to be tested against the testing set. Using stepAIC precludes the use
of k-fold cross validation.

```{r m1PTrain}
# Using stepAIC so turn off cross-validation
trc <- trainControl(method = 'none')
set.seed(350047)
m1PFit <- train(TARGET ~ . + FreeSulfurDioxide:TotalSulfurDioxide +
                  AcidIndex * FixedAcidity * VolatileAcidity * pH +
                  Rated:STARS + LabelAppeal:Rated + LabelAppeal:Rated:STARS -
                  INDEX - STARS, data = m1trnXimp,
                trControl = trc, method = 'glmStepAIC',
                family = poisson(link = 'log'), direction = 'both', trace = 0)
# Save AIC
m1PFitAIC <- AIC(m1PFit$finalModel)
```

\small
```{r m1PTable}
# Print results. This needed to ne in its own section so that the LaTeX commands
# to change the text size can be wrapped around it.
kable(summary(m1PFit$finalModel)$coefficients,
      caption = "Model 1 Poisson Regression Output")
```
\normalsize

#### Model Checking

```{r m1PFitCheck}
m1PFitDisp <- sum(residuals(m1PFit$finalModel, type = 'pearson') ^ 2) /
  m1PFit$finalModel$df.residual
```

A Poisson model should have a dispersion parameter close to 1, as the Poisson by
definition has its mean and variance equal. This model has a dispersion
parameter of `r prettyNum(m1PFitDisp, digits = 3L)` which is deemed acceptable.

#### Coefficient Discussion
Similar to the master data set, the training data set has an empirical mean
purchased cases of `r prettyNum(mean(m1trnXimp$TARGET), digits = 3L)`. Having no
other information, the Poisson model expects about
`r prettyNum(exp(m1PFit$finalModel$coefficients[[1]]), digits = 3L)` cases, due
to the intercept value of `r m1PFit$finalModel$coefficients[[1]]`. This means
that the model identifies factors which lean more to *increasing* purchases than
to decreasing purchases.

The absolute magnitude of the coefficients makes sense. Factors related to
acidity tend to lower the number of cases bought. The factors with the greatest
magnitude effect are all positive, and they are the fact of being rated, and the
interactions between that and the rating level and label appeal. Keeping
everything else equal, merely being rated increases the number of cases by about
\(e^{0.6677926} \approx  1.95\)! Each additional star is predicted to contribute
another 1.2 cases to the total purchase. Interestingly, `LabelAppeal` has fallen
out of the model; its contributions are through its interactions. As surmised,
looking at all three individually may have proven too strong, as the three-way
interaction coefficient tempers the others.

All the coefficients are significant at at least the 5% level except for
Sulphates, but in terms of AIC, leaving it in provided a better model than did
taking it out.

#### Variable Importance
The further down the list below a variable preceded by a `-` is, the more
important it is, as its removal would cause a greater disruption to the
deviance and the AIC. The three rating variables, being rated, the rating level,
and the label appeal, are the most important variables when it comes to future
case purchases.

It is also interesting to note that there are a number of variables whose
inclusion as predictors would result in a lower deviance model than the
selected, but said drop in deviance was not enough to offset the possible
parameter error. These include `VolatileAcidity`, `LabelAppeal`, `pH`,
`Density`, `ResidualSugar`, `VolatileAcidity:pH`, and `CitricAcid`.

```
                                              Df Deviance   AIC
<none>                                             9550.7 31902
+ VolatileAcidity                              1   9549.4 31903
- Sulphates                                    1   9553.5 31903
- `FreeSulfurDioxide:TotalSulfurDioxide`       1   9553.7 31903
+ LabelAppeal                                  1   9549.9 31903
+ pH                                           1   9550.3 31904
+ Density                                      1   9550.4 31904
+ ResidualSugar                                1   9550.4 31904
+ `VolatileAcidity:pH`                         1   9550.5 31904
+ CitricAcid                                   1   9550.6 31904
- FixedAcidity                                 1   9554.6 31904
+ `FixedAcidity:pH:AcidIndex`                  1   9550.7 31904
+ `FixedAcidity:VolatileAcidity`               1   9550.7 31904
+ `FixedAcidity:pH`                            1   9550.7 31904
+ `FixedAcidity:VolatileAcidity:pH`            1   9550.7 31904
+ `VolatileAcidity:pH:AcidIndex`               1   9550.7 31904
+ `FixedAcidity:VolatileAcidity:AcidIndex`     1   9550.7 31904
+ `FixedAcidity:VolatileAcidity:pH:AcidIndex`  1   9550.7 31904
- `FixedAcidity:AcidIndex`                     1   9554.9 31904
- Alcohol                                      1   9555.1 31904
- Chlorides                                    1   9555.2 31904
- `pH:AcidIndex`                               1   9555.4 31905
- FreeSulfurDioxide                            1   9558.6 31908
- `LabelAppeal:STARS:RatedRated`               1   9565.2 31914
- TotalSulfurDioxide                           1   9568.7 31918
- `VolatileAcidity:AcidIndex`                  1   9569.6 31919
- AcidIndex                                    1   9585.3 31935
- `LabelAppeal:RatedRated`                     1   9719.7 32069
- `STARS:RatedRated`                           1  10152.1 32501
- RatedRated                                   1  10247.3 32597
```

### Negative Binomial Model # 1
```{r m1NBTrain, cache=TRUE}
glmc <- glm.control(maxit = 25, epsilon = 1e-12)
set.seed(72)
m1NBFit2a <- glm.nb(TARGET ~ . + FreeSulfurDioxide:TotalSulfurDioxide +
                      AcidIndex * FixedAcidity * VolatileAcidity * pH +
                      Rated:STARS + LabelAppeal:Rated +
                      LabelAppeal:Rated:STARS - INDEX - STARS,
                    data = m1trnXimp, control = glmc, link = log,
                    na.action = na.exclude)

m1NBFit2b <- stepAIC(m1NBFit2a, direction = 'both', trace = 1)

# Save AIC
m1NBFitAIC <- AIC(m1NBFit2b)
```

\small
```{r m1NBTable}
# Print results. This needed to ne in its own section so that the LaTeX commands
# to change the text size can be wrapped around it.
kable(summary(m1NBFit2b)$coefficients,
      caption = "Model 1 Negative Binomial Regression Output")
```
\normalsize

## Poisson Model #2

## Negative Binomial Model #2

## Multiple Linear Regrssion Model # 1

## Multiple Linear Regrssion Model # 2

# SELECT MODELS
## Model Selection Criteria
We will look at RMSE and MAE as our metrics and select the model which performs
best on both. If no model peforms best on both, the AIC of the model results on
the training set will be used to break the tie.

## Model Test Results

### Poisson Model 1

### Poisson Model 2

### Negative Binomial Model 1

### Negative Binomial Model 2

### Multiple Linear Regression Model 1

### Multiple Linear Regression Model 2

## Comparsion

## Selection

# PREDICTION

# REFERENCES

  * Kuhn, M. (2019, March 27). *The `caret` Package*.
  https://topepo.github.io/caret/pre-processing.html#imputation

# CODE APPENDIX
```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```
```{r loadData}
```
```{r missingVal}
```
```{r statsN}
```
```{r graphsD}
```
```{r graphsH}
```
```{r corrgram}
```
```{r targetH}
```
```{r trainTestSplit}
```
```{r model1addVars}
```
```{r model1impute}
```
```{r m1postImputeD}
```
```{r m1postImputeH}
```
```{r m1PTrain}
```
```{r m1PTable}
```
```{r m1PFitCheck}
```
