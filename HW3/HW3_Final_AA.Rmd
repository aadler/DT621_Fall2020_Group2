---
title: "DATA 621 - Business Analytics and Data Mining"
subtitle: 'Fall 2020 - Group 2 - Homework #3'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "10/11/2020"
output:
  pdf_document: default

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE)
```

```{r loadData, include=FALSE}
library(tidyverse)
library(tidymodels)
library(data.table)
library(stacks)
library(kernlab)
library(vip)
library(summarytools)
library(corrplot)
library(knitr)
library(rsample) # model 1 libraries
library(caret)
library(e1071)
library(geoR)


set.seed(9450)

urlRemote  = "https://raw.githubusercontent.com/"
pathGithub = "aadler/DT621_Fall2020_Group2/master/HW3/data/"
fileTrain = "crime-training-data_modified.csv"
fileTest = "crime-evaluation-data_modified.csv"

df <- read.csv(paste0(urlRemote, pathGithub, fileTrain))
nobs <- dim(df)[[1]]
DT <- as.data.table(df)
DT[, target := factor(target)]
# df <- as_tibble(df)
eval <- read.csv(paste0(urlRemote, pathGithub, fileTest))
```

# DATA EXPLORATION
## Data Description
The training data set contains 466 records summarizing attributes of various
neighborhoods in the city of Boston. The response variable is coded such that it
is `1` when the neighborhoodâ€™s crime rate is above the median and `0` when it is
not. In all, there are 12 predictors. These include:

Predictor Variables|Description   
-|----
zn | proportion of residential land zoned for large lots (over 25000 square feet) 
indus | proportion of non-retail business acres per suburb  
chas | a dummy var. for whether the suburb borders the Charles River (1) or not (0)
nox | nitrogen oxides concentration (parts per 10 million) 
rm | average number of rooms per dwelling   
age | proportion of owner-occupied units built prior to 1940   
dis | weighted mean of distances to five Boston employment centers  
rad | index of accessibility to radial highways   
tax | full-value property-tax rate per $10,000 
ptratio | pupil-teacher ratio by town 
lstat | lower status of the population (percent) 
medv | median value of owner-occupied homes in $1000s 

This data set has complete cases, thus there is no need for imputation. Based on
some common summary statistics, there are more observations where the crime rate
is below the median. It is already apparent that some of the predictors varies
depending the crime rate. For instance, there is a noticeable difference in the
means of `age`, `lstat`, `rad`, and `zn` between the crime rate groups. 

```{r sumstat}
# DATA EXPLORATION 
summarystat = stby(data = df, INDICES = df$target, FUN = psych::describe)
kable(summarystat[[1]][-13,-c(1,7)], 
      caption = "Descriptive Statistics: Crime Rate > Median", 
      digit = 2L)
kable(summarystat[[2]][-13,-c(1,7)], 
      caption = "Descriptive Statistics: Crime Rate < Median", 
      digit = 2L)
```

## Data Distribution
For each predictors, we computed kernel density estimators to understand their
distribution. The following plots show how predictors are distributed between
areas where the crime rate is higher than the median (blue) and areas where the
crime rate is below the median (red). It is of interest to understand variables
that highlight large variations between the two groups. 

```{r density, fig.width=6}
# density plots
DT[, IDX := .I]
DTM <- melt(DT, id.vars = c('IDX', 'target'), measure.vars = 1:12,
            variable.name = 'metric', value.name = 'value')
ggplot(DTM, aes(x = value, fill = target, color = target)) +
  geom_density(alpha = 0.4, show.legend = FALSE) +
  facet_wrap(~ metric, scales = 'free') 
```

The density plots reveal that most of the data does not have a normal
distribution. There are some are heavily right tailed variables and others which
are multi-modal. The most Gaussian of the variables appears to be the one
related to the average number of rooms per dwelling: `rm`. Another
interesting predictor is `zn`, the proportion of residential land zoned for
large lots. Nearly 73% of the observations have a value of 0 with the remaining
spread widly between 1% and 100%. This may the source of possible distortion. A
third highly-skewed variable is `chas`, the indicator as to whether the lot
borders the Charles River. Out of the `r nobs` observations,
`r sum(DT$chas == 0)` do **not** border the river.

To provide another view, this time highlighting outliers, the data was analyzed
using boxplos.

```{r boxplots, fig.width=6}
# boxplots 

ggplot(DTM, aes(x = metric, y = value)) + 
  geom_boxplot(color = "blue", fill = "blue", alpha = 0.2, notch = TRUE,
               notchwidth = 0.8, outlier.colour = "red", outlier.fill = "red",
               outlier.size = 3) + 
  stat_summary(fun.y = mean, color = "red", geom = "point", shape = 16,
               size = 2) + coord_flip() +
  labs(title = "Boxplot of Predictor Variables") + scale_y_log10()

ggplot(DTM, aes(x = metric, y = value)) + 
  geom_boxplot(alpha = 0.5, outlier.colour = "red", outlier.fill = "red",
               outlier.size = 2, aes(fill = target)) + 
  facet_wrap( ~ metric, scales = "free") + 
  labs(title = "Boxplot of Predictor Variables by Crime Rate") +
  scale_y_log10()
```

It is clear from the second boxplot, similar to the densities, that the
distribution of the predictor variables is different for for two outcomes. This
suggests that a model will be able to extract signal from the data.

## Data Correlation
Below is a corrgram of the data. The shape of the ellipse reflects the strength
of the correlation and the color represents the direction. Thus, the diagonal
which represents the perfect correlation of each variable with itself is a blue
diagional line. The shape and the color scheme reflect the guide at the bottom
of the image.

```{r correlation, fig.height=6, fig.width=6}
# correlations
corrplot(cor(df), method = 'ellipse', type = 'lower', order = 'hclust')
```

Looking at the corrgram, we see that there are many variables which are
moderately to highly correlated, $\left|\rho\right| > 0.50$. Of note is that
`nox` has a largest positive correlation with `target` and `dis` has the largest
negative correlation with `target`. Lastly, it seems that the variable `chas`,
which indicate whether the suburb borders the Charles River, has statistically
insignificant correlation with almost all of the other variables.

# DATA PREPARATION
There are no missing values so none need to be imputed.
Different preparation techniques may be used for differen models.
## Models 1 & 2
The variable `chas` will be removed due to having statistically insignificant\
correlation with almost all the variables in the dataset. Next, outliers will be
tempered by capping from below at the 5th percentile and from above at the 95th
percentile.

```{r, model1_2_adjust}
new_df <- subset(df, select = -chas)

# Cap all observations at their 5th and 95th percentiles
low5 <- apply(new_df, 2, quantile, prob = 0.05)
up95 <- apply(new_df, 2, quantile, prob = 0.95)
for (i in seq_along(new_df)) {
  new_df[, i] <- pmin(new_df[, i], up95[i])
  new_df[, i] <- pmax(new_df[, i], low5[i])
}
dens_ageO <- ggplot(new_df, aes(age)) + geom_density()
dens_lstatO <- ggplot(new_df, aes(lstat)) + geom_density()
dens_rmO <- ggplot(new_df, aes(rm)) + geom_density()

```

The Boxcox transformation will be applied to the `age`, `lstat`, and `rm`
variables.

```{r m12BC}
ageBC <- boxcoxfit(new_df$age)
lstatBC <- boxcoxfit(new_df$lstat)
rmBC <- boxcoxfit(new_df$rm)
```

The transform suggests a \(\lambda\) of `r ageBC$lambda` for `age`,
`r lstatBC$lambda` for `lstat`, and `r rmBC$lambda` for `rm`

```{r, m12applyBC}
new_df$age <- new_df$age ^ ageBC$lambda
new_df$lstat <- new_df$lstat ^ lstatBC$lambda
new_df$rm <- new_df$rm ^ rmBC$lambda
```

Below are the densities of the variables prior (left) and post (right) the
Box-Cox transform. The variables `lstat` and `rm` have benefitted from the
transform, but the strong bimodality of `age` has not been affected
significantly.

```{r, m12BC_plot}
dens_age <- ggplot(new_df, aes(age)) + geom_density()
dens_lstat <- ggplot(new_df, aes(lstat)) + geom_density()
dens_rm <- ggplot(new_df, aes(rm)) + geom_density()
grid.arrange(dens_ageO, dens_lstatO, dens_rmO, dens_age, dens_lstat, dens_rm,
             layout_matrix = cbind(c(0, 1, 2), c(3, 4, 5)))
```

## Model 3
# BUILD MODELS
## Models 1 & 2
First, we will split the training data into two sets. This will allow us to
perform a cross validation scheme on the models to tune parameters for optimum
performance. 

```{r m12_tts}
# BUILD MODELS
# train-test split
crime_df <- as.tibble(df) %>%
  mutate(target = as.factor(target)) %>% 
  mutate_if(is.integer, as.numeric)

data_split <- initial_split(crime_df, 
                            strata = target, 
                            prop = 0.8)
train_df<- training(data_split)
test_df<- testing(data_split)
```

## Model 1
### Base Model
We will start with a simple logistic regression model to serve as a baseline.
This includes all variables in the dataset. 

```{r baseModel1}
baseModel <- glm(target~., family = binomial, data = train_df)
knitr::kable(summary(baseModel)$coefficients, digits = 3L,
             caption = 'Base Model Logistic Regression Output')
```

We can immediately see that a few variables *exceed* the 0.05 p-value threshold
for significance. 

### Enhanced Model
We will use **backwards stepwise regression** to remove features that are not
statistically significant in predicting the target. The result is a model that
includes the following features: `zn`, `nox`, `age`, `dis`, `rad`, `tax`,
`ptratio`, `lstat`, and `medv`. 

### Coefficient Discussion
It is important to note that the coefficients from the model are predicting
whether or not the target variable is a **1** (the crime rate is *above* the
median value). Additionally, it must be noted that the numeric coefficients are
relative to the range of values that the variable encompasses. What this means
is that it's possible to have a coefficient that seems small when we look at the
absolute magnitude, but that actually has a very strong effect when applied to
the data.

```{r model1_coeff}
# 10-fold cross validation
train_control <- trainControl(method = "cv", number = 10)

# train the model on training set
model1 <- train(target ~ zn + nox + age + dis + rad + tax + ptratio + lstat + medv,
               data = train_df,
               trControl = train_control,
               method = "glm",
               family=binomial())
knitr::kable(summary(model1)$coefficients, digits = 3L,
             caption = 'Backwards Model Regression Output')
```

Let's take a look at the *signs* of the coefficients. We can see that the
coefficients for the variables `zn` and `tax` are negative. This is indicative
of an inverse relationship; ie, the higher these values, the less likely the
crime rate is above the median. The relationship for the zone variable aligns
with the findings from our initial data exploration. However, the relationship
for the tax variable does not; in fact, we saw that there was a positive
correlation between the two (as the tax rate increases, so does the probability
that the crime rate will be above the median).  
  
The rest of the variables have positive coefficients. Many of these are
expected. For example, in our exploratory data analysis, we noticed that the
`nox` variable (nitrogen oxides concentration (parts per 10 million)) has the
greatest positive correlation with the target variable. This is surprising; we
hadn't thought that the nitrogen oxide concentration would have as large of an
impact on the crime rate, but some quick googling shows us that there may
actually be a relationship
([https://www.sciencedaily.com/releases/2019/10/191003114007.htm](Source)).

## Model 2
### Stacked Classifier

```{r m2setup}
cv_folds <- vfold_cv(train_df, v = 10, repeats = 1)

crime_recipe <- recipe(target ~., data=train_df)

crime_wf<- workflow() %>%
  add_recipe(crime_recipe)

crtl_grid <- control_stack_grid()
```

Specify the models. Using SVM as another model to blend with the logistic. 

```{r m2svm}
logit_specs <- 
  logistic_reg(
    penalty = tune(),
    mixture = tune()
    ) %>%
  set_engine("glm") %>%
  set_mode("classification")

svm_specs <- 
  svm_rbf(
    cost = tune(),
    rbf_sigma = tune(),
    margin = tune()
  ) %>%
  set_mode("classification") %>%
  set_engine("kernlab")
```

Tuning the models for best parameters. The stack function will cherry pick the
models that best optimize the blended final model. 

```{r m2stack}
svm_wf <-
  crime_wf %>%
  add_model(svm_specs)

svm_results<-
  tune_grid(
    svm_wf, 
    resample = cv_folds,
    control = crtl_grid,
    grid = 10,
    save_pred = TRUE,
    save_workflow = FALSE
  )

logit_wf <- 
  crime_wf %>%
  add_model(logit_specs)

logit_results <-
  tune_grid(
    logit_wf,
    resamples = cv_folds,
    control = crtl_grid,
    grid = 10,
    save_pred = TRUE,
    save_workflow = FALSE
)
```

Blend!

```{r m2Blend}
crime_model_stack <- 
  stacks() %>%
  add_candidates(logit_results) %>%
  add_candidates(svm_results) %>%
  blend_predictions() %>%
  fit_members()

crime_model_stack
```

```{r m2Predict}
crime_pred <- 
  test_df %>%
  bind_cols(predict(crime_model_stack, ., type = "prob"), 
            predict(crime_model_stack, ., type = "class"))
```

Computing the metrics for the stacked model:

```{r m2Stats}
roc<- yardstick::roc_auc(
  crime_pred,
  truth = target,
  contains(".pred_1")
  )

acc<- yardstick::accuracy(
  crime_pred, 
  truth = target, 
  estimate = .pred_class
)

recall<- yardstick::recall(
  crime_pred,
  truth = target, 
  estimate = .pred_class
)

precise<- yardstick::precision(
  crime_pred,
  truth = target, 
  estimate = .pred_class
)
metrics_df<-
  bind_rows(roc, acc, recall, precise)

crime_pred %>%
  conf_mat(truth = target, estimate = .pred_class)

metrics_df
```

# SELECT MODELS
## Criteria

## Performance 

### Model 1
```{r model1_perf}

# SELECT MODELS
# Model 1 Performance
model1_preds <- predict(model1, test_df, type = "raw")
model1_probs <- predict(model1, test_df, type = "prob")
colnames(model1_probs) <- c('pred0', 'pred1')


model1_results <- test_df %>%
  bind_cols(pred = model1_preds, model1_probs)

####### metrics
m1_roc<- yardstick::roc_auc(
  model1_results,
  truth = target,
  pred0 # select the prob class that corresponds to first level of target
)

m1_acc<- yardstick::accuracy(
  model1_results, 
  truth = target, 
  estimate = pred
)

m1_recall<- yardstick::recall(
  model1_results,
  truth = target, 
  estimate = pred
)

m1_precise<- yardstick::precision(
  model1_results,
  truth = target, 
  estimate = pred
)

m1_metrics_df<-
  bind_rows(m1_roc, m1_acc, m1_recall, m1_precise)

m1_metrics_df


```

### Sub-Models and Coefficient Discussion

The blended model is comprised of two sub-models, the logistic and the SVM. Below are the prediction results for each sub-model. 

```{r}
best_model_metric <- select_best(svm_results, "accuracy")
finalize_workflow(svm_wf, best_model_metric) %>%
  last_fit(data_split) %>%
  collect_metrics()
```



```{r}
logit_wf %>%
  last_fit(data_split) %>%
  collect_metrics()

logit_wf %>%
  fit(data = train_df) %>%
  pull_workflow_fit() %>%
  vip(geom = "col", aesthetics = list(fill='red4'))
```

We see that they are both in the low eighties percent for accuracy, and low nineties for roc auc. Together make for a better overall prediction. 

We again see that the primary predictor in the logistic regression is the `nox` with `rad` and `ptratio` rounding out the top three predictors. This is in line with the base model. Living close to the center of town, having teachers, and not having polluted air are the largest predictors in terms of above median crime. 


# APPENDIX
The code chunks below represent the R code called in order during the analysis. They are reproduced in the appendix for review and comment.
```{r appendix, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```
```{r loadData}
```
```{r sumstat}
```
```{r density}
```
```{r boxplot}
```
```{r correlation}
```
