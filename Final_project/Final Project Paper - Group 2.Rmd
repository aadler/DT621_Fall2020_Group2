---
title: "DATA 621 - Business Analytics and Data Mining"
subtitle: 'Fall 2020 - Group 2 - Final Project Paper'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "12/14/2020"
output:
  pdf_document:
    extra_dependencies:
      amsmath: null
      inputenc: utf8
      xcolor: dvipsnames
      setspace: singlespacing
    toc: FALSE
    toc_depth: 3
urlcolor: purple
bibliography: finalprojectrefs.bib
csl: ieee-with-url.csl
abstract: "Using data from YouTube, we attempt to predict the number of views a video will receive using criteria such as location, category, number of likes, number of dislikes, and number of comments. Using forms of linear regression we have covered this semester, we will test various combinations of features for predictive power. [FILL IN OUTCOMES HERE]\\par\\textbf{Keywords:} Youtube, linear regression, count regression, R"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE)
```
```{r loadLibraries}
library(jsonlite)
library(knitr)
library(stringr)
library(ggplot2)
library(scales)
library(caret)
library(data.table)
```
```{r loadData}
currentPath <- getwd()
dataPath <- "Data2"
us_path <- file.path(currentPath, dataPath, "USvideos.csv")

colClass <- c(rep('character', 4L), 'integer', 'POSIXct', 'character',
              rep('double', 4L), rep('logical', 3L))
us_set <- fread(us_path, encoding = 'UTF-8', colClasses = colClass)
nobsUS <- nrow(us_set)

# Category IDs
us_cat_path <- file.path(currentPath, dataPath, "US_category_id.json")
us_cats <- fromJSON(us_cat_path)

us_cats <- data.table(id = as.integer(us_cats$items$id),
                      category = us_cats$items$snippet[, 2])
setkey(us_cats, id)
```

# Introduction
YouTube has changed the future of video entertainment forever [@Moylan2015ADo]. 
In 2019, the platform was estimated to have between \$16 billion and \$25
billion in revenue [@Wakabayashi2019YIa]. YouTube’s model connects a user’s
creativity with a desire for global recognition [@GoogleUtY]. Before YouTube,
international fame was not conceivable outside of a standard television or movie
studio. Today, creators from all over the world are gaining international
prominence using their own equipment and space. “YouTube is central to today’s
video ecosystem,” says Enders Analysis research analyst Jamie McGowan Stuart
[@Foster2020Yt1]. The current top channel “Vlad and Nikita” earns around \$312
thousand per video [@TYL]. According to a survey from Google, 6 out of 10 people
already prefer online video platforms or streaming services over live TV
[@OBTlv]. There are researchers predicting that by 2025---four years now---half
of viewers under the age of 32 will not pay TV service [@McQuivey2015B25].
Understanding some of the underpinnings of what generates views is beneficial to
anyone entering, or already in, the YouTube world.

The remainder of this paper will cover a literature review, an overview of out
methodology, the specifics of our modeling, a discussion of our findings,
thoughts for future work, a statistical appendix, a detailed code appendix, and
finally our references.

# Literature Review
With the popularity of Youtube, this is not a new question. Approaches this
question in the greater data science network include those based on more
advanced machine learning techniques such as SGD or neural net classifiers
[@LEZ2019YVP]. Others leveraged NLP and specially engineered features such as
"clickbait" or "NSFW" tags [@Srinivasan2017YVP]. These attempts usually used the
same dataset as we are.

Reviewing more academic literature uncovers research into the use of Support
Vector Regression with various basis functions on Youtube and/or Facebook videos
[@TR2017PPo; @PAG2013UEV]. Other attempts included building multi-stage treed
regression models where the outcome of a first stage determined which specific
second-stage model would be used for final popularity prediction [@OLL2016API].

These approaches usually added a temporal element to their analysis, and used
"earlier" values to predict later views. Using more sophisticated algorithms and
temporal elements tended to return statistically significant models. The
downside of these approaches are their complexity and opaqueness, of course. Our
approach will necessarily be simpler, although likely more transparent, being
restricted to the family of linear models covered in this course and not
regressing over time.

# Methodology
Using the famous data set from Kaggle [@Mitchell2019TYV], we will explore
relationships between a video’s views and the number of likes, dislikes, and
comments using `R` [@RCT2020RAL] and the `caret` package [@Kuhn2020cCa]. We may
also use a video’s category as predictors.

We manually scrubbed the data and discovered that the country-specific files
really were not! They neither refer to videos created by country nor do they
refer to views *specific* to country. Rather they are the total number of views
and other predictors for that video on that day as collected by someone within
that country. Meaning that aggregation is almost always multiplying. There may
be some videos unique to a specific country---one which was not viewed in other
countries, but they are many magnitudes smaller than those seen by all. As the
United States had the most observations, we decided to analyze its data set.

With the data, we identified both numeric and factor predictors, and engineered
features for convenience as well. We will use these features to investigate
relationships with actual views using linear regression models.

Given the models, we will compare the RMSE, \(R^2\), and MAE on a holdout set
and will select the model that performs best as the winner for this paper. We do
not expect it to outperform more sophisticated models.

# Experimentation & Results
## Data Exploration
Since we are not factoring in time, it is incorrect to use all the observations.
Therefore, we will extract the latest observation by video by country and use
this subset.

```{r dataPrep}
# Select the row numbers of the first entry of the latest trending date by title
# and by country. There are 34 duplicates. Use this as our restricted data set
# corresponding to the most recent view count. The inner set of brackets gets
# the row number (called V1) and the outer set is a simple subset by those row
# numbers.
usExtract <- us_set[
  us_set[, .I[which.max(views)], by = c('title')]$V1
  ]
nobsUSExt <- dim(usExtract)[[1]]

# Substitute category for numeric ID through joining
usExtract <- us_cats[usExtract, on = 'id == category_id']

# Change data types for convenience, count tags and then get rid of actual tags
# and replace category IDs with actual categories
usExtract[, `:=`(trending_date = as.IDate(trending_date, format = "%y.%d.%m"),
                 views = as.double(views),
                 tag_count = 1L + str_count(tags, '[|]'),
                 id = NULL,
                 category = as.factor(category))
          ][, `:=`(tags = NULL,
                   category = relevel(category, 'Entertainment'))]

# Make name shorter for display purposes
setnames(usExtract,
         c('comment_count', 'tag_count', 'comments_disabled',
           'ratings_disabled', 'video_error_or_removed'),
         c('comments', 'tags', 'cmtDisabled', 'rtgDisabled', 'vidError'))

# Categorize the variable names
numVars <- c('views', 'likes', 'dislikes', 'comments', 'tags')

# Melt for numerics
usSetN <- melt(usExtract, measure.vars = numVars, variable.factor = FALSE,
               variable.name = 'measure', value.name = 'value')

# Melt for category
usSetF <- melt(usExtract, measure.vars = 'category', variable.factor = FALSE,
               variable.name = 'measure', value.name = 'value')

# Table of summary statistics
statsN <- usSetN[, .(Mean = mean(value, na.rm = TRUE),
                     SD = sd(value, na.rm = TRUE),
                     Min = min(value, na.rm = TRUE),
                     Q1 = quantile(value, prob = 0.25, na.rm = TRUE),
                     Median = median(value, na.rm = TRUE),
                     Q3 = quantile(value, prob = 0.75, na.rm = TRUE),
                     Max = max(value, na.rm = TRUE),
                     IQR = IQR(value, na.rm = TRUE)),
                 keyby = c('measure')]
```

### Target Variable

```{r targDens}
ggplot(usSetN[measure == 'views'], aes(x = value)) +
  geom_density(kernel = "epanechnikov") +
  ggtitle("Target Density (semi-log scale)") +
  scale_x_log10(label = scientific)
```

On a log scale, `views` looks rather Gaussian, which implies it has a lognormal
distribution.

### Numeric Predictors
For numeric predictors, we are using the number of `likes`, `dislikes`, `tags`,
and `comments`. We may also consider the country of viewing as well. From the 
There is not that much difference in the distribution of the numeric predictors
between the countries where people view. 

```{r numLogDensities, fig.height=4.5, fig.width=8}
ggplot(usSetN[measure != 'views'], aes(x = value, color = measure)) +
  geom_density(kernel = "epanechnikov") +
  ggtitle("Predictor Density (semi-log scale)") +
  scale_x_log10(label = scientific)
```

As expected, the number of tags is orders of magnitude less than the rating or
comment variables. On average there are more likes than comments and more
comments then dislikes, but all three exhibit symmetric Gaussian-like behavior.

### Categories
While tabular representation of factor predictors is difficult, a distribution
of videos by categories may prove informative. It is clear from the graphs below
that interests vary by category. `Entertainment` seems to be the most common in
the US with `Music` coming in second. 

```{r catPlot, fig.height=4.5, fig.width=8}
ggplot(usSetF[measure == 'category'], aes(y = value)) +
  geom_bar() +
  ggtitle("Distribution by Category") +
  scale_x_continuous(labels = comma)
```

## Feature Selection & Engineering
We will consider the relationship between `views` and the numerical predictors
of `likes`, `dislikes`, `comments`, and `tags`. We will also consider the
`category`, whether or not comments or ratings were disabled and, if there was
an error with the video.

```{r likeDislikeCheck}
ldOutlier <- usExtract[!rtgDisabled & dislikes == 0 & likes == 0]
ldOn <- dim(ldOutlier)[[1]]
```

We will add two features. The first is `balance`: the ratio between the
likes and dislikes. The hypothesis is that if a video is either universally
loved or panned, it will get fewer views than if there is a healthy disagreement
about it. 

To minimize division by 0 errors, `balance` is defined as follows:
\[
balance =\begin{cases}
\mathrm{ratings\;enabled}\quad\frac{\min(likes, dislikes)}{\max(likes, dislikes)}\\
\mathrm{ratings\;disabled}\quad 1
\end{cases}
\]
This constrains the ratio to the interval \([0, 1]\) with a maximum of 1 when
the two are equal. Now, division by 0 can only occure when both are 0. This 
usually occurs when ratings are disabled. There is only `r ldOn` case out of the
`r nobsUSExt` observations where the ratings were not disabled, yet there are
neither likes nor dislikes. As this is a distinct incongruity for YouTube, we
will remove that one observation from the data. When ratings are disabled,
perforce there is no disparity so the ratio will be set to 1.

The second is `exceedMean`. This will be a factor if the views for the video
are greater than the *mean* views for all of the extracted data set. This will
also indicate the skewness of the data. Were it symmetrical, and we know it is
not from the graphs above, the mean should be near the median and this factor
should be close to equal for both 0 and 1. 

```{r addFeatures}
# Remove the outlier
usExtract <- usExtract[!(!rtgDisabled & dislikes == 0 & likes == 0)]

# Precaclulate the mean views
usMean <- mean(usExtract$views)

# Add the engineered feature
usExtract[, `:=`(balance = ifelse(rtgDisabled, 1,
                               pmin(likes, dislikes) / pmax(likes, dislikes)),
                 exceedMean = factor(ifelse(views > usMean, 1, 0),
                                     levels = c(0, 1), labels = c(0, 1)))]
```
```{r plotBalance, fig.height=4, fig.width=8}
ggplot(usExtract[, .(balance)], aes(x = balance)) +
  geom_density() +
  ggtitle("Distribution of Balance")
```

It's pretty clear from the distribution of `balance` that, at least for videos
viewed in the US, there is a healthy dose of disagreement!

```{r plotExceedMean, fig.height=4, fig.width=8}
# Melt for exceedMean
usSetF <- melt(usExtract, measure.vars = 'exceedMean', variable.factor = FALSE,
               variable.name = 'measure', value.name = 'value')

# Plot the exceedMean variable
ggplot(usSetF[measure == 'exceedMean'], aes(x = value)) +
  geom_bar() +
  ggtitle("Proportion Above and Below Mean") +
  scale_y_continuous(labels = comma)
```

As with many events which follow the Pareto rule, most of the views are 
attributable to a minority of the videos, as seen by the 0/1 disparity.

## Model Building & Interpretation
We will first separate 20% of the data as a true holdout set. It is on this data
that our models will be compared. We will train models on the remaining 80% of
the data.

```{r holdout}
# Create seen and hidden sets
set.seed(617)
seenIDX <- createDataPartition(usExtract$views, p = 0.8)$Resample1
seenSet <- usExtract[seenIDX, ]
hideSet <- usExtract[-seenIDX, ]
```

### Simple Linear Regression
Linear regression may be the best known algorithm used when analyzing a
continuous numeric outcome. It searches for a linear relationship of the
predictors that minimizes the squared error between the "predictor" function and
the observations [@Sheather2009AMA].

This model will start with the numeric and logical features and the engineered
disparity ratio. It will not use the exceeding mean indicator, as that is
generated from the target variable, and it is felt it will distort the
prediction to regress `views` on a function of `views`. The algorithm will
proceed through feature selection using the AIC as the optimization metric.

```{r dummyVarsAA}
# Create dummy Variables. These will be used for the next three models
modDum <- dummyVars(views ~ likes + dislikes + comments + tags + category +
                   cmtDisabled + rtgDisabled + vidError + disp,
                   data = seenSet, fullRank = TRUE)
seenX <- predict(modDum, seenSet)
seenY <- seenSet$views
```

```{r lm1Train}
# Using stepAIC means no cross-validation. Train on entire dataset.
trC <- trainControl(method = 'none')
set.seed(181)
lm1 <- train(x = seenX, y = seenY, family = gaussian(link = 'identity'),
             method = 'glmStepAIC', direction = 'both', trace = 0,
             trControl = trC)
```

\footnotesize
```{r lm1Table}
kable(summary(lm1$finalModel)$coefficients,
      caption = "Model 1 Linear Regression Output",
      digts = 3L, format.args = list(big.mark = ','))
```
\normalsize

The signs of these coefficients make sense in the main. Increased `likes` and
`dislikes` are correlated with increased views. Of course one usually views a
video at least once prior to rating it. Interestingly, increased `comments` are
negatively correlated with views. 

We set the baseline category to `Entertainment`, considering it was the most
popular. Therefore we expected negative coefficients for other categories found
significant. We were surprised that `Film & Animation` was an exception. The
"worst" category predictor by magnitude is clearly `Nonprofits & Activism`.

The factor predictors tend to have much higher magnitude coefficients than do
the numeric ones. This makes sense. The amount of `likes`, `dislikes`, and
`comments` are many orders of magnitude greater than 1. Therefore, their
coefficients can be much smaller. A Boolean variable is either 1 or 0, therefore
its coefficient is much greater even if its actual contribution is lower.

When ratings are disabled, there is a bump to views. This is probably because
the intercept is artificially low due to the predictive power of `likes` and
`dislikes`. To balance that when there are none needs a big boost.

What may be most interesting is that the disparity ratio indicator us a very
powerful and significant indicator. Conflict seems to be good for Youtube
videos. The more a video is argued over, the more views it seems to get!

### Generalized Linear Model: Gaussian Errors & Log Link
The generalized linear model (GLM) is an extension of the simple linear model,
but the errors can be distributed per any member of the exponential family and
the relationship between some function of the predictors---called the link---and
the mean needs to be linear, not that the mean itself must be linear in the
predictors [@Faraway2006EtL]. 

The model under consideration here assumes a Gaussian distribution of the
errors, but a multiplicative relationship between the mean and the predictors.
This is expressed by using a log link function. This is **not** the canonical
link function for the Gaussian GLM, but as we are using numerical methods there
is no issue.

This approach *appears* similar to that of the common technique of performing a
standard linear regression on the logs of the observations, but is different. As
per [@Gelman2006Lta], one approach *"…log transforms observed values, while the*
*second one log transforms the expected value.…the key difference being the*
*relation between the predicted value and the variance."*

```{r lm2Train}
set.seed(181)
lm2 <- train(x = seenX, y = seenY, family = gaussian(link = 'log'),
             method = 'glmStepAIC', direction = 'both', trace = 0,
             trControl = trC)
```

\footnotesize
```{r lm2Table}
kable(summary(lm2$finalModel)$coefficients,
      caption = "Model 3 Linear Regression Output",
      digts = 3L, format.args = list(big.mark = ','))
```
\normalsize

This model has more predictors than does the simple Gaussian (G)LM. Moreover,
the signs are different from the simple LM. The very fact that
`Nonprofits & Activism` has a positive coefficient should raise concerns. This
model is probably a poor one.

### ElasticNet: Penalized Regression
Instead of using AIC to select features, one can make use of penalized
regression. Using an \(L_1\) penalty is the underpinnings of Lasso regression,
which can perform feature selection. Using a squared error term, \(L_2\), is at
the heart of ridge regression [@HTF2009TEo]. Using both methods together is
called the elastic net [@ZH2005RaV; @ZH2020eEN]. To tune the hyperparameters,
which includes the weighting between Lasso and Ridge, we will use 10-fold
cross-validation.

```{r lm3Train}
trC <- trainControl(method = 'cv', number = 10L)
tG <- expand.grid(fraction = seq(0.85, 1, 0.001),
                  lambda = seq(0.01, 0.025, 0.001))
set.seed(181)
lm3 <- train(x = seenX, y = seenY,
             method = 'enet', trControl = trC, tuneGrid = tG)
```

```{r lm3Table}
# The only way to extract the coefficients from an elastic net model in R
# Is to predict them using the optimal tuning values

lm3C <- predict(lm3$finalModel, s = lm3$finalModel$tuneValue[[1]],
                type = 'coefficients', mode = 'fraction')
lm3CC <- lm3C$coefficients[lm3C$coefficients != 0]
kable(lm3CC, digits = 3L, format.args = list(big.mark = ','),
      caption = 'Coefficients of "Optimal" Elastic Net Model')
```

There is no clean table of coefficients with elasticNet. Rather there is a
sequence of models built behind the scenes. The coefficients of the selected
model can be found through predicting them, but there are no corresponding
p- or Z-values. Worse, there is no intercept returned.

Nevertheless, the *relative* magnitude and sign of the parameters are in line
with our expectations. Both `likes` and `dislikes` are positively correlated
with views and `comments` is negatively correlated. A few more categories join
`Film & Animation` as contributing to excess views over the baseline
`Entertainment`. But `Nonprofits & Activism` has the largest factor by far, and
it is negative, which stands in contrast to the second model. Lastly, having a
larger disparity (ratio of likes to dislikes closer to 1of likes tends to reduce views.
Conflict seems to be good for Youtubers!

## Model Evaluation

```{r modTest}
tstDum <- dummyVars(views ~ likes + dislikes + comments + tags + category +
                   cmtDisabled + rtgDisabled + vidError + disp,
                   data = hideSet, fullRank = TRUE)
hideX <- predict(tstDum, hideSet)
hideY <- hideSet$views
lm1P <- predict(lm1, newdata = hideX)
lm2P <- predict(lm2, newdata = hideX)
lm3P <- predict(lm3, newdata = hideX)
compTable <- data.table(Model = c('LM', 'GLM: Gauss+Log', 'ElasticNet'),
                        RMSE = c(RMSE(lm1P, hideY), RMSE(lm2P, hideY),
                                 RMSE(lm3P, hideY)),
                        R2 = c(R2(lm1P, hideY, formula = 'traditional'),
                               R2(lm2P, hideY, formula = 'traditional'),
                               R2(lm3P, hideY, formula = 'traditional')),
                        MAE = c(MAE(lm1P, hideY), MAE(lm2P, hideY),
                                MAE(lm3P, hideY)))
kable(compTable, digits = 3L, format.args = list(big.mark = ','),
      caption = "Model Performance on Test Set")
```

# Discussion & Conclusions
The three models contain both intuitive and counter-intuitive results. As
expected, videos with more ratings tend to have more views. However, as likes
outnumber dislikes, that tends to reduce the number of views. Having a video
witk

# Appendix
## Statistical Appendix
A more detailed analysis of the empirical statistics for the numeric data is 
found in the table below.

\footnotesize
```{r statsTable}
kable(statsN, digits = 2,
      caption = "Statistics for Numeric Variables by Country & Global",
      format.args = list(big.mark = ","))
```
\normalsize

## Code Appendix
The code chunks below represent the R code called in order during the analysis.
They are reproduced in the appendix for review and comment.

```{r appendix, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

```{r loadLibraries}
```
```{r loadData}
```
<!-- Data Exploration -->
```{r dataPrep}
```
```{r numLogDensities}
```
```{r catPlot}
```
```{r subsetExtraction}
```
```{r likeDislikeCheck}
```
```{r addFeatures}
```
```{r plotBalance}
```
```{r plotExceedMean}
```

<!-- Model Building -->
```{r holdout}
```
```{r dummyVarsAA}
```
```{r lm1Train}
```
```{r lm1Table}
```
```{r lm2Train}
```
```{r lm2Table}
```
```{r lm3Train}
```

<!-- Model Evaluation -->
```{r modTest}
```

<!-- Statistical Appendix-->
```{r statsTable}
```

<!-- References needs to be last -->  
# References
