---
title: "DATA 621 - Business Analytics and Data Mining"
subtitle: 'Fall 2020 - Group 2 - Final Project Paper'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "12/14/2020"
output:
  pdf_document:
    extra_dependencies:
      amsmath: null
      inputenc: utf8
      xcolor: dvipsnames
      setspace: singlespacing
    toc: FALSE
    toc_depth: 3
urlcolor: purple
bibliography: finalprojectrefs.bib
csl: ieee-with-url.csl
abstract: "Using data from YouTube, we attempt to predict the number of views a video will receive using criteria such as location, category, number of likes, number of dislikes, and number of comments. Using forms of linear regression we have covered this semester, we will test various combinations of features for predictive power. [FILL IN OUTCOMES HERE]\\par\\textbf{Keywords:} Youtube, linear regression, count regression, R"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE)
```
```{r loadLibraries}
library(jsonlite)
library(knitr)
library(stringr)
library(ggplot2)
library(scales)
library(caret)
library(data.table)
```
```{r loadData}
currentPath <- getwd()
dataPath <- "Data2"
us_path <- file.path(currentPath, dataPath, "USvideos.csv")

colClass <- c(rep('character', 4L), 'integer', 'POSIXct', 'character',
              rep('double', 4L), rep('logical', 3L))
us_set <- fread(us_path, encoding = 'UTF-8', colClasses = colClass)
nobsUS <- nrow(us_set)

# Category IDs
us_cat_path <- file.path(currentPath, dataPath, "US_category_id.json")
us_cats <- fromJSON(us_cat_path)

us_cats <- data.table(id = as.integer(us_cats$items$id),
                      category = us_cats$items$snippet[, 2])
setkey(us_cats, id)
```

# Introduction
YouTube has changed the future of video entertainment forever [@Moylan2015ADo]. 
In 2019, the platform was estimated to have between \$16 billion and \$25
billion in revenue [@Wakabayashi2019YIa]. YouTube’s model connects a user’s
creativity with a desire for global recognition [@GoogleUtY]. Before YouTube,
international fame was not conceivable outside of a standard television or movie
studio. Today, creators from all over the world are gaining international
prominence using their own equipment and space. “YouTube is central to today’s
video ecosystem,” says Enders Analysis research analyst Jamie McGowan Stuart
[@Foster2020Yt1]. The current top channel “Vlad and Nikita” earns around \$312
thousand per video [@TYL]. According to a survey from Google, 6 out of 10 people
already prefer online video platforms or streaming services over live TV
[@OBTlv]. There are researchers predicting that by 2025---four years now---half
of viewers under the age of 32 will not pay TV service [@McQuivey2015B25].
Understanding some of the underpinnings of what generates views is beneficial to
anyone entering, or already in, the YouTube world.

The remainder of this paper will cover a literature review, an overview of out
methodology, the specifics of our modeling, a discussion of our findings,
thoughts for future work, a statistical appendix, a detailed code appendix, and
finally our references.

# Literature Review
With the popularity of Youtube, this is not a new question. Approaches this
question in the greater data science network include those based on more
advanced machine learning techniques such as SGD or neural net classifiers
[@LEZ2019YVP]. Others leveraged NLP and specially engineered features such as
"clickbait" or "NSFW" tags [@Srinivasan2017YVP]. These attempts usually used the
same dataset as we are.

Reviewing more academic literature uncovers research into the use of Support
Vector Regression with various basis functions on Youtube and/or Facebook videos
[@TR2017PPo; @PAG2013UEV]. Other attempts included building multi-stage treed
regression models where the outcome of a first stage determined which specific
second-stage model would be used for final popularity prediction [@OLL2016API].

These approaches usually added a temporal element to their analysis, and used
"earlier" values to predict later views. Using more sophisticated algorithms and
temporal elements tended to return statistically significant models. The
downside of these approaches are their complexity and opaqueness, of course. Our
approach will necessarily be simpler, although likely more transparent, being
restricted to the family of linear models covered in this course and not
regressing over time.

# Methodology
Using the famous data set from Kaggle [@Mitchell2019TYV], we will explore
relationships between a video’s views and the number of likes, dislikes, and
comments. We may also use a video’s country of origin and the category or type
of video as predictors.

We manually scrubbed the data and discovered that the country-specific files
really were not! They neither refer to videos created by country nor do they
refer to views *specific* to country. Rather they are the total number of views
and other predictors for that video on that day as collected by someone within
that country. Meaning that aggregation is almost always multiplying. There may
be some videos unique to a specific country---one which was not viewed in other
countries, but they are many magnitudes smaller than those seen by all. As the
United States had the most observations, we decided to analyze its data set.

With the data, we identified both numeric and factor predictors, and engineered
features for convenience as well. We will use these features to investigate
relationships with actual views using linear regression models.

Given the models, we will compare the RMSE, \(R^2\), and MAE on a holdout set
and will select the model that performs best as the winner for this paper. We do
not expect it to outperform more sophisticated models.

# Experimentation & Results
## Data Exploration

```{r dataPrep}
# Substitute category for numeric ID through joining
us_set <- us_cats[us_set, on = 'id == category_id']

# Change data types for convenience, add target bucket, count tags and then get
# rid of actual tags and category IDs

us_mean <- mean(us_set$views)

us_set[, `:=`(trending_date = as.IDate(trending_date, format = "%y.%d.%m"),
              views = as.double(views),
              tag_count = 1L + str_count(us_set$tags, '[|]'),
              view_target = factor(ifelse(views > us_mean, 1L, 0L),
                                   levels = c(0, 1), labels = c(0, 1)),
              id = NULL,
              category = as.factor(category))
       ][, `:=`(tags = NULL,
                category = relevel(category, 'Entertainment'))]

# Make name shorter for display purposes
setnames(us_set, c('comment_count', 'tag_count'), c('comments', 'tags'))

# Categorize the variable names
boolVars <- c('comments_disabled', 'ratings_disabled', 'video_error_or_removed')
numVars <- c('views', 'likes', 'dislikes', 'comments', 'tags')
facVars <- c('category', 'view_target')
dateVars <- c('trending_date', 'publish_time')
charVars <- c('video_id', 'title', 'channel_title')

# Melt for numerics
usSetN <- melt(us_set, measure.vars = numVars, variable.factor = FALSE,
               variable.name = 'measure', value.name = 'value')

# Melt for factors
usSetF <- melt(us_set,
               measure.vars = c('category', 'view_target'),
               variable.factor = FALSE, variable.name = 'measure',
               value.name = 'value')

# Table of summary statistics
statsN <- usSetN[, .(Mean = mean(value, na.rm = TRUE),
                     SD = sd(value, na.rm = TRUE),
                     Min = min(value, na.rm = TRUE),
                     Q1 = quantile(value, prob = 0.25, na.rm = TRUE),
                     Median = median(value, na.rm = TRUE),
                     Q3 = quantile(value, prob = 0.75, na.rm = TRUE),
                     Max = max(value, na.rm = TRUE),
                     IQR = IQR(value, na.rm = TRUE)),
                 keyby = c('measure')]
```

### Target Variable

```{r targDens}
ggplot(usSetN[measure == 'views'], aes(x = value)) +
  geom_density(kernel = "epanechnikov") +
  ggtitle("Target Density (semi-log scale)") +
  scale_x_log10(label = scientific)
```

On a log scale, `views` looks rather Gaussian, which implies it has a lognormal
distribution.

### Numeric Predictors
For numeric predictors, we are using the number of `likes`, `dislikes`, `tags`,
and `comments`. We may also consider the country of viewing as well. From the 
There is not that much difference in the distribution of the numeric predictors
between the countries where people view. 

```{r numLogDensities, fig.height=4.5, fig.width=8}
ggplot(usSetN[measure != 'views'], aes(x = value, color = measure)) +
  geom_density(kernel = "epanechnikov") +
  ggtitle("Predictor Density (semi-log scale)") +
  scale_x_log10(label = scientific)
```

As expected, the number of tags is orders of magnitude less than the rating or
comment variables. On average there are more likes than comments and more
comments then dislikes, but all three exhibit symmetric Gaussian-like behavior.

### Factor Predictors
While tabular representation of factor predictors is difficult, a distribution
of factors by value may prove informative. It is clear from the graphs below
that interests vary by category. `Entertainment` seems to be the most common in
the US with `Music` coming in second. 

```{r factorPlot1, fig.height=4.5, fig.width=8}
ggplot(usSetF[measure == 'category'], aes(y = value)) +
  geom_bar() +
  ggtitle("Distribution by Category") +
  scale_x_continuous(labels = comma)
```

```{r factorPlot2, fig.height=3, fig.width=8}
ggplot(usSetF[measure != 'category'], aes(x = value, fill = measure)) +
  geom_bar(position = 'dodge2') +
  ggtitle("Country and Global Mean Views Exceedence") +
  scale_y_continuous(labels = comma)
```

As with many events which follow the Pareto rule, most of the views are 
attributable to a minority of the videos, as seen by the 0/1 disparity.

## Feature Engineering & Selection
Since we are not factoring in time, it is incorrect to use all the observations.
Therefore, we will extract the latest observation by video by country and use
this subset.

```{r subsetExtraction}
# Select the row numbers of the first entry of the latest trending date by title
# and by country. There are 34 duplicates. Use this as our restricted data set
# corresponding to the most recent view count. The inner set of brackets gets
# the row number (called V1) and the outer set is a simple subset by those row
# numbers.
usExtract <- us_set[
  us_set[, .I[which.max(views)], by = c('title')]$V1
  ]
```

We will consider the relationship between `views` and the numerical predictors
of `likes`, `dislikes`, `comments`, and `tags`. We will also consider the
`category`, whether or not comments or ratings were disabled and, if there was
an error with the video.

A feature that we will also consider is `lkratio`: the ratio of likes to
dislikes. This will help ascertain if it is the magnitude or the preponderance
which has a greater effect. As there may be some rare videos with no dislikes,
the ratios will be set to 1. The variable `unan`, short for unanimous, will
reflect that some videos have likes but no dislikes, to balance the ratio of 1
which should have been `NaN`, a division by 0 error.

```{r addFeatures}
usExtract[, `:=`(lkratio = ifelse(dislikes == 0, 1, likes / dislikes),
                 unan = ifelse(dislikes == 0 & likes > 0, TRUE, FALSE))]

modDum <- dummyVars(views ~ likes + dislikes + comments + tags + category +
                   comments_disabled + ratings_disabled +
                   video_error_or_removed + lkratio + unan,
                   data = usExtract, fullRank = TRUE)
modMat <- predict(modDum, usExtract)
```

## Model Building & Interpretation
We will first separate 20% of the data as a true holdout set. It is on this data
that our models will be compared. We will train models on the remaining 80% of
the data.

```{r holdOut}
set.seed(617)
seenIDX <- createDataPartition(usExtract$views, p = 0.8)$Resample1
seenX <- modMat[seenIDX, ]
seenY <- usExtract$views[seenIDX]
hideX <- as.data.frame(modMat[-seenIDX, ])
hideY <- usExtract$views[-seenIDX]
```

### Simple Linear Regression
Linear regression may be the best known algorithm used when analyzing a
continuous numeric outcome. It searches for a linear relationship of the
predictors that minimizes the squared error between the "predictor" function and
the observations [@Sheather2009AMA]. This model will engage in stepwise feature
selection using the AIC as the optimization metric.

```{r lm1Train}
# Using stepAIC means no cross-validation. Train on entire dataset.
trC <- trainControl(method = 'none')
set.seed(181)
lm1 <- train(x = seenX, y = seenY, family = gaussian(link = 'identity'),
             method = 'glmStepAIC', direction = 'both', trace = 0,
             trControl = trC)
```

\footnotesize
```{r lm1Table}
kable(summary(lm1$finalModel)$coefficients,
      caption = "Model 1 Linear Regression Output",
      digts = 3L, format.args = list(big.mark = ','))
```
\normalsize

The signs of these coefficients make sense in the main. Increased `likes` and
`dislikes` are correlated with increased views. Of course one usually views a
video at least once prior to rating it. Increased `comments` is negatively
correlated with views. 

We set the baseline category to `Entertainment`, considering it was the most
popular. Therefore we expected negative coefficients for other categories found
significant. We were surprised that `Film & Animation` was an exception. The
"worst" category predictor by magnitude is the `Nonprofits & Activism`.

The factor predictors tend to have much higher magnitude coefficients than do
the numeric ones. This makes sense. The amount of `likes`, `dislikes`, and
`comments` are many orders of magnitude greater than 1. Therefore, their
coefficients can be much smaller. A Boolean variable is either 1 or 0, therefore
its coefficient is much greater even if its actual contribution is lower.

### Generalized Linear Model: Gaussian Errors & Log Link
The generalized linear model (GLM) is an extension of the simple linear model,
but the errors can be distributed per any member of the exponential family and
the relationship between some function of the predictors---called the link---and
the mean needs to be linear, not that the mean itself must be linear in the
predictors [@Faraway2006EtL]. 

The model under consideration here assumes a Gaussian distribution of the
errors, but a multiplicative relationship between the mean and the predictors.
This is expressed by using a log link function. This is **not** the canonical
link function for the Gaussian GLM, but as we are using numerical methods there
is no issue.

This approach *appears* similar to that of the common technique of performing a
standard linear regression on the logs of the observations, but is different. As
per [@Gelman2006Lta], one approach *"…log transforms observed values, while the*
*second one log transforms the expected value.…the key difference being the*
*relation between the predicted value and the variance."*

```{r lm2Train}
set.seed(181)
lm2 <- train(x = seenX, y = seenY, family = gaussian(link = 'log'),
             method = 'glmStepAIC', direction = 'both', trace = 0,
             trControl = trC)
```

\footnotesize
```{r lm2Table}
kable(summary(lm2$finalModel)$coefficients,
      caption = "Model 3 Linear Regression Output",
      digts = 3L, format.args = list(big.mark = ','))
```
\normalsize

### ElasticNet: Penalized Regression
Instead of using AIC to select features, one can make use of penalized
regression. Using an \(L_1\) penalty is the underpinnings of Lasso regression,
which can perform feature selection. Using a squared error term, \(L_2\), is at
the heart of ridge regression [@HTF2009TEo]. Using both methods together is
called the elastic net [@ZH2005RaV]. To tune the hyperparameters, which includes
the weighting between Lasso and Ridge, we will use 10-fold cross-validation.

```{r lm3Train}
trC <- trainControl(method = 'cv', number = 10L)
tG <- expand.grid(fraction = seq(0.8, 1, 0.001),
                  lambda = seq(0, 0.02, 0.001))
set.seed(181)
lm3 <- train(x = seenX, y = seenY,
             method = 'enet', trControl = trC, tuneGrid = tG)
```

There is no clean table of coefficients with elasticNet. Rather there is a
sequence of models built behind the scenes. The coefficients of the selected
model can be found in the appendix.

## Model Evaluation

```{r modTest}
lm1P <- predict(lm1, newdata = hideX)
lm2P <- predict(lm2, newdata = hideX)
lm3P <- predict(lm3, newdata = hideX)
compTable <- data.table(Model = c('LM', 'GLM: Gauss+Log', 'ElasticNet'),
                        RMSE = c(RMSE(lm1P, hideY), RMSE(lm2P, hideY),
                                 RMSE(lm3P, hideY)),
                        R2 = c(R2(lm1P, hideY, formula = 'traditional'),
                               R2(lm2P, hideY, formula = 'traditional'),
                               R2(lm3P, hideY, formula = 'traditional')),
                        MAE = c(MAE(lm1P, hideY), MAE(lm2P, hideY),
                                MAE(lm3P, hideY)))
kable(compTable, digits = 3L, format.args = list(big.mark = ','),
      caption = "Model Performance on Test Set")
```

# Discussion & Conclusions

# Appendix
## Statistical Appendix
A more detailed analysis of the empirical statistics for the numeric data is 
found in the table below.

\footnotesize
```{r statsTable}
kable(statsN, digits = 2,
      caption = "Statistics for Numeric Variables by Country & Global",
      format.args = list(big.mark = ","))
```
\normalsize

## Code Appendix
The code chunks below represent the R code called in order during the analysis.
They are reproduced in the appendix for review and comment.

```{r appendix, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

```{r loadLibraries}
```
```{r loadData}
```
```{r dataPrep}
```
```{r numLogDensities}
```
```{r factorPlot1}
```
```{r factorPlot2}
```

<!-- Feature Engineering & Selection -->
```{r subsetExtraction}
```
```{r addFeatures}
```

<!-- Model Building -->
```{r holdOut}
```
```{r lm1Train}
```
```{r lm1Table}
```
```{r lm2Train}
```
```{r lm2Table}
```
```{r lm3Train}
```

<!-- Model Evaluation -->
```{r modTest}
```

<!-- Statistical Appendix-->
```{r statsTable}
```

<!-- References needs to be last -->  
# References
