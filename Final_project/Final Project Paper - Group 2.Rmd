---
title: "DATA 621 - Business Analytics and Data Mining"
subtitle: 'Fall 2020 - Group 2 - Final Project Paper'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "12/11/2020"
output:
  pdf_document:
    extra_dependencies:
      amsmath: null
      inputenc: utf8
      xcolor: dvipsnames
      setspace: singlespacing
    toc: TRUE
    toc_depth: 3
urlcolor: purple
bibliography: finalprojectrefs.bib
csl: ieee-with-url.csl
abstract: "Using data from YouTube, we attempt to predict the number of views a video will receive using criteria such as location, category, number of likes, number of dislikes, and number of comments. Using forms of linear regression we have covered this semester, we will test various combinations of features for predictive power. YouTubers are advised to allow viewers to interact via ratings and to post content which will engender heated discussion. \\par\\textbf{Keywords:} Youtube, linear regression, elastic net, R"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE)
```
```{r loadLibraries}
library(jsonlite)
library(knitr)
library(stringr)
library(ggplot2)
library(scales)
library(caret)
library(data.table)
```
```{r loadData}
currentPath <- getwd()
dataPath <- "Data2"
us_path <- file.path(currentPath, dataPath, "USvideos.csv")

colClass <- c(rep('character', 4L), 'integer', 'POSIXct', 'character',
              rep('double', 4L), rep('logical', 3L))
us_set <- fread(us_path, encoding = 'UTF-8', colClasses = colClass)
nobsUS <- nrow(us_set)

# Category IDs
us_cat_path <- file.path(currentPath, dataPath, "US_category_id.json")
us_cats <- fromJSON(us_cat_path)

us_cats <- data.table(id = as.integer(us_cats$items$id),
                      category = us_cats$items$snippet[, 2])
setkey(us_cats, id)
```

# Introduction
YouTube has changed the future of video entertainment forever [@Moylan2015ADo]. 
In 2019, the platform was estimated to have between \$16 billion and \$25
billion in revenue [@Wakabayashi2019YIa]. YouTube’s model connects a user’s
creativity with a desire for global recognition [@GoogleUtY]. Before YouTube,
international fame was not conceivable outside of a standard television or movie
studio. Today, creators from all over the world are gaining international
prominence using their own equipment and space. “YouTube is central to today’s
video ecosystem,” says Enders Analysis research analyst Jamie McGowan Stuart
[@Foster2020Yt1]. The current top channel “Vlad and Nikita” earns around
\$312,000 per video [@TYL]. According to a survey from Google, 6 out of 10
people already prefer online video platforms or streaming services over live TV
[@OBTlv]. There are researchers predicting that by 2025---four years now---half
of viewers under the age of 32 will not pay TV service [@McQuivey2015B25].
Understanding some of the underpinnings of what generates views is beneficial to
anyone entering, or already in, the YouTube world.

The remainder of this paper will cover a literature review, an overview of out
methodology, the specifics of our modeling, a discussion of our findings,
thoughts for future work, a statistical appendix, a detailed code appendix, and
finally our references.

# Literature Review
With the popularity of Youtube, this is not a new question. Approaches this
question in the greater data science network include those based on more
advanced machine learning techniques such as SGD or neural net classifiers
[@LEZ2019YVP]. Others leveraged NLP and specially engineered features such as
"clickbait" or "NSFW" tags [@Srinivasan2017YVP]. These attempts often used the
same dataset as we are using.

Reviewing more academic literature uncovers research into the use of Support
Vector Regression with various basis functions on Youtube and/or Facebook videos
[@TR2017PPo; @PAG2013UEV]. Other attempts included building multi-stage treed
regression models where the outcome of a first stage determined which specific
second-stage model would be used for final popularity prediction [@OLL2016API].

These approaches usually added a temporal element to their analysis, and used
"earlier" values to predict later views. Using more sophisticated algorithms and
temporal elements tended to return statistically significant models. The
downside of these approaches are their complexity and opaqueness, of course. Our
approach will necessarily be simpler, although likely more transparent, being
restricted to the family of linear models covered in this course and not
regressing over time.

# Methodology
Using the famous data set from Kaggle [@Mitchell2019TYV], we will explore
relationships between a video’s views and the number of likes, dislikes, and
comments using `R` [@RCT2020RAL] and the `caret` package [@Kuhn2020cCa]. We may
also use a video’s category as predictors.

We manually scrubbed the data and discovered that the country-specific files
really were not! They neither refer to videos created by country nor do they
refer to views *specific* to country. Rather they are the total number of views
and other predictors for that video on that day as collected by someone within
that country. Meaning that aggregation is almost always multiplying. There may
be some videos unique to a specific country---one which was not viewed in other
countries, but they are many magnitudes smaller than those seen by all. As the
United States had the most observations, we decided to analyze its data set.

With the data, we identified both numeric and factor predictors, and engineered
features for convenience as well. We will use these features to investigate
relationships with actual views using linear regression models.

Given the models, we will compare the RMSE, \(R^2\), and MAE on a holdout set
and will select the model that performs best as the winner for this paper. We do
not expect it to outperform more sophisticated models.

# Experimentation & Results
## Data Exploration
Since we are not factoring in time, it is incorrect to use all the observations.
Therefore, we will extract the latest observation by video by country and use
this subset.

```{r dataPrep}
# Select the row numbers of the first entry of the latest trending date by title
# and by country. There are 34 duplicates. Use this as our restricted data set
# corresponding to the most recent view count. The inner set of brackets gets
# the row number (called V1) and the outer set is a simple subset by those row
# numbers.
usExtract <- us_set[
  us_set[, .I[which.max(views)], by = c('title')]$V1
  ]
nobsUSExt <- dim(usExtract)[[1]]

# Substitute category for numeric ID through joining
usExtract <- us_cats[usExtract, on = 'id == category_id']

# Change data types for convenience, count tags and then get rid of actual tags
# and replace category IDs with actual categories
usExtract[, `:=`(trending_date = as.IDate(trending_date, format = "%y.%d.%m"),
                 views = as.double(views),
                 tag_count = 1L + str_count(tags, '[|]'),
                 id = NULL,
                 category = as.factor(category))
          ][, `:=`(tags = NULL,
                   category = relevel(category, 'Entertainment'))]

# Make name shorter for display purposes
setnames(usExtract,
         c('comment_count', 'tag_count', 'comments_disabled',
           'ratings_disabled', 'video_error_or_removed'),
         c('comments', 'tags', 'cmtDisabled', 'rtgDisabled', 'vidError'))

# Categorize the variable names
numVars <- c('views', 'likes', 'dislikes', 'comments', 'tags')

# Melt for numerics
usSetN <- melt(usExtract, measure.vars = numVars, variable.factor = FALSE,
               variable.name = 'measure', value.name = 'value')

# Melt for category
usSetF <- melt(usExtract, measure.vars = 'category', variable.factor = FALSE,
               variable.name = 'measure', value.name = 'value')

# Table of summary statistics
statsN <- usSetN[, .(Mean = mean(value, na.rm = TRUE),
                     SD = sd(value, na.rm = TRUE),
                     Min = min(value, na.rm = TRUE),
                     Q1 = quantile(value, prob = 0.25, na.rm = TRUE),
                     Median = median(value, na.rm = TRUE),
                     Q3 = quantile(value, prob = 0.75, na.rm = TRUE),
                     Max = max(value, na.rm = TRUE),
                     IQR = IQR(value, na.rm = TRUE)),
                 keyby = c('measure')]
```

### Target Variable

```{r targDens}
ggplot(usSetN[measure == 'views'], aes(x = value)) +
  geom_density(kernel = "epanechnikov") +
  ggtitle("Target Density (semi-log scale)") +
  scale_x_log10(label = scientific)
```

On a log scale, `views` looks rather Gaussian, which implies it has a lognormal
distribution.

### Numeric Predictors
For numeric predictors, we are using the number of `likes`, `dislikes`, `tags`,
and `comments`. 

```{r numLogDensities, fig.height=4.5, fig.width=8}
ggplot(usSetN[measure != 'views'], aes(x = value, color = measure)) +
  geom_density(kernel = "epanechnikov") +
  ggtitle("Predictor Density (semi-log scale)") +
  scale_x_log10(label = scientific)
```

As expected, the number of tags is orders of magnitude less than the rating or
comment variables. On average there are more likes than comments and more
comments then dislikes, but all three exhibit symmetric Gaussian-like behavior.

### Categories
While tabular representation of factor predictors is difficult, a distribution
of videos by categories may prove informative. It is clear from the graphs below
that interests vary by category. `Entertainment` seems to be the most common in
the US with `Music` coming in second. 

```{r catPlot, fig.height=4.5, fig.width=8}
ggplot(usSetF[measure == 'category'], aes(y = value)) +
  geom_bar() +
  ggtitle("Distribution by Category: United States") +
  scale_x_continuous(labels = comma)
```

## Feature Selection & Engineering
We will consider the relationship between `views` and the numerical predictors
of `likes`, `dislikes`, `comments`, and `tags`. We will also consider the
`category`, whether or not comments or ratings were disabled and, if there was
an error with the video.

```{r likeDislikeCheck}
ldOutlier <- usExtract[!rtgDisabled & dislikes == 0 & likes == 0]
ldOn <- dim(ldOutlier)[[1]]
```

We will add two features. The first is `balance`: the ratio between the
likes and dislikes. The hypothesis is that if a video is either universally
loved or panned, it will get fewer views than if there is a healthy disagreement
about it. 

To minimize division by 0 errors, `balance` is defined as follows:
\[
balance =\begin{cases}
\mathrm{ratings\;enabled}\quad\frac{\min(likes, dislikes)}{\max(likes, dislikes)}\\
\mathrm{ratings\;disabled}\quad 1
\end{cases}
\]
This constrains the ratio to the interval \([0, 1]\) with a maximum of 1 when
the two are equal. Now, division by 0 can only occur when both are 0. This 
usually occurs when ratings are disabled. There is only `r ldOn` case out of the
`r nobsUSExt` observations where the ratings were not disabled, yet there are
neither likes nor dislikes. As this is a distinct incongruity for YouTube, we
will remove that one observation from the data. When ratings are disabled,
perforce there is no disparity so the ratio will be set to 1.

<!-- The second is `exceedMean`. This will be a factor if the views for the video -->
<!-- are greater than the *mean* views for all of the extracted data set. This will -->
<!-- also indicate the skewness of the data. Were it symmetrical, and we know it is -->
<!-- not from the graphs above, the mean should be near the median and this factor -->
<!-- should be close to equal for both 0 and 1.  -->

The second is `engagement`. This will be the ratio of comments to sum of likes and 
dislikes, which should give us some measure of comments to ratings. We will use a
similar approach as `balance` in terms of constraining the interval to \([0,1])\). 

```{r addFeatures}
# Remove the outlier
usExtract <- usExtract[!(!rtgDisabled & dislikes == 0 & likes == 0)]

# Precaclulate the mean views
usMean <- mean(usExtract$views)

# Add the engineered feature
usExtract[, `:=`(balance = ifelse(rtgDisabled, 1,
                               pmin(likes, dislikes) / pmax(likes, dislikes)),
                 engagement = ifelse(rtgDisabled, 1, 
                                     comments / sum(likes, dislikes)))]
                 # exceedMean = factor(ifelse(views > usMean, 1, 0),
                 #                     levels = c(0, 1), labels = c(0, 1)))
          

```
```{r plotBalance, fig.height=4, fig.width=8}
ggplot(usExtract[, .(balance)], aes(x = balance)) +
  geom_density() +
  ggtitle("Distribution of Balance")

ggplot(usExtract[, .(engagement)], aes(x = engagement)) +
  geom_density() +
  scale_x_log10() +
  ggtitle("Distribution of Engagement Log Scale")

```

It's pretty clear from the distribution of `balance` that, at least for videos
viewed in the US, there is a healthy dose of disagreement!

<!-- ```{r plotExceedMean, fig.height=4, fig.width=8} -->
<!-- # Melt for exceedMean -->
<!-- usSetF <- melt(usExtract, measure.vars = 'exceedMean', variable.factor = FALSE, -->
<!--                variable.name = 'measure', value.name = 'value') -->

<!-- # Plot the exceedMean variable -->
<!-- ggplot(usSetF[measure == 'exceedMean'], aes(x = value)) + -->
<!--   geom_bar() + -->
<!--   ggtitle("Proportion Above and Below Mean") + -->
<!--   scale_y_continuous(labels = comma) -->
<!-- ``` -->

<!-- As with many events which follow the Pareto rule, most of the views are  -->
<!-- attributable to a minority of the videos, as seen by the 0/1 disparity. -->

## Model Building & Interpretation
We will first separate 20% of the data as a true holdout set. It is on this data
that our models will be compared. We will train models on the remaining 80% of
the data.

```{r holdout}
# Create seen and hidden sets
set.seed(617)
seenIDX <- createDataPartition(usExtract$views, p = 0.8)$Resample1
seenSet <- usExtract[seenIDX, ]
hideSet <- usExtract[-seenIDX, ]
```

### Simple Linear Regression
Linear regression may be the best known algorithm used when analyzing a
continuous numeric outcome. It searches for a linear relationship of the
predictors that minimizes the squared error between the "predictor" function and
the observations [@Sheather2009AMA].

This model will start with the numeric and logical features and the engineered
disparity ratio. It will not use the exceeding mean indicator, as that is
generated from the target variable, and it is felt it will distort the
prediction to regress `views` on a function of `views`. The algorithm will
proceed through feature selection using the AIC as the optimization metric.

```{r dummyVarsAA}
# Create dummy Variables. These will be used for the next three models
modDum <- dummyVars(views ~ likes + dislikes + comments + tags + category +
                   cmtDisabled + rtgDisabled + vidError + balance + engagement,
                   data = seenSet, fullRank = TRUE)
seenX <- predict(modDum, seenSet)
seenY <- seenSet$views
```

```{r lm1Train}
# Using stepAIC means no cross-validation. Train on entire dataset.
trC <- trainControl(method = 'none')
set.seed(181)
lm1 <- train(x = seenX, y = seenY, family = gaussian(link = 'identity'),
             method = 'glmStepAIC', direction = 'both', trace = 0,
             trControl = trC)
```

\footnotesize
```{r lm1Table}
kable(summary(lm1$finalModel)$coefficients,
      caption = "Model 1 Linear Regression Output",
      digts = 3L, format.args = list(big.mark = ','))
```
\normalsize

The signs of these coefficients make sense in the main. Increased `likes` and
`dislikes` are correlated with increased views. Of course one usually views a
video at least once prior to rating it. Interestingly, increased `comments` are
negatively correlated with views. 

We set the baseline category to `Entertainment`, considering it was the most
popular. Therefore we expected negative coefficients for other categories found
significant. We were surprised that `Film & Animation` was an exception. The
"worst" category predictor by magnitude is clearly `Nonprofits & Activism`.

The factor predictors tend to have much higher magnitude coefficients than do
the numeric ones. This makes sense. The amount of `likes`, `dislikes`, and
`comments` are many orders of magnitude greater than 1. Therefore, their
coefficients can be much smaller. A Boolean variable is either 1 or 0, therefore
its coefficient is much greater even if its actual contribution is lower.

When ratings are disabled, there is a bump to views. This is probably because
the intercept is artificially low due to the predictive power of `likes` and
`dislikes`. To balance that when there are none needs a big boost.

What may be most interesting is that the disparity ratio indicator us a very
powerful and significant indicator. Conflict seems to be good for Youtube
videos. The more a video is argued over, the more views it seems to get!

### Generalized Linear Model: Gaussian and Poisson Errors with Log Link
The generalized linear model (GLM) is an extension of the simple linear model,
but the errors can be distributed per any member of the exponential family and
the relationship between some function of the predictors---called the link---and
the mean needs to be linear, not that the mean itself must be linear in the
predictors [@Faraway2006EtL]. 

The models under consideration here assume either a Poisson or Gaussian 
distribution of the errors, but a multiplicative relationship between the
mean and the predictors. This is expressed by using a log link function. 
This is **not** the canonical link function for the GLM distributions,
but as we are using numerical methods there is no issue.

This approach *appears* similar to that of the common technique of performing a
standard linear regression on the logs of the observations, but is different. As
per [@Gelman2006Lta], one approach *"…log transforms observed values, while the*
*second one log transforms the expected value.…the key difference being the*
*relation between the predicted value and the variance."*

```{r lm2Train}
set.seed(181)
lm2 <- train(x = seenX, y = seenY, family = gaussian(link = 'log'),
             method = 'glmStepAIC', direction = 'both', trace = 0,
             trControl = trC)
```


```{r lm3Train_poi}
set.seed(9450)
lm3_test<- train(x = seenX, y = seenY, family = poisson(link='log'),
            method = 'glmStepAIC', direction = 'both', trace = 0,
            trControl = trC)
```

\footnotesize
```{r poi_coeff}
kable(summary(lm3_test$finalModel)$coefficients,
      caption = "Model 3 (Poisson) Linear Regression Output",
      digts = 3L, format.args = list(big.mark = ','))
```
\normalsize


\footnotesize
```{r lm2Table}
kable(summary(lm2$finalModel)$coefficients,
      caption = "Model 3 (Gaussian) Linear Regression Output",
      digts = 3L, format.args = list(big.mark = ','))
```
\normalsize

These models have more predictors than the simple Gaussian (G)LM. Moreover,
the signs are different from the simple LM. The very fact that
`Nonprofits & Activism` has a positive coefficient should raise concerns. These
models are probably poor ones.

### ElasticNet: Penalized Regression
Instead of using AIC to select features, one can make use of penalized
regression. Using an \(L_1\) penalty is the underpinnings of Lasso regression,
which can perform feature selection. Using a squared error term, \(L_2\), is at
the heart of ridge regression [@HTF2009TEo]. Using both methods together is
called the elastic net [@ZH2005RaV; @ZH2020eEN]. To tune the hyperparameters,
which includes the weighting between Lasso and Ridge, we will use 10-fold
cross-validation.

```{r lm3Train}
trC <- trainControl(method = 'cv', number = 10L)
tG <- expand.grid(fraction = seq(0.85, 1, 0.001),
                  lambda = seq(0.01, 0.025, 0.001))
set.seed(181)
lm3 <- train(x = seenX, y = seenY,
             method = 'enet', trControl = trC, tuneGrid = tG)
```

```{r lm3Table}
# The only way to extract the coefficients from an elastic net model in R is to
# predict them using the optimal tuning values

lm3C <- predict(lm3$finalModel, s = lm3$finalModel$tuneValue[[1]],
                type = 'coefficients', mode = 'fraction')
lm3CC <- lm3C$coefficients[lm3C$coefficients != 0]
kable(lm3CC, digits = 3L, format.args = list(big.mark = ','),
      caption = 'Coefficients of "Optimal" Elastic Net Model')
```

There is no clean table of coefficients with elasticNet. Rather there is a
sequence of models built behind the scenes. The coefficients of the selected
model can be found through predicting them, but there are no corresponding
p- or Z-values. Worse, there is no intercept returned.

Nevertheless, the *relative* magnitude and sign of the parameters are in line
with our expectations. Both `likes` and `dislikes` are positively correlated
with views and `comments` is negatively correlated. A few more categories join
`Film & Animation` as contributing to excess views over the baseline
`Entertainment`. But `Nonprofits & Activism` has the largest factor by far, and
it is negative, which stands in contrast to the second model. Lastly, having a
higher balance---ratio of likes to dislikes closer to 1---tends to increase
views. Conflict seems to be good for YouTubers!

## Model Evaluation

```{r modTest}
# Process the hidden set
tstDum <- dummyVars(views ~ likes + dislikes + comments + tags + category +
                   cmtDisabled + rtgDisabled + vidError + balance + engagement,
                   data = hideSet, fullRank = TRUE)
hideX <- predict(tstDum, hideSet)
hideY <- hideSet$views

# Predict using the models
lm1P <- predict(lm1, newdata = hideX)
lm2P <- predict(lm2, newdata = hideX)
lm3P <- predict(lm3, newdata = hideX)
lm3_testP <- predict(lm3_test, newdata = hideX)
# Compare the results
compTable <- data.table(Model = c('LM', 'GLM: Gauss+Log', 'ElasticNet', 'GLM: Poisson+Log '),
                        RMSE = c(RMSE(lm1P, hideY), RMSE(lm2P, hideY),
                                 RMSE(lm3P, hideY), RMSE(lm3_testP, hideY)),
                        R2 = c(R2(lm1P, hideY, formula = 'traditional'),
                               R2(lm2P, hideY, formula = 'traditional'),
                               R2(lm3P, hideY, formula = 'traditional'),
                               R2(lm3_testP, hideY, formula = 'traditional')),
                        MAE = c(MAE(lm1P, hideY), MAE(lm2P, hideY),
                                MAE(lm3P, hideY), MAE(lm3_testP, hideY)))
kable(compTable, digits = 3L, format.args = list(big.mark = ','),
      caption = "Model Performance on Test Set")
```

# Discussion & Conclusions
The three models contain both intuitive and counter-intuitive results. As
expected, videos with more ratings tend to have more views. However, as likes
outnumber dislikes, that tends to reduce the number of views. All the models
agree that having a video whose likes and dislikes are close in magnitude
increases the propensity for views.
 
The elasticNet model performed best, and it's clear that the log link is
inferior to the identity link for this data set. If there is any clear takeaways
for prospective YouTubers, it would be:

  * Make sure your videos will engender heated discussions. You want as many
  *dislikes* as likes. That seems to drive attention to your video!
  * Keep your ratings enabled. Allowing users to rate your video drives
  attention to them!
  * Perhaps disable comments, though.
  * Stay **away** from activism. 
  * Talk about films or comics. This probably includes the anime and manga
  phenomena.
  * Having more tags is better than having fewer.

The main limitation of this analysis is that the linear model, even with
penalization, is probably not the best model for human behavior. There are
likely non-linear effects and tipping points which could be much better captured
by tree-based models or even SVMs with the appropriate kernel.

Also, this analysis may be valuable for someone planning a video, but it does
not help them react once the video is posted, as we did not investigate any
changes over time.

Lastly, we encountered problems trying to aggregate disjoint data by country
without falling prey to gross overcounting.

Areas of future work would include investigating more sophisticated algorithms,
analyzing not only final views but growth patterns, and either finding better
sources or methods to increase the available data to include other countries
without overcounting.

\clearpage
# Appendix
## Statistical Appendix
### Table of Numeric Predictors
A more detailed analysis of the empirical statistics for the numeric data is 
found in the table below.

\footnotesize
```{r statsTable}
# Empirical stats Table
kable(statsN, digits = 2, caption = "Table of Numeric Predictors",
      format.args = list(big.mark = ","))
```
\normalsize

### Variable Importance: Model 1

```{r lm1VI}
# Model 1: VarImp 
ggplot(varImp(lm1)) + ggtitle('Variable Importance: Model 1')
```

### Variable Importance: Model 2

```{r lm2VI}
# Model 2: VarImp 
ggplot(varImp(lm2)) + ggtitle('Variable Importance: Model 2')
```

### Variable Importance: Model 3
```{r lm3VI}
# Model 3: VarImp 
ggplot(varImp(lm3)) + ggtitle('Variable Importance: Model 3')
```

### Hyperparameter Tuning: Model 3

```{r lm3Plot}
# Model 3: Tuning paths
ggplot(lm3) + ggtitle('Tuning Parameters: Model 3')
```

## Code Appendix
The code chunks below represent the R code called in order during the analysis.
They are reproduced in the appendix for review and comment.

```{r appendix, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

```{r loadLibraries}
```
```{r loadData}
```
<!-- Data Exploration -->
```{r dataPrep}
```
```{r numLogDensities}
```
```{r catPlot}
```
```{r subsetExtraction}
```
```{r likeDislikeCheck}
```
```{r addFeatures}
```
```{r plotBalance}
```
```{r plotExceedMean}
```

<!-- Model Building -->
```{r holdout}
```
```{r dummyVarsAA}
```
```{r lm1Train}
```
```{r lm1Table}
```
```{r lm2Train}
```
```{r lm2Table}
```
```{r lm3Train}
```

<!-- Model Evaluation -->
```{r modTest}
```

<!-- Statistical Appendix-->
```{r statsTable}
```
```{r lm1VI}
```
```{r lm2VI}
```
```{r lm2VI}
```
```{r lm3Plot}
```

<!-- References needs to be last -->
\clearpage
# References
