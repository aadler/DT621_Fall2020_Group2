---
title: "DATA 621 - Business Analytics and Data Mining"
subtitle: 'Fall 2020 - Group 2 - Final Project Paper'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "12/14/2020"
output:
  pdf_document:
    extra_dependencies:
      amsmath: null
      inputenc: utf8
      xcolor: dvipsnames
      setspace: singlespacing
    toc: FALSE
    toc_depth: 3
urlcolor: purple
bibliography: finalprojectrefs.bib
csl: ieee-with-url.csl
abstract: "Using data from YouTube, we attempt to predict the number of views a video will receive using criteria such as location, category, number of likes, number of dislikes, and number of comments. Using forms of linear regression we have covered this semester, we will test various combinations of features for predictive power. [FILL IN OUTCOMES HERE]\\par\\textbf{Keywords:} Youtube, linear regression, count regression, R"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE)
```
```{r loadLibraries}
library(jsonlite)
library(knitr)
library(dplyr)
library(dtplyr)
library(stringr)
library(ggplot2)
library(scales)
library(caret)
library(data.table)
```
```{r loadData}
currentPath <- getwd()
dataPath <- "Data2"
us_path <- file.path(currentPath, dataPath, "USvideos.csv")

colClass <- c(rep('character', 4L), 'integer', 'POSIXct', 'character',
              rep('double', 4L), rep('boolean', 3L))
us_set <- fread(us_path, encoding = 'UTF-8', colClasses = colClass)
nobsUS <- nrow(us_set)

# Category IDs
us_cat_path <- file.path(currentPath, dataPath, "US_category_id.json")
us_cats <- fromJSON(us_cat_path)

us_cats <- data.table(id = as.integer(us_cats$items$id),
                      category = us_cats$items$snippet[, 2])
setkey(us_cats, id)
```

# Introduction
YouTube has changed the future of video entertainment forever [@Moylan2015ADo]. 
In 2019, the platform was estimated to have between \$16 billion and \$25
billion in revenue [@Wakabayashi2019YIa]. YouTube’s model connects a user’s
creativity with a desire for global recognition [@GoogleUtY]. Before YouTube,
international fame was not conceivable outside of a standard television or movie
studio. Today, creators from all over the world are gaining international
prominence using their own equipment and space. “YouTube is central to today’s
video ecosystem,” says Enders Analysis research analyst Jamie McGowan Stuart
[@Foster2020Yt1]. The current top channel “Vlad and Nikita” earns around \$312
thousand per video [@TYL]. According to a survey from Google, 6 out of 10 people
already prefer online video platforms or streaming services over live TV
[@OBTlv]. There are researchers predicting that by 2025---four years now---half
of viewers under the age of 32 will not pay TV service [@McQuivey2015B25].
Understanding some of the underpinnings of what generates views is beneficial to
anyone entering, or already in, the YouTube world.

The remainder of this paper will cover a literature review, an overview of out
methodology, the specifics of our modeling, a discussion of our findings,
thoughts for future work, a statistical appendix, a detailed code appendix, and
finally our references.

# Literature Review
With the popularity of Youtube, this is not a new question. Approaches this
question in the greater data science network include those based on more
advanced machine learning techniques such as SGD or neural net classifiers
[@LEZ2019YVP]. Others leveraged NLP and specially engineered features such as
"clickbait" or "NSFW" tags [@Srinivasan2017YVP]. These attempts usually used the
same dataset as we are.

Reviewing more academic literature uncovers research into the use of Support
Vector Regression with various basis functions on Youtube and/or Facebook videos
[@TR2017PPo; @PAG2013UEV]. Other attempts included building multi-stage treed
regression models where the outcome of a first stage determined which specific
second-stage model would be used for final popularity prediction [@OLL2016API].

These approaches usually added a temporal element to their analysis, and used
"earlier" values to predict later views. Using more sophisticated algorithms and
temporal elements tended to return statistically significant models. The
downside of these approaches are their complexity and opaqueness, of course. Our
approach will necessarily be simpler, although likely more transparent, being
restricted to the family of linear models covered in this course and not
regressing over time.

# Methodology
Using the famous data set from Kaggle [@Mitchell2019TYV], we will explore
relationships between a video’s views and the number of likes, dislikes, and
comments. We may also use a video’s country of origin and the category or type
of video as predictors.

We manually scrubbed the data and discovered that the country-specific files
really were not! They neither refer to videos created by country nor do they
refer to views *specific* to country. Rather they are the total number of views
and other predictors for that video on that day as collected by someone within
that country. Meaning that aggregation is almost always multiplying. There may
be some videos unique to a specific country---one which was not viewed in other
countries, but they are many magnitudes smaller than those seen by all. As the
United States had the most observations, we decided to analyze its data set.

With the data, we identified both numeric and factor predictors, and engineered
features for convenience as well. We will use these features to investigate
relationships with actual views using linear regression models.

Given the models, we will compare the RMSE and MAE on a special holdout set and
will select the model that performs best as the winner for this paper. We do not
expect it to outperform more sophisticated models.

# Experimentation & Results
## Data Exploration

```{r dataPrep}
# Substitute category for numeric ID through joining
us_set <- us_cats[us_set, on = 'id == category_id']

# Change data types for convenience, add target bucket, count tags and then
# get rid of actual tags and category IDs

us_mean <- mean(us_set$views)

us_set[, `:=`(trending_date = as.IDate(trending_date, format = "%y.%d.%m"),
              views = as.double(views),
              tag_count = 1L + str_count(us_set$tags, '[|]'),
              view_target = factor(ifelse(views > us_mean, 1L, 0L),
                                   levels = c(0, 1), labels = c(0, 1)),
              id = NULL,
              category = as.factor(category))
       ][, `:=`(tags = NULL,
                category = relevel(category, 'Entertainment'))]

# Make name shorter for display purposes
setnames(us_set, c('comment_count', 'tag_count'), c('comments', 'tags'))

# Categorize the variable names
boolVars <- c('comments_disabled', 'ratings_disabled', 'video_error_or_removed')
numVars <- c('views', 'likes', 'dislikes', 'comments', 'tags')
facVars <- c('category', 'view_target')
dateVars <- c('trending_date', 'publish_time')
charVars <- c('video_id', 'title', 'channel_title')

# Melt for numerics
usSetN <- melt(us_set, measure.vars = numVars, variable.factor = FALSE,
               variable.name = 'measure', value.name = 'value')

# Melt for factors
usSetF <- melt(us_set,
               measure.vars = c('category', 'view_target'),
               variable.factor = FALSE, variable.name = 'measure',
               value.name = 'value')

# Table of summary statistics
statsN <- usSetN[, .(Mean = mean(value, na.rm = TRUE),
                     SD = sd(value, na.rm = TRUE),
                     Min = min(value, na.rm = TRUE),
                     Q1 = quantile(value, prob = 0.25, na.rm = TRUE),
                     Median = median(value, na.rm = TRUE),
                     Q3 = quantile(value, prob = 0.75, na.rm = TRUE),
                     Max = max(value, na.rm = TRUE),
                     IQR = IQR(value, na.rm = TRUE)),
                 keyby = c('measure')]
```

### Numeric Predictors
For numeric predictors, we are using the number of `likes`, `dislikes`, `tags`,
and `comments`. We may also consider the country of viewing as well. From the 
There is not that much difference in the distribution of the numeric predictors
between the countries where people view. 

```{r numLogDensities, fig.height=4.5, fig.width=8}
ggplot(usSetN[measure != 'views'], aes(x = value, color = measure)) +
  geom_density(kernel = "epanechnikov") +
  ggtitle("Predictor Density (semi-log scale)") +
  scale_x_log10(label = scientific)
```

### Factor Predictors
While tabular representation of factor predictors is difficult, a distribution
of factors by value may prove informative. It is clear from the graphs below
that interests vary by category. `Entertainment` seems to be the most common in
the US with `Music` coming in second. 

```{r factorPlot1, fig.height=4.5, fig.width=8}
ggplot(usSetF[measure == 'category'], aes(y = value)) +
  geom_bar() +
  ggtitle("Distribution by Category") +
  scale_x_continuous(labels = comma)
```

```{r factorPlot2, fig.height=3, fig.width=8}
ggplot(usSetF[measure != 'category'], aes(x = value, fill = measure)) +
  geom_bar(position = 'dodge2') +
  ggtitle("Country and Global Mean Views Exceedence") +
  scale_y_continuous(labels = comma)
```

As with many events which follow the Pareto rule, most of the views are 
attributable to a minority of the videos, as seen by the 0/1 disparity.

## Feature Engineering & Selection
Since we are not factoring in time, it is incorrect to use all the observations.
Therefore, we will extract the latest observation by video by country and use
this subset.

```{r subsetExtraction}
# Select the row numbers of the first entry of the latest trending date by title
# and by country. There are 34 duplicates. Use this as our restricted data set
# corresponding to the most recent view count. The inner set of brackets gets
# the row number (called V1) and the outer set is a simple subset by those row
# numbers.
usExtract <- us_set[
  us_set[, .I[which.max(views)], by = c('title')]$V1
  ]
```

A feature that we will also consider is `lkratio`: the ratio of likes to
dislikes. This will help ascertain if it is the magnitude or the preponderance
which has a greater effect. As there may be some rare videos with no dislikes,
the ratios will be set to 1. To capture the effect of likes with no dislikes, an
indicator variable will be created or those "perfect" videos.

```{r addFeatures}
usExtract[, `:=`(perfect =ifelse(likes > 0 & dislikes == 0, TRUE, FALSE),
                 lkratio = ifelse(dislikes == 0, 1, likes / dislikes))]
```

## Model Building
We will first separate 20% of the data as a true holdout set. It is on this data
that our models will be compared. We will train models on the remaining 80% of
the data.

```{r holdOut}
set.seed(617)
seenIDX <- createDataPartition(usExtract$views, p = 0.8)$Resample1
seenSet <- usExtract[seenIDX, ]
hideSet <- usExtract[-seenIDX, ]
```

### Multiple Linear Regression
Linear regression may be the best known algorithm used when analyzing a
continuous numeric outcome. It searches for a linear relationship of the
predictors that minimizes the squared error between the "predictor" function and
the observations [@Sheather2009AMA]. This model will engage in stepwise feature
using the AIC as the optimization metric.

```{r modelLinear1}
# Using stepAIC means no cross-validation. Train on entire dataset.
trC <- trainControl(method = 'none')

# Capture the unique country codes
set.seed(181)
linMod1 <- train(views ~ likes + dislikes + comments + tags + category +
                   comments_disabled + ratings_disabled +
                   video_error_or_removed + perfect + lkratio,
                 data = usExtract, family = gaussian(link = 'identity'),
                 method = 'glmStepAIC', direction = 'both', trace = 0,
                 trControl = trC)
```

\footnotesize
```{r m1Table}
kable(summary(linMod1$finalModel)$coefficients,
      caption = "Model 1 Linear Regression Output",
      digts = 3L, format.args = list(big.mark = ','))
```
\normalsize

The sign of these coefficients make sense in the main. The most predictive
feature for views is `likes` followed by `dislikes`. Of course, one usually
views a video at least once prior to rating it. We set the baseline category to
`Entertainment`, considering it was the most popular. Therfore we expected
negative coefficients for other categories. Interestingly, `Film & Animation`
is positively correlated with likes over and above `Entertainment`, despite
having fewer views.

Other observations are that the magnitude of the coefficients is similar to that
of the intercept, indicating that the predictors provide large swings to final
views. The "worst" predictor by magnitude is the `Nonprofits & Activism`
category; its negative coefficient is more than twice the baseline intercept
views. The best, interestingly is if ratings are disabled. This is
counterintuitive at first, but is logical. The amount of `likes`, `dislikes`,
and `comments` are many orders of magnitude greater than 1. Therefore, their
coefficients can be much smaller. A Boolean variable is either 1 or 0, therefore
its coefficient is much greater even if its actual contribution is lower.

Variable importance can be seen from the table below. The further down the list
a coefficient preceded by a **`-`** is, the more important it is and the more it
affected the AIC.
\footnotesize
```
                                  Df   Deviance    AIC
<none>                               7.1243e+16 212209
+ `categoryAutos & Vehicles`       1 7.1222e+16 212209
+ tags                             1 7.1223e+16 212209
- `categoryPeople & Blogs`         1 7.1278e+16 212210
- categoryEducation                1 7.1278e+16 212210
- perfectTRUE                      1 7.1279e+16 212210
+ video_error_or_removedTRUE       1 7.1237e+16 212210
+ `categoryPets & Animals`         1 7.1239e+16 212210
+ categoryGaming                   1 7.1239e+16 212210
+ `categoryTravel & Events`        1 7.1240e+16 212210
+ comments_disabledTRUE            1 7.1240e+16 212210
+ categorySports                   1 7.1240e+16 212210
+ `categoryScience & Technology`   1 7.1241e+16 212211
+ categoryShows                    1 7.1243e+16 212211
- `categoryHowto & Style`          1 7.1287e+16 212211
- `categoryFilm & Animation`       1 7.1299e+16 212212
- `categoryNews & Politics`        1 7.1321e+16 212214
- categoryMusic                    1 7.1338e+16 212215
- `categoryNonprofits & Activism`  1 7.1345e+16 212216
- ratings_disabledTRUE             1 7.1463e+16 212227
- categoryComedy                   1 7.1467e+16 212227
- lkratio                          1 7.3555e+16 212413
- dislikes                         1 8.9091e+16 213650
- comments                         1 8.9807e+16 213701
- likes                            1 2.0217e+17 218939
```
\normalsize

## Model Evaluation

## Findings

# Discussion & Conclusions

# Appendix
## Statistical Appendix
A more detailed analysis of the empirical statistics for the numeric data is 
found in the table below.

\footnotesize
```{r statsTable}
kable(statsN, digits = 2,
      caption = "Statistics for Numeric Variables by Country & Global",
      format.args = list(big.mark = ","))
```
\normalsize

## Code Appendix
The code chunks below represent the R code called in order during the analysis.
They are reproduced in the appendix for review and comment.

```{r appendix, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

```{r loadLibraries}
```
```{r loadData}
```
```{r dataPrep}
```
```{r numLogDensities}
```
```{r factorPlot1}
```
```{r factorPlot2}
```

<!-- Feature Engineering & Selection -->
```{r subsetExtraction}
```
```{r addFeatures}
```

<!-- Model Building -->
```{r holdOut}
```
```{r modelLinear1}
```
```{r m1Table}
```

<!-- Statistical Appendix-->
```{r statsTable}
```

<!-- References needs to be last -->  
# References
