---
title: "DATA 621 - Business Analytics and Data Mining"
subtitle: 'Fall 2020 - Group 2 - Homework #3'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "10/28/2020"
output:
  pdf_document: default
urlcolor: purple
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE)

#header-includes:
#    - \usepackage{setspace}
#    - \doublespacing
```

```{r libraries}
library(tidyverse)
library(tidymodels)
library(data.table)
library(corrplot)
library(vip)
library(psych)
library(summarytools)
library(readr)
library(knitr)
library(GGally)
```

```{r load_data}
ca_path = "https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/Final_project/Data/CAvideos.csv"
ca_set <- read_csv(file = ca_path)
us_path = "https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/Final_project/Data/USvideos.csv"
us_set <- read_csv(file = us_path)
gb_path = "https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/Final_project/Data/GBvideos.csv"
gb_set <- read_csv(file = gb_path)
fr_path = "https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/Final_project/Data/FRvideos.csv"
fr_set <- read_csv(file = fr_path)
de_path = "https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/Final_project/Data/DEvideos.csv"
de_set <- read_csv(file = de_path)
```

# Abstract 
`Short summary of the research problem and its importance, what you do and what you find

# Introduction
  
YouTube has changed the future of video entertainment forever 
and the platform is now a multi-billion dollar industry in itself.  
In 2013, less than 8 years after its launch, the platform was seeing 
one billion active users a month. Additionally, in 2019 the 
platform generated $15 billion in advertising revenue.  YouTube’s 
model connected a user’s creativity with a desire for global 
recognition.  Before YouTube, international fame was not conceivable 
outside of a standard television or movie studio.  Today, creators 
from all over the world are gaining international prominence using 
their own equipment and space.   “YouTube is central to today’s video 
ecosystem,” says Enders Analysis research analyst Jamie McGowan Stuart. 
  
The top 10 YouTube channels for half this year alone amassed over 162 
million.  The current top channel “Vlad and Nikita” earn around 312 
thousand per video .   Each video invariably contains at least one 
advertisement as well as most of the videographers have a separate 
sponsor who pays them directly.  According to a survey from Google, 
6 out of 10 people already prefer online video platforms or streaming 
services over live TV .  Additionally Dr. James McQuivey states, by 2025 
(4 years from the authorship of this paper), he expects half of viewers 
under the age of 32 will not have paid for TV service.  Understanding 
some of the underpinnings of what gets the most views is beneficial to 
anyone entering or already in the YouTube world.  
  
## Research Question
  
Using data from kaggle (https://www.kaggle.com/datasnaek/youtube-new), 
we will identify if there is direct or indirect link between a video’s 
number of views and an increase or decrease in the number of likes, 
dislikes, and comments.  We will also use a video’s country of origin and 
the category or type of video as predictors.  
  
## Methodology
  
We will be utilizing, at minimum, the json’s and CSV’s for Canada, 
United States, Great Britain, Germany and France, totaling 
30581 unique values.  The site has data for Japan, Mexico, Korea, India, and 
Russia; we will attempt to pull them in, if required, for a larger picture.  
However, the five already chosen datasets should be sufficient to cover the 
project statement. 
  
The structure of the paper is based on the flow of the research:

* Introduction
* Data 
  * The data sets have an expected heteroscedasticity.  To work around this, we structured the data using the XXXXX method.  
  * After preparing the data, we created randomized subsets of the datasets in a train/test structure for use in the models.
  * Distribution and Correlation plots are used to further the model selection process
* Models
  * We generated X models using methods such as XXXXXX in order to find and chose the most appropriate and accurate model. 
* Results
  * We discuss the performance and prediction accuracy of the model
* Summary and Conclusions
  * Quick recap of the findings, identify areas of further research and shortcomings found during the research.
* References
* Code Appendix
  * All R code used in the research

# Data
•Description of Data
–A complete description of the desired output

## Data Description

The datasets in total have 186,922 unique values across the 5 countries used. They 
contain the daily trending videos from 2017 with up to 200 listed trending videos 
per day.  The first response variable is coded as a 1 when a video's views are above 
the mean of views in their respective country and a 0 if they are below. The second 
response varible XXX is coded as a 1 for above the mean of the global population and 
0 if below.

In all, there are 8 predictors, which include:

Variables|Description   
-|----
views | The number of times a video was watched
category | the type of video, such as "Film & Animation", "Sports", "Music", etc... 
likes | the number of times a user clicked "like"
dislikes | the number of times a user clicked "dislike"
comment_count | the number of comments on an individual video   
comments_disabled | boolian value if the owner of the video disabled users from commenting   
ratings_disabled | boolian value if the owner of the video disabled ratings
country_code | code created to identify country of origin
view_target | Above (1) or below  (0) the mean views from country of origin
view_global_target | Above (1) or below  (0) the mean views globally
 

## DATA PREPARATION

~discuss using Excel for data cleaning for unique values in France.

```{r ca_data_prep}
ca_set <- ca_set %>% filter(complete.cases(ca_set))

ca_mean <- mean(ca_set$views)

ca_set <- ca_set%>%
  mutate(country = 'CA')%>%
  mutate(country_code = 1)%>%
  mutate(view_target = ifelse(views > ca_mean,1,0))
```

```{r de_data_prep}
de_set <- de_set %>% filter(complete.cases(de_set))

de_mean <- mean(de_set$views)

de_set <- de_set%>%
  mutate(country = 'DE')%>%
  mutate(country_code = 2)%>%
  filter(complete.cases(de_set))%>%
  mutate(view_target = ifelse(views > de_mean,1,0))
```

```{r fr_data_prep}
fr_set <- fr_set %>% filter(complete.cases(fr_set))

fr_mean <- mean(fr_set$views)

fr_set <- fr_set%>%
  mutate(country = 'FR')%>%
  mutate(country_code = 3)%>%
  filter(complete.cases(fr_set))%>%
  mutate(view_target = ifelse(views > fr_mean,1,0))
```

```{r gb_data_prep}
gb_set <- gb_set %>% filter(complete.cases(gb_set))

gb_mean <- mean(gb_set$views)

gb_set <- gb_set%>%
  mutate(country = 'GB')%>%
  mutate(country_code = 4)%>%
  filter(complete.cases(gb_set))%>%
  mutate(view_target = ifelse(views > gb_mean,1,0))
```

```{r us_data_prep}
us_set <- us_set %>% filter(complete.cases(us_set))

us_mean <- mean(us_set$views)

us_set <- us_set%>%
  mutate(country = 'US')%>%
  mutate(country_code = 5)%>%
  filter(complete.cases(us_set))%>%
  mutate(view_target = ifelse(views > us_mean,1,0))
```

```{r merge_datasets}
mergeCols = c('video_id','trending_date','title','channel_title','category_id','publish_time','tags','views','likes','dislikes','comment_count','thumbnail_link','comments_disabled','ratings_disabled','video_error_or_removed','description','country','country_code', 'view_target')

full_set <- ca_set %>% full_join(de_set, by=mergeCols)
full_set <- full_set %>% 
  full_join(fr_set, by=mergeCols)%>%
  full_join(gb_set, by=mergeCols)%>%
  full_join(us_set, by=mergeCols)

full_mean <- mean(full_set$views)

full_set <- full_set %>%
  mutate(view_global_target = ifelse(views > full_mean,1,0))
```

```{r global_target}
ca_set <- ca_set %>%
  mutate(view_global_target = ifelse(views > full_mean,1,0))

de_set <- de_set %>%
  mutate(view_global_target = ifelse(views > full_mean,1,0))

fr_set <- fr_set %>%
  mutate(view_global_target = ifelse(views > full_mean,1,0))

gb_set <- gb_set %>%
  mutate(view_global_target = ifelse(views > full_mean,1,0))

us_set <- us_set %>%
  mutate(view_global_target = ifelse(views > full_mean,1,0))
```


## Data Distribution and Correlation
Canada data

```{r summary_ca}
summaryca <- ca_set%>% select(views, category_id,likes, dislikes, comment_count, comments_disabled, ratings_disabled)
describe(summaryca)

ggpairs(
  summaryca,
  lower = list(continuous = ggally_points, combo = ggally_dot_no_facet)
  )

ca_corr <- cor(summaryca)
corrplot(ca_corr, order = "hclust", tl.col = "black", tl.srt = 45, method = "ellipse", bg="black")

```

```{r summary_de}
summaryde <- de_set%>% select(views, category_id,likes, dislikes, comment_count, comments_disabled, ratings_disabled)
describe(summaryus)

ggpairs(
  summaryde,
  lower = list(continuous = ggally_points, combo = ggally_dot_no_facet)
  )
us_corr <- cor(summaryde)
corrplot(us_corr, order = "hclust", tl.col = "black", tl.srt = 45, method = "ellipse", bg="black")

```


```{r summary_us}
summaryus <- us_set%>% select(views, category_id,likes, dislikes, comment_count, comments_disabled, ratings_disabled)
describe(summaryus)

ggpairs(
  summaryus,
  lower = list(continuous = ggally_points, combo = ggally_dot_no_facet)
  )
us_corr <- cor(summaryus)
corrplot(us_corr, order = "hclust", tl.col = "black", tl.srt = 45, method = "ellipse", bg="black")

```

```{r summary_us}
summarygb <- gb_set%>% select(views, category_id,likes, dislikes, comment_count, comments_disabled, ratings_disabled)
describe(summarygb)

ggpairs(
  summarygb,
  lower = list(continuous = ggally_points, combo = ggally_dot_no_facet)
  )

gb_corr <- cor(summarygb)
corrplot(gb_corr, order = "hclust", tl.col = "black", tl.srt = 45, method = "ellipse", bg="black")
```

```{r summary_fr}
summaryfr <- fr_set%>% select(views, category_id,likes, dislikes, comment_count, comments_disabled, ratings_disabled)
describe(summaryfr)

ggpairs(
  summaryfr,
  lower = list(continuous = ggally_points, combo = ggally_dot_no_facet)
  )

fr_corr <- cor(summaryfr)
corrplot(fr_corr, order = "hclust", tl.col = "black", tl.srt = 45, method = "ellipse", bg="black")
```

```{r summary_global}
summarygl <- full_set%>% select(views, category_id,likes, dislikes, comment_count, comments_disabled, ratings_disabled)
describe(summarygl)

ggpairs(
  summarygl,
  lower = list(continuous = ggally_points, combo = ggally_dot_no_facet)
  )

fr_corr <- cor(summarygl)
corrplot(fr_corr, order = "hclust", tl.col = "black", tl.srt = 45, method = "ellipse", bg="black")
```



```{r}

```


There is definite heteroskedasticity across all data points, we will need to address.

# Models
•Data Analysis
–Describe the instrumentation
–Describe the analysis plan
•Describe the scope and limitations of the methodology
## Model Creation

## Model Selection

### Criteria
### Performance

### Predictions

# Results
– Needs to systematically and clearly articulate the study findings. If the results are unclear, the reviewer must decide whether the analysis of the data was poorly executed or whether the Results section is poorly organized.

– Should state wheather the hypothesis were verified or proven untrue or, if no hypotheses were given, weather the research questions were answered.
The authors should also comment on their results in light of previous studies and explain what differences (if any) exist between the findings and those reported by others and attempt to provide an explaination of the differnces.  

# Summary and Conclusions

•Recap briefly what you do in the paper
•Evaluate the effectiveness of your research and provide recommendations (if applicable)
•Make sure that all of the questions raised in the introduction and the literature review have been addressed
•Compare the final results against the original aims and objectives
•Identify any shortcomings and future research

# REFERENCES

 * https://www.ibc.org/trends/analysis-the-youtube-revolution/5796.article 
 * https://www.cashlady.com/youtube-league/
 * Google/comScore custom survey, U.S., January 2016. (n=2,940 adults aged 18+ who like to watch video content in a typical week; video content defined as TV shows, movies, music videos, videos uploaded by people and/or videos uploaded by brands) https://www.thinkwithgoogle.com/marketing-strategies/video/video-trends-where-audience-watching

 * https://go.forrester.com/blogs/15-10-07-by_2025_50_of_adults_under_age_32_will_not_pay_for_tv/


# CODE APPENDIX
The code chunks below represent the R code called in order during the analysis.
They are reproduced in the appendix for review and comment.

```{r appendix, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

