---
title: "DATA 621 - Business Analytics and Data Mining"
subtitle: 'Fall 2020 - Group 2 - Homework #3'
author: Avraham Adler, Samantha Deokinanan, Amber Ferger, John Kellogg,
    Bryan Persaud, Jeff Shamp
date: "10/28/2020"
output:
  pdf_document: default
urlcolor: purple
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, error=FALSE, warning=FALSE, message=FALSE)

#header-includes:
#    - \usepackage{setspace}
#    - \doublespacing
```

```{r libraries}
library(tidyverse)
library(tidymodels)
library(data.table)
library(corrplot)
library(vip)
library(psych)
library(summarytools)
library(readr)
library(knitr)
library(GGally)


set.seed(951)
```

```{r load_data}
ca_path = "https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/Final_project/Data/CAvideos.csv"
ca_set <- read_csv(file = ca_path)
us_path = "https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/Final_project/Data/USvideos.csv"
us_set <- read_csv(file = us_path)
gb_path = "https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/Final_project/Data/GBvideos.csv"
gb_set <- read_csv(file = gb_path)
fr_path = "https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/Final_project/Data/FRvideos.csv"
fr_set <- read_csv(file = fr_path)
de_path = "https://raw.githubusercontent.com/aadler/DT621_Fall2020_Group2/master/Final_project/Data/DEvideos.csv"
de_set <- read_csv(file = de_path)
```

# Abstract 
`Short summary of the research problem and its importance, what you do and what you find

# Introduction
  
YouTube has changed the future of video entertainment forever 
and the platform is now a multi-billion dollar industry in itself.  
In 2013, less than 8 years after its launch, the platform was seeing 
one billion active users a month. Additionally, in 2019 the 
platform generated $15 billion in advertising revenue.  YouTube’s 
model connected a user’s creativity with a desire for global 
recognition.  Before YouTube, international fame was not conceivable 
outside of a standard television or movie studio.  Today, creators 
from all over the world are gaining international prominence using 
their own equipment and space.   “YouTube is central to today’s video 
ecosystem,” says Enders Analysis research analyst Jamie McGowan Stuart. 
  
The top 10 YouTube channels for half this year alone amassed over 162 
million.  The current top channel “Vlad and Nikita” earn around 312 
thousand per video .   Each video invariably contains at least one 
advertisement as well as most of the videographers have a separate 
sponsor who pays them directly.  According to a survey from Google, 
6 out of 10 people already prefer online video platforms or streaming 
services over live TV .  Additionally Dr. James McQuivey states, by 2025 
(4 years from the authorship of this paper), he expects half of viewers 
under the age of 32 will not have paid for TV service.  Understanding 
some of the underpinnings of what gets the most views is beneficial to 
anyone entering or already in the YouTube world.  
  
## Research Question
  
Using data from kaggle (https://www.kaggle.com/datasnaek/youtube-new), 
we will identify if there is direct or indirect link between a video’s 
number of views and an increase or decrease in the number of likes, 
dislikes, and comments.  We will also use a video’s country of origin and 
the category or type of video as predictors.  
  
## Methodology
  
We will be utilizing, at minimum, the json’s and CSV’s for Canada, 
United States, Great Britain, Germany and France, totaling 
30581 unique values.  The site has data for Japan, Mexico, Korea, India, and 
Russia; we will attempt to pull them in, if required, for a larger picture.  
However, the five already chosen datasets should be sufficient to cover the 
project statement. 
  
The structure of the paper is based on the flow of the research:

* Introduction
* Data 
  * The data sets have an expected heteroscedasticity.  To work around this, 
    we structured the data using the XXXXX method.  
  * After preparing the data, we created randomized subsets of the datasets in 
    a train/test structure for use in the models.
  * Distribution and Correlation plots are used to further the model selection 
    process
* Models
  * We generated X models using methods such as XXXXXX in order to find and 
    chose the most appropriate and accurate model. 
* Results
  * We discuss the performance and prediction accuracy of the model
* Summary and Conclusions
  * Quick recap of the findings, identify areas of further research and shortcomings 
    found during the research.
* References
* Code Appendix
  * All R code used in the research

# Data
  
## Data Description

The datasets in total have 186,922 unique values across the 5 countries used. They 
contain the daily trending videos from 2017 with up to 200 listed trending videos 
per day.  The first response variable is coded as a 1 when a video's views are above 
the mean of views in their respective country and a 0 if they are below. The second 
response varible XXX is coded as a 1 for above the mean of the global population and 
0 if below.

In all, there are 8 predictors, which include:

Variables|Description   
-|----
views | The number of times a video was watched
category | the type of video, such as "Film & Animation", "Sports", "Music", etc... 
likes | the number of times a user clicked "like"
dislikes | the number of times a user clicked "dislike"
comment_count | the number of comments on an individual video   
comments_disabled | boolian value if the owner of the video disabled users from commenting   
ratings_disabled | boolian value if the owner of the video disabled ratings
country_code | code created to identify country of origin
view_target | Above (1) or below  (0) the mean views from country of origin
view_global_target | Above (1) or below  (0) the mean views globally
tag_count | number of metadata tags applied to each video
   
## DATA PREPARATION

The owner of the dataset seemed to combine the five countries in question into the 
France dataset. Unfortunately, when this operation occurred, the owner left no marker 
in the dataset to identify which country the data came from originally.  As we are 
going to use the country as predictor, we needed to find a method to remove the unwanted 
values.  
  
Using Excel, we performed a simple VLOOKUP on the video ID from the France dataset to 
the other four datasets.  If the Video ID was in another dataset, we flagged and removed 
it.  The resulting shortened dataset matched in item count to the other datasets.  As 
the datasets are static and we do not plan to have then constantly updated with fresh 
data, using the quickest simplest method is appropriate.  
  
As each dataset is over 30,000 individual objects, incomplete cases were dropped completely. 
Out of the 40,881 objects in the raw Canada dataset, removing the incomplete cases only 
removed 1,296 (roughly 3.2%).  France had the highest number removed for incomplete cases 
at 8.1% and US being the lowest at 1.4%.  France being the highest is understandable due 
to the prior state of the dataset discussed earlier.  

Summary statistic markers were calculated for each dataset and for the environment as a 
whole.  
  
  * A country code (1-5) was applied to each dataset.  
  * Target values of view_target and view_target_global were created 
    * A value of '1' designates the view count is above the mean of the country 
      and global respectively. 
    * A value of '0' designates the view count falls below the mean respectively 
  * 'tag_count', A predictor value, was calculated by counting the number of tags applied 
    to each video entry.

```{r ca_data_prep}
# remove incomplete cases
ca_set <- ca_set %>% filter(complete.cases(ca_set))

#Calculating the mean of the CA dataset
ca_mean <- mean(ca_set$views)


ca_set <- ca_set%>%
  mutate(country = 'CA')%>% #Add country label
  mutate(country_code = 1)%>% #Add country code
  mutate(view_target = ifelse(views > ca_mean,1,0))%>% #calculate above/below view mean
  mutate(tag_count = 1+(str_count(ca_set$tags, ';'))) #count number of tags
```

```{r de_data_prep}
de_set <- de_set %>% filter(complete.cases(de_set))

de_mean <- mean(de_set$views)

de_set <- de_set%>%
  mutate(country = 'DE')%>%
  mutate(country_code = 2)%>%
  filter(complete.cases(de_set))%>%
  mutate(view_target = ifelse(views > de_mean,1,0))%>%
  mutate(tag_count = 1+(str_count(de_set$tags, ';')))
```

```{r fr_data_prep}
fr_set <- fr_set %>% filter(complete.cases(fr_set))

fr_mean <- mean(fr_set$views)

fr_set <- fr_set%>%
  mutate(country = 'FR')%>%
  mutate(country_code = 3)%>%
  filter(complete.cases(fr_set))%>%
  mutate(view_target = ifelse(views > fr_mean,1,0))%>%
  mutate(tag_count = 1+(str_count(fr_set$tags, ';')))
```

```{r gb_data_prep}
gb_set <- gb_set %>% filter(complete.cases(gb_set))

gb_mean <- mean(gb_set$views)

gb_set <- gb_set%>%
  mutate(country = 'GB')%>%
  mutate(country_code = 4)%>%
  filter(complete.cases(gb_set))%>%
  mutate(view_target = ifelse(views > gb_mean,1,0))%>%
  mutate(tag_count = 1+(str_count(gb_set$tags, ';')))
```

```{r us_data_prep}
us_set <- us_set %>% filter(complete.cases(us_set))

us_mean <- mean(us_set$views)

us_set <- us_set%>%
  mutate(country = 'US')%>%
  mutate(country_code = 5)%>%
  filter(complete.cases(us_set))%>%
  mutate(view_target = ifelse(views > us_mean,1,0))%>%
  mutate(tag_count = 1+(str_count(us_set$tags, ';')))
```

```{r merge_datasets}
# Prep column names for joins
mergeCols = c('video_id','trending_date','title','channel_title','category_id','publish_time','tags','views','likes','dislikes','comment_count','thumbnail_link','comments_disabled','ratings_disabled','video_error_or_removed','description','country','country_code', 'view_target','tag_count')

# Join the datasets into a single dataset
full_set <- ca_set %>% full_join(de_set, by=mergeCols)
full_set <- full_set %>% 
  full_join(fr_set, by=mergeCols)%>%
  full_join(gb_set, by=mergeCols)%>%
  full_join(us_set, by=mergeCols)

# Calculate Global mean
full_mean <- mean(full_set$views)

#calculate above/below global mean
full_set <- full_set %>%
  mutate(view_global_target = ifelse(views > full_mean,1,0))
```

```{r global_target}
# add global mean calculation to individual datasets
ca_set <- ca_set %>%
  mutate(view_global_target = ifelse(views > full_mean,1,0))

de_set <- de_set %>%
  mutate(view_global_target = ifelse(views > full_mean,1,0))

fr_set <- fr_set %>%
  mutate(view_global_target = ifelse(views > full_mean,1,0))

gb_set <- gb_set %>%
  mutate(view_global_target = ifelse(views > full_mean,1,0))

us_set <- us_set %>%
  mutate(view_global_target = ifelse(views > full_mean,1,0))
```


## Data Distribution and Correlation
Canada data

```{r summary_ca}
summaryca <- ca_set%>% select(views, category_id,likes, dislikes, comment_count, comments_disabled, ratings_disabled, view_target, view_global_target, tag_count)
describe(summaryca)

ggpairs(
  summaryca,
  lower = list(continuous = ggally_points, combo = ggally_dot_no_facet)
  )

ca_corr <- cor(summaryca)
corrplot(ca_corr, order = "hclust", tl.col = "black", tl.srt = 45, method = "ellipse", bg="black")

```

```{r summary_de}
summaryde <- de_set%>% select(views, category_id,likes, dislikes, comment_count, comments_disabled, ratings_disabled, view_target, view_global_target, tag_count)
describe(summaryus)

ggpairs(
  summaryde,
  lower = list(continuous = ggally_points, combo = ggally_dot_no_facet)
  )
us_corr <- cor(summaryde)
corrplot(us_corr, order = "hclust", tl.col = "black", tl.srt = 45, method = "ellipse", bg="black")

```

```{r summary_gb}
summarygb <- gb_set%>% select(views, category_id,likes, dislikes, comment_count, comments_disabled, ratings_disabled, view_target, view_global_target, tag_count)
describe(summarygb)

ggpairs(
  summarygb,
  lower = list(continuous = ggally_points, combo = ggally_dot_no_facet)
  )

gb_corr <- cor(summarygb)
corrplot(gb_corr, order = "hclust", tl.col = "black", tl.srt = 45, method = "ellipse", bg="black")
```

```{r summary_fr}
summaryfr <- fr_set%>% select(views, category_id,likes, dislikes, comment_count, comments_disabled, ratings_disabled, view_target, view_global_target, tag_count)
describe(summaryfr)

ggpairs(
  summaryfr,
  lower = list(continuous = ggally_points, combo = ggally_dot_no_facet)
  )

fr_corr <- cor(summaryfr)
corrplot(fr_corr, order = "hclust", tl.col = "black", tl.srt = 45, method = "ellipse", bg="black")
```


```{r summary_us}
summaryus <- us_set%>% select(views, category_id,likes, dislikes, comment_count, comments_disabled, ratings_disabled, view_target, view_global_target, tag_count)
describe(summaryus)

ggpairs(
  summaryus,
  lower = list(continuous = ggally_points, combo = ggally_dot_no_facet)
  )
us_corr <- cor(summaryus)
corrplot(us_corr, order = "hclust", tl.col = "black", tl.srt = 45, method = "ellipse", bg="black")

```


```{r summary_global}
summarygl <- full_set%>% select(views, category_id,likes, dislikes, comment_count, comments_disabled, ratings_disabled, view_target, view_global_target, tag_count)
describe(summarygl)

ggpairs(
  summarygl,
  lower = list(continuous = ggally_points, combo = ggally_dot_no_facet)
  )

fr_corr <- cor(summarygl)
corrplot(fr_corr, order = "hclust", tl.col = "black", tl.srt = 45, method = "ellipse", bg="black")
```


There is definite heteroskedasticity across all data points, we will need to address.

# Models
•Data Analysis
–Describe the instrumentation
–Describe the analysis plan
•Describe the scope and limitations of the methodology

```{r ca_train_split}
#potential train/test split
ca_set_model <- ca_set %>%
  select(-title, -tags, -channel_title, -trending_date, -publish_time, -video_id, -thumbnail_link, -description, -country)


ca_smp_size <- floor(0.75*nrow(ca_set))
ca_ind <- sample(seq_len(nrow(ca_set)), size = ca_smp_size)

ca_train <- ca_set[ca_ind,]
ca_test <- ca_set[-ca_ind,]
```

## Model Creation
• describe which model are created and why

## Model Selection

### Criteria
• criteria of the model selection

### Performance
• describe the performance of the selection

### Predictions
• describe the prediction values 

# Results
•Needs to systematically and clearly articulate the study findings. If the results are unclear, the reviewer must decide whether the analysis of the data was poorly executed or whether the Results section is poorly organized.

• Should state wheather the hypothesis were verified or proven untrue or, if no hypotheses were given, weather the research questions were answered.
The authors should also comment on their results in light of previous studies and explain what differences (if any) exist between the findings and those reported by others and attempt to provide an explaination of the differnces.  

# Summary and Conclusions

•Recap briefly what you do in the paper
•Evaluate the effectiveness of your research and provide recommendations (if applicable)
•Make sure that all of the questions raised in the introduction and the literature review have been addressed
•Compare the final results against the original aims and objectives
•Identify any shortcomings and future research

# REFERENCES

 * https://www.ibc.org/trends/analysis-the-youtube-revolution/5796.article 
 * https://www.cashlady.com/youtube-league/
 * Google/comScore custom survey, U.S., January 2016. (n=2,940 adults aged 18+ who like to watch video content in a typical week; video content defined as TV shows, movies, music videos, videos uploaded by people and/or videos uploaded by brands) https://www.thinkwithgoogle.com/marketing-strategies/video/video-trends-where-audience-watching

 * https://go.forrester.com/blogs/15-10-07-by_2025_50_of_adults_under_age_32_will_not_pay_for_tv/


# CODE APPENDIX
The code chunks below represent the R code called in order during the analysis.
They are reproduced in the appendix for review and comment.

```{r appendix, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

```{r libraries}
```
```{r load_data}
```
```{r ca_data_prep}
```
```{r de_data_prep}
```
```{r fr_data_prep}
```
```{r gb_data_prep}
```
```{r us_data_prep}
```
```{r merge_datasets}
```
```{r global_target}
```
```{r summary_ca}
```
```{r summary_de}
```
```{r summary_gb}
```
```{r summary_fr}
```
```{r summary_us}
```
```{r summary_global}
```
```{r ca_train_split}
```